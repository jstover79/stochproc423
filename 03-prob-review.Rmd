#  Probability review

**under construction**

## Probability basics

## Random variables and distributions

::: defbox
**Definition.** A **random variable** $X$ is a variable where you must perform a random experiment to determine its value. The **sample space** $S$ is the set of numerical values that the random variable can take. An **event** is a subset of the sample space. A random variable can be either *discrete* (if it takes values in a discrete set such as $\mathbb N$) or *continuous* (if it can take on any value in some intveral).
:::

::: exbox
**Examples.**

(1) Let $X$ be the number of dots on the upper face of a dice roll.

(2) Let $X$ be the mass of a randomly selected person form a specific population.

(3) Let $X$ be the number of cars that pass by a given intersection druing a particular day.

(4) Let $X$ be the number of radioactive decays in one hour of some particular material.
:::

::: defbox
**Definition.** A **probability function** allows us to calculate the probabilities of observing specific numerical values or ranges of values for random variable $X$. A discrete random variable has a **probability mass function** (pmf) $f_X(x)=P(X=x)$, and a continuous random variable has a **probability density function** (pdf) $f_X(x)$ which we must integrate to get probabilities $P(a<X<b)=\int_a^b f_X(x)dx$. The **cumulative distribution function** (cdf) gives cumulative probabilities $F_X(x)=P(X\leq x)$.
:::

## Independence of random variables

When working with introductory probability the idea of *independent events* was covered. Events $A$ and $B$ are independent if and only if $P(A\cap B)=P(A)P(B)$. Independence for random variables works similarly.

::: defbox
**Definition.** Random variables $X$ and $Y$ are **independent** if and only if every $X$-event is independent of every $Y$-event. That is for every subset of possible $X$-values $A$ and every subset of possible $Y$-values $B$ we have 
$$P(\{X\in A\}\cap \{Y\in B\})=P(X\in A)P(Y\in B).$$

For discrete random variables $X$ and $Y$, they are independent if and only if $P(X=x,Y=y)=P(X=x)P(Y=y)$ for all $x$ and $y$. That is the joint pmf must be exactly the product of the marginal pmfs. 

For continuous random variables $X$ and $Y$, they are independent if and only if their joint pdf is exactly the product of the marginal pdfs: $f_{X,Y}(x,y)=f_X(x)f_Y(y)$.

Note that for independent random variables, independence requires that the allowable values for one variable must not be affected by the other variable, e.g. the set of possible $X$-values must not depend on $Y$ in any way.
:::

::: exbox
**Example.** Let $X$ be the outcome of a fair 6-sided die and $Y$ be the outcome of a fair coin flip ($0=~$tails,$1=~$heads). Then we naturally think of them as independent, and they are in the technical sense of the term. It makes physical sense that the coin flip shouldn't impact the die roll at all unless we are to use some contrived mechanism to force them to interact, e.g. if a machine were to control the force of the coin flip and die roll is some specific way so that the coin tended to be heads when the die tended to be even.

Here the marginal pmfs are $f_X(x)=\frac16$ for $x=1,2,3,4,5,6$ (and zero otherwise), and the pmf for $Y$ is $f_Y(y)=\frac12$ for $y=0,1$. Their joint pmf is $f_{X,Y}(x,y)=\frac1{12}$ for $(x,y)=(1,0),\ldots,(6,0),(1,1),\ldots,(6,1)$.
:::

::: exbox
**Example.** Let $X$ and $Y$ have joint probabilities given by the table below.

|   |     |     X     |
|---|-----|-----|-----|
|   |     | 10  | 20  |
| Y | 5   | 0.5 | 0.2 |
|   | 500 | 0.1 | 0.2 |

You can actually look at the table and see that $X$ and $Y$ are not independent by seeing the $Y$-values of 5 and 500 have equal probability mass under the $X=20$ column but unequal probability mass under the $X=10$ column. It isn't always that simple though. Here is a joint pmf for an independent $X,Y$ pair, and it isn't obvious they are independent.

|   |     |      | X   |       |
|---|-----|------|------|-------|
|   |     | 10   | 20   | 30    |
| Y | 5   | 0.36 | 0.18 | 0.06  |
|   | 500 | 0.24 | 0.12 | 0.04  |

:::

## Discrete: Bernoulli, binomial, geometric, Poisson

## Continuous: Uniform, exponential, normal

## Summary 

::: defbox
**Summary of notation, formulas, and terminology**

Discrete RVs:<br>
&nbsp; pmf $f_X(x)=P(X=x)$ <br>
&nbsp; cdf $F_X(x)=P(X\leq x)=\sum_{j\leq x} f_X(j)$ <br>
&nbsp; $E(X)=\sum_{x} x P(X=x)=\sum_{x} x f_X(x)$ 

Continuous RVs:<br>
&nbsp; pdf $f_X(x)$, $P(a<X<b)=\int_a^b f_X(x)dx$ <br>
&nbsp; cdf $F_X(x)=P(X\leq x)=\int_{-\infty}^x f_X(t)dt$, $f_X(x)=\frac{d}{dx}F_X(x)$<br>
&nbsp; $E(X)=\int_{-\infty}^\infty x f_X(x) dx$ 

Variance: $\textrm{Var}(X)=E[(X-E(X))^2]=E(X^2)-E(X)^2$

Expected value of function of RV: $E(h(X))=\sum_x h(x) f_X(x)$ (discrete)<br> 
&nbsp; $E(h(X))=\int_{-\infty}^\infty h(x) f_X(x) dx$ (continuous)

Jointly distributed RVs: <br>
&nbsp; $f_{X,Y}(x,y)=P(X=x,Y=y)=P(\{X=x\}\cap\{Y=y\})$

Independence of RVs: <br>
&nbsp; $X,Y$ independent if and only if $f_{X,Y}(x,y)=f_X(x) f_Y(y)$, <br>
&nbsp; i.e. $P(X=x,Y=y)=P(X=x)P(Y=y)$, for all $x,y$

**Discrete RVs**

*Bernoulli:* models a process with only two outcomes<br>
&nbsp; $X\sim\mathsf{Bernoulli}(p)$<br>
&nbsp; $f_X(x)=\begin{cases}p &\text{ for } x=1\\ 1-p &\text{ for } x=0\end{cases}$<br>
&nbsp; $E(X)=p$, $Var(X)=p(1-p)$<br>
&nbsp; R: $P(X=x)=~$`dbinom(x,size=1,prob=p)`<br>

*Binomial:* models number of successes in $n$ independent trials with $p$ the probability of success for each trial<br>
&nbsp; $X\sim\mathsf{Binom}(n,p)$<br>
&nbsp; $f_X(x)={n\choose x}p^x (1-p)^{n-x}$, $x=0,1,\ldots,n$<br>
&nbsp; $E(X)=np$, $Var(X)=np(1-p)$<br>
&nbsp; R: $P(X=x)=~$`dbinom(x,size=n,prob=p)`<br>

*Geometric:* models number of trials up to and including first success<br>
&nbsp; $X\sim\mathsf{Geom}(p)$<br>
&nbsp; $f_X(x)=p(1-p)^{x-1}$, $x\in\{1,2,\ldots\}=\mathbb N$<br>
&nbsp; $E(X)=\frac1p$, $Var(X)=\frac{1-p}{p^2}$<br>
&nbsp; R: $P(X=x)=~$`dgeom(x-1,prob=p)` (note that R only counts the failures)<br>

*Poisson:* models number of events over a continuous extent<br>
&nbsp; $X\sim\mathsf{Pois}(\lambda)$<br>
&nbsp; $f_X(x)=\frac{e^{-\lambda}\lambda^x}{x!}$, $x\in\{0,1,\ldots\}=\mathbb N_0$<br>
&nbsp; $E(X)=\lambda$, $Var(X)=\lambda$<br>
&nbsp; R: $P(X=x)=~$`dpois(x,lambda=λ)`<br>

**Continuous RVs**

*Uniform:* models a continuous quantity that takes any value in an interval with equal likelihood<br>
&nbsp; $X\sim\mathsf{Unif}(a,b)$<br>
&nbsp; $f_X(x)=\frac1{b-a}$, $x\in(a,b)$<br>
&nbsp; $E(X)=\frac12(a+b)$, $Var(X)=\frac1{12}(b^2-a^2)$<br>
&nbsp; R: $P(X\leq x)=~$`punif(x,min=a,max=b)`<br>

*Normal:* models quantity that is symmetrically distributed and random variation from many small additive contributions<br>
&nbsp; $X\sim\mathsf{N}(\mu,\sigma^2)$<br>
&nbsp; $f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$, $x\in\mathbb R$<br>
&nbsp; $E(X)=\mu$, $Var(X)=\sigma^2$<br>
&nbsp; R: $P(X\leq x)=~$`pnorm(x,mean=μ,sd=σ)`<br>

*Exponential:* models wait times between events occurring at random times<br>
&nbsp; $X\sim\mathsf{Exp}(\lambda)$<br>
&nbsp; $f_X(x)=\lambda e^{-\lambda x}$, $x>0$<br>
&nbsp; $E(X)=\frac1\lambda$, $Var(X)=\frac1{\lambda^2}$<br>
&nbsp; R: $P(X\leq x)=~$`pexp(x,rate=λ)`<br>

:::
