[["index.html", "Math 423 Stochastic Processes Course Notes Introduction", " Math 423 Stochastic Processes Course Notes Joseph Stover 2023-03-31 Introduction Stochastic processes is the mathematical theory of random phenomena. Our working definition of a random phenomenon or process, is one that we cannot predict accurately. Physical processes are often not predictable for a variety of reasons. Typically, we quantify a physical process somehow, e.g. by taking measurement at specific times. If each measurement is unpredicatable (random), then our sequence of measurements is a stochastic process! You have seen random variables in previous probability or statistics courses. A random variable \\(X\\) takes on real number values, but we cannot predict what precise value it will take perfectly it is random. One can think of performing a random experiment such as rolling a die and letting \\(X\\) be the number of dots on the upper face or observing some physical process like drilling an oil well with \\(X\\) being the amount of oil produced on the first day or growing a plant and letting \\(X\\) be the height of the plant after one month of growth. In all of the examples \\(X\\) just takes on a single numerical value. A stochastic process tracks these processes over time. Let \\(X_n\\) be the outcome of the \\(n^{th}\\) die roll, the height of the plant after month \\(n\\), or the amount of oil produced during day \\(n\\). In this way a stochastic process can initially be thought of as a sequence of random variables. Here are some examples of how a stochastic process might model a physical process. Modeling the daily closing price of a stock for one year. Modeling the number of insurance claims in each month over a year. The number of new infections each day for a particular disease. Tracking radioactive decays over time. The location of an animal as it moves through its habitat, e.g. the distance from a bird to its nest as a funciton of time. Each of these physical phenomena are highly unpredictable, and so we generally treat them as random. "],["introduction-to-r-and-rstudio.html", "Chapter 1 Introduction to R and RStudio 1.1 Getting access to R and RStudio 1.2 R basics 1.3 Installing and using packages in R 1.4 Random variables 1.5 Basics of programming, R scripts 1.6 Importing datasets into R Summary", " Chapter 1 Introduction to R and RStudio R is a statistical software package and a computer programming language. It is widely used in both industry (e.g. by data scientists) and academic research. RStudio is a graphical interface for R. Here I will give a brief introduction to R and RStudio. 1.1 Getting access to R and RStudio There are two main ways to use R: install R and RStudio locally on your computer, or use Posit cloud. (Posit is the company that makes RStudio) Posit Cloud: I strongly suggest getting an account on Posit Cloud (this is the company that makes RStudio). Then you have access to a fully-capable and up-to-date version of R and RStudio form any web browser on any device. Go to: https://posit.cloud/, and sign up. Their free account simply has computation time limitations which should be no problem for most casual users. Alternatively, you can download and install R and RStudio Desktop locally on your computer. I recommend doing this, especially if you are more serious about learning R or running programs that require more computation time. Downloading and installing R statistical software: I recommend two things: Install R. This is the actual statistical software. Install RStudio Desktop. This is a nice user interface for R. *Note that the actual software version numbers change frequently, and they are, as of writing, R 4.2.2 and RStudio Desktop 2022.12.0+353. Getting R: First, download the appropriate version (Windows, Mac, etc.) of R from here: https://cloud.r-project.org/ For Windows: Click on Download R for Windows, then click on install R for the first time, then click on Download R-4.2.2 for Windows. Then install the software. For Mac OS: Click on Download R for macOS, then click on either R-4.2.2-arm64.pkg or R-4.2.2.pkg (depending on macOS version and processor type). Then install the software. I am more proficient at Windows than Mac, but if you have trouble installing it, come see me and I should be able to help you get it figured out. There are also Linux/Unix optionsI am familiar with Ubuntu and so should be able to help on those platforms as well. Getting RStudio Desktop: After you have R installed, I recommend installing RStudio Desktop in order to have access to a more friendly user interface. I will always be using RStudio when I show demonstrations in class. Simply go here: https://posit.co/download/rstudio-desktop/#download and choose your desired version. The webpage should automatically detect your operating system and provide you with a link to the correct version of RStudio. You can scroll down the page and see links to various versions though. For Windows, the first link should work: Windows 10/11 RSTUDIO-2022.12.0-353.EXE. For MacOS the second link should work: macOS 11+ RSTUDIO-2022.12.0-353.DMG. Install the software. Done! There is also a link for older versions of RStudio in case you have an older version of Windows, MacOS, or Linux, etc. Brief test of R &amp; RStudio: Launch RStudio Desktop or open a Posit Cloud Workspace/Project. Locate the Console subwindow. This is where we will type our commands. Type x &lt;- 3 in the console next to the &gt; (which is a command prompt that you will always type command next to). Then press the enter or return key on your keyboard. This saves the value of 3 for the variable x. Then type x+5 and hit enter. You should see the output [1] 8. This indicates the result of the computation. The [1] indicates that the output is a single number. Later on we will learn many interesting commands that are useful. Now you know how to open RStudio and use it as a calculator! The RStudio user interface is shown below. The most important parts are highlighted and labeled wtih brief descriptions. The RStudio user interface. Using R online through other sources: Another option for using R statistical software is to use one of the many places online where you can use it through a web browser. There are many such websites. Here is one website where you can conveniently evaluate R code online from a web browser in any device: https://rdrr.io/snippets/ Another option for having quick access to R (and this is useful for a smartphone) is SageCell at: https://sagecell.sagemath.org/. This website can be used to evaluate commands from a variety of programming languages (including MATLAB and Python). Just select R from the language tab at the lower right of the textbox. If you are familiar with MATLAB, choose the option Octave. Octave is basically an open source version of MATLAB and you can run MATLAB code using the Octave language option on SageCell. 1.2 R basics We can assign variables in R with arrows or equal signs, e.g. x &lt;- 3, 3 -&gt; x, and x = 3 all achieve the same result, setting the variable \\(x\\) to store the numerical value \\(3\\). We can perform most arithmetic operations in the usual way, e.g. x+2, 5-x, x/4, x^2, and 5*x. There are also many special functions such as log() (natural log, \\(\\texttt{log(2)}=\\ln(2)\\)), sin(), exp() (natural exponentiation, \\(\\texttt{exp(5)}=e^5\\)), etc. 1.2.1 Data structures, vectors, matrices The basic types of data structures in R include (but are not limited to) numbers, vectors, matrices, and characters. Here are some additional data structures you might see: NULL means empty but space is reserved for storing something NA means nat available but space is reserved as well \"3\" means the string character 3, not a number. We can coerce this to become a number by as.numeric(\"3\") Here are some relevant commands: numeric() creates an empty vector with no dimensions numeric(length=n) creates a vector of \\(n\\) zeros matrix() creates an empty \\(1\\times1\\) matrix with \\(\\texttt{NA}\\) as the entry matrix(nrows=n,ncol=m) creates an empty matrix of NAs wtih \\(n\\) rows and \\(m\\) columns c(1,2,3) creates an numeric vector \\((1,2,3)\\) 0:5 creates an numeric vector \\((0,1,2,3,4,5)\\) seq(0,5,by=1) creates an numeric vector \\((0,1,2,3,4,5)\\) seq(0,1,by=0.5) creates an numeric vector \\((0,0.5,1)\\) seq(0,1,by=0.3) creates an numeric vector \\((0,0.3,0.6,0.9)\\) seq(0,1,length.out=4) creates an numeric vector \\((0,\\frac12,\\frac23,1)\\) You access the components of a vector or matrix with square brackets: x[4] is the \\(4^{th}\\) component of vector \\(x\\). If \\(x\\) was a matrix, it would treat it like a vector concatenating the columns together and access the \\(4^{th}\\) component. We can access the components of a matrix with M[i,j] to get \\(M_{ij}\\) the component in the \\(i^{th}\\) row and \\(j^{th}\\) column. We can create a \\(2\\times3\\) matrix and store it with the name \\(\\texttt{M}\\) with M &lt;- matrix(nrow=2,ncol=3). Alternatively we can use M &lt;- matrix(ncol=3,nrow=2). Normally for R commands, there is a default order for the arguments you pass through to it, and you dont need to specify the name of the arguments if you follow that default ordering. If you specify all arguments by name (e.g. nrow=2 and ncol=3), then you can input the arguments in any order. Its best practice to stick with the default order to develop good habits though. We can specify a specific matrix in the following way. M &lt;- matrix(c(1,0,2,3,1,5),nrow=2,ncol=3,byrow=TRUE) This takes the numeric vector \\((1,0,2,3,1,5)\\) and constructs the matrix \\[\\left(\\begin{matrix} 1 &amp; 0 &amp; 2\\\\ 3 &amp; 1 &amp; 5 \\end{matrix}\\right).\\] Alternatively, we can leave off the byrow=TRUE and it will default to constructing the matrix column-by-column. RStudio tends to play nicely with automatically indenting things, so one can put a line break in order to see the rows better in the code. M &lt;- matrix(c(1,0,2, 3,1,5),nrow=2,ncol=3,byrow=TRUE) You can also store characters and strings in vectors and matrices, but once you put in a string, R turns all numerical entries into character data structure as well. As long as two vectors or two matrices have the same dimensions, you can add or subtract them in the natural way: x+y or A-B. Component-wise multiplication is x*y which will keep the same dimensions but just with components multiplied, and similarly one can do x/y for component-wise division. Matrix multiplication is done with %*% such as x %*% M, but you need to make sure the dimensions are appropriate. Here are a few other useful commands. t(M) transpose of matrix or vector \\(M\\). inv(M) inverse of matrix \\(M\\). dim(x) gives dimensions of object \\(x\\) (length of vector or number of rows and columns for a matrix). class(x) gives the class for object \\(x\\), e.g. it will tell us if it is numeric, character, or a matrix, or some other type of object such as Boolean/logical, etc. 1.3 Installing and using packages in R It is common for software developers to develop libraries or packages which contain functions and methods. The purpose is the same as a mathematical function. Rather than you rewriting the equation each time, you can program the function and simply call that computer function with a single letter rather than a complicated formula. As an example, R has some base functionality for matrix computations, but in order to raise matrices to exponents, you need to expm package. Generally to install a package named packagename we execute the command install.packages(\"packagename\"). Once a package is installed, in order to use the methods and functions it includes, they must be loaded into Rs active workspace and can be done by executing the command library(packagename). In order to run the stock simulation code we need to install the packages quantmod and RQuantLib. To do this, execute the commands: install.packages(\"quantmod\") and install.packages(\"RQuantLib\"). Youll see some output indicating what R is doing, downloading some files, unpacking them, and installing them. Sometimes it may even be compiling some code in the background. Usually it takes a few seconds to minutes to install a new package. Once a package is installed, in order to use the methods and functions it includes, execute the command library(packagename), e.g. library(quantmod). Notice that installing a package require quotation marks around the package name, but calling it into Rs workspace does not. 1.4 Random variables R has built in methods for many different random variables. Generally for each random variable there are four methods: dname() (pmf or pdf), pname() (cdf), qname() (quantiles), and rname() (random simulation) where you replace name by the name of a particular distribution. Here are the R names for some common distributions: Distribution R name prefix Binomial binom Exponential exp Normal norm Poisson pois 1.5 Basics of programming, R scripts under construction 1.6 Importing datasets into R Sometimes you might want to import a matrix of vector or otherwise a datset from a website, pdf, or other file format. There are several ways I will discuss getting data into R: text file, csv file (comma separated value), Excel (xls or xlsx), or through the computers local clipboard functionality. This is only a small number of input file types that R is capable of handling though. Import data from text file. To import a single list of data into R, here is the table read method of importing data: 1. Open an Excel spreadsheet. Put a name for your variable/data in the first cell (recommended text only, no spaces or special characters). Type all data values in the column under the name. Save as filetype \\(\\texttt{Text (MS-Dos) (*.txt)}\\). Alternatively, open it in MS Notepad, and copy and paste the entire data column, including the name, into Notepad and save it as a *.txt file. Lets say we saved it as \\(\\texttt{filename.txt}\\). Open R and execute the command: x &lt;- read.table(\"fileneame.txt\", header=T). Alternatively, if the dataset is small enough that manually typing out the entire list is not a major fuss, we can just use the R command x &lt;- c(x1,x2,x3,...,xn) where \\(\\texttt{x1,x2,x3,...,xn}\\) is our list of actual numerical data values. I have saved our list of water well depth data is \\(\\texttt{welldepth.txt}\\). So we can execute: wd &lt;- read.table(\"welldepth.txt\",header=TRUE). This will give our data a data frame\" type of tabular structure. A data frame is a specific type of data object in \\R. A data frame is composed of multi-variate data, which each individual variable stored as a column. Thenames of each column (which indicate the data stored in that column) are stored at the top of the data frame, in the header. To extract just the numerical well depth data, well need to execute x &lt;- wd$well_depth. The name well\\_depth\" is the header name for our column of data. In R, the dollar symbol$ is used to reference the data stored within a specific column of a data frame. A data frame can have multiple columns with each column given a unique name in the header, e.g. a data frame named \\(\\texttt{y}\\) with columns named \\(\\texttt{V1, V2, V3}\\). We can extract the data from the 2nd column by \\(\\texttt{y\\$V2}\\). Headers: Generally R expects the first horizontal row of your data file to contain the names of the data stored in each column. If your data file has more than two rows of text before the actual data begins, it may cause problems. When loading data you need to pay attention to whether you have copied a header or not. wd &lt;- read.table(\"filename.txt\", header=TRUE) (reads top line as header) wd = read.table(\"filename.txt\", header=FALSE) (reads top line as data) Import data via the file browser. Now that you have gone through the work of manually setting your working directory and importing dataset in with a command line, you will be shown the easy way. Simply click on the three dots in the file pane in RStudio and navigate to the local directory on your computer where the file is stored. Click on the file from within RStuidos file pane. Then RStudio will automatically give you a preview of the file and dataset and will format the commands necessary to import it, including loading any packages that are necessary. This will work for Excel files, csv files, text files, and many other types of files. Import data by manually typing. We can also import our well depth data directly by typing it all out separated by commas inside of a c() with:\\ x &lt;- c(115,107,114,465,250,220,690,280, 520,100,450,200,80,500,120,375, 575,700,80,640,640,680,260,100, 300,160,440,165,660,120,560,180, 110,65,120,86,360,100,300,165, 500,600,600,260,300,620,480,440, 240,120,420,175,585,240,460,200, 200,430,482,290,290,620,660,440) Note that in order to break lines without executing code, use a \\(\\texttt{Shift-Enter}\\) (hold the shift key while pressing the Enter/Return key). Import data from csv file. If your data is stored as a csv file, it is very much like reading from a text file. Again pay attention to whether or not the top row is a header with names for each of the data variables. d &lt;- read.csv(\"filename.csv\",header=TRUE). Import data from Excel file. If you have data store in a \\(\\texttt{xls}\\) or \\(\\texttt{xlsx}\\) file, you can import it with the following commands. First we must load the R library readxl. An R library is a specific software extension that gives us additional capabilities. You may need to install this package first using the command: install.packages(\"readxl\"). Then once you have confirmed that readxl package is installed, issue the command library(readxl) in order to load the capability into your R session. Then you can finally load the data form the Excel file as follows: d &lt;- read_excel(\"filename.xlsx\",header=TRUE). Again, pay attention to whether or not your data file include a header row and set the command to either \\(\\texttt{TRUE}\\) or \\(\\texttt{FALSE}\\) appropriately. Import data from clipboard. A very convenient way to import data is to highlight it on screen and copy it to your computers clipboard memory. This works easiest if the data is already formatted nicely in a spreadsheet, but you can copy the data from a pdf, webpage, text file, and many other digital files this way too. It can become tricky as will be illustrated in the example below. Make sure to know whether you are only selecting the data or including a header or name. Disclaimer: The following works when viewing this pdf in Adobe Reader. If the pdf is opened in other viewing software, this may not work. If we highlight the list of data above in this pdf and copy it to the computers clipboard (right-click and copy or \\(\\texttt{Ctrl-C}\\)), we can import it with the command:read.table(\"clipboard\"). This data will be imported as a data frame which is a specific data structure in R and it will be exactly in the format with rows and columns as in this pdf. If the data copied from the clipboard is simply a list of numbers for a single variable, then we will need to extract the numerical data from the data frame and turn it into a numerical list with the following command. This can be tricky to accomplish though. The following command will turn the data frame (imported via clipboard from copying the well depth data in this pdf within Adobe Reader): x &lt;- as.vector(t(wd)). Now we have our list of well depths stored as \\(\\texttt{x}\\), and we are ready to analyze it with statistical techniques. Summary Summary of notation, formulas, and terminology under construction "],["counting-sets-and-probability-basics.html", "Chapter 2 Counting, sets, and probability basics 2.1 Ticket-in-a-box model of probability 2.2 Relative area model of probability 2.3 Sample Spaces and Events 2.4 Set Operations 2.5 Venn Diagrams 2.6 Counting, permutations, and combinations 2.7 Probability", " Chapter 2 Counting, sets, and probability basics Now we review basic introductory probability theory. An experiment with observable outcomes that has some level of unpredictability to them is often called random. You may have an intuitive understanding of what random means and what it means for something to have a probability, chance, or likelihood of happening. We want to mathematically formalize the concept of probability. An experiment here could be as simple as selecting one or more individuals from a fixed population. It could be a process such as an industrial machine producing some item. We will investigate the probability of different individuals being selected in the former example and the probability of the produced item having certain properties in the latter example. 2.1 Ticket-in-a-box model of probability Here is a what I hope will be an intuitive model of probability and random experiments. Assume we have a box full of tickets. We are to shake the box up and draw one or more tickets out. It should be intuitively clear that we have little (if any) control over what ticket we draw out (assuming we arent looking and sorting through the tickets until we find the one we want). Now, let the tickets be labeled with letters, numbers, colors, names, or any label you can imagine. The tickets could contain the names of all students in a class, 1 per student, or they could have the sides of a die written on them. We could even account for an experiment that is biased towards a certain outcome by having more tickets with one label and less of another label. For example, if we want to simulate a coin that has a 75% chance of flipping heads and a 25% chance for tails, then we could put in 75 H tickets and 25 T tickets. Now if you were to draw a ticket, replace it, shake the box up and draw again, you should intuitively feel that out of 10 draws, we expect to get 7 or 8 Hs and the rest Ts. The result is unpredictable, but if you drew a very large number of tickets (replacing each as we go), then if you drew only Hs, you would probably agree that you might get suspicious, say, that the T tickets were stuck to the walls or missing. Keep this model of probability in mind as we go through this course. We will come back to this idea several times. 2.1.1 Sampling from a box of tickets If we have a box of tickets with \\(k\\) different ticket labels, and \\(n_i\\) of tickets with label \\(L_i\\), then our box looks like this: \\[\\{L_1,L_1,\\ldots,L_1, L_2, L_2,\\ldots, L_2, \\ldots, L_k, L_k\\ldots, L_k\\}.\\] We wish to shuffle these up randomly and draw out \\(n\\) total tickets. This can be accomplished in R as follows: S=c(rep(L1,n1),rep(L2,n2),...,rep(Lk,nk)) sample(S,size=n,replace=T/F) Examples: Flip a fair coin once: sample(c(\"H\",\"T\"),size=1,replace=T) Flip a fair coin 10 times: sample(c(\"H\",\"T\"),size=10,replace=T) Roll a fair die 20 times: sample(1:6,size=20,replace=T) Draw without replacement until all tickets are gone with 3 blue tickets, 2 red tickets, and 1 green ticket: S=c(rep(&quot;blue&quot;,3),rep(&quot;red&quot;,2),rep(&quot;green&quot;,1)) sample(S,size=length(S),replace=F) ## [1] &quot;green&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; 2.2 Relative area model of probability First consider tossing a tossing a dart at a dartboard. Consider a dartboard that is a disc with a small circle in the center of the dartboard (the bulls-eye). Having a dart land in the bulls-eye is normally a higher point value than another other location on the board. Why is this? We can intuitively reason that the bulls-eye is small, therefor the dart can land in the bulls-eye in fewer ways than the rest of the board which has many more places for he dart to land. This is actually a continuous version of the ticket-in-a-box model. 2.2.1 One dimension Consider that we have \\(n\\) tickets that indicate sub-intervals of the interval \\([0,1]\\) so that ticket \\(1\\) indicates the interval \\([0,\\frac1n]\\), ticket \\(2\\) indicates the interval \\((\\frac1n,\\frac2n]\\), , and ticket \\(n\\) indicates interval \\((\\frac{n-1}{n},1]\\). So we can put the tickets in a box, shake it up and draw one out to randomly select a sub-interval of \\([0,1]\\). \\[\\text{Tickets: } \\quad ~\\quad \\fbox{$1: \\left[0,\\frac1n\\right]$}, ~\\fbox{$2: \\left(\\frac1n,\\frac2n\\right]$}, ...,~\\fbox{$n: \\left(\\frac{n-1}{n},1\\right]$}\\] Now imagine increasing the number of tickets, \\(n\\). We can imagine laying the tickets in a line that is one unit long and just sequentially cutting the tickets into smaller and smaller pieces while preserving the total length of the interval. For a very large number of tickets, the sub-intervals will be very short. Randomly choosing a sub-interval from \\([\\frac12,\\frac34]\\), for example, will depend on how many sub-intervals it is divided into, i.e. the number of tickets that that region was cut into. The number of tickets that any interval is cut into is proportional to the length of that interval. In the limit as \\(n\\rightarrow\\infty\\), the tickets will effectively indicate every point on the interval \\([0,1]\\). And we can imagine tossing a dart onto a line, and assuming that every location is equally likely, the probability of any sub-interval is equivalent to its length (relative to the length of the entire landing strip). We can think of this as a way to model selecting a number at random from the interval \\([0,1]\\). If we require any number of decimals of precision, e.g. 8 decimal places, then we can put \\(10^8\\) tickets in the box to indicate all numbers at that precision level. 2.2.2 Two or more dimensions We can generalize the same ideas to two dimensions. Take any region and cut it up into equal area tickets. The probability of selecting a ticket from sub-region \\(A\\) will be the number of tickets that \\(A\\) is cut into. This is equal to the area of \\(A\\) relative to the area of the entire region. Back to the dartboard example. The probability of the dart landing in the bulls-eye is the area of the bulls-eye divided by the area of the entire dartboard. This is an intuitive way to extend the idea of equally likely from a finite number of tickets to a continuous interval or a continuous area. We can even extend this idea to selecting randomly from a three dimensional volume. For example, if a molecule is randomly moving around in space, with the assumption that every location is equally likely, the probability of finding the molecule in any region is equal to the volume of that region divided by the entire volume of the whole space. It is important here that we start with a finite region of all possible locations. Now: what if we dont want all locations to be equally likely? This requires us to develop the mathematical theory of probability. That is what follows. 2.3 Sample Spaces and Events 2.3.1 Sample space The set of all possible outcomes of a random process or experiment is called the sample space and is denoted by \\(S\\). Examples: \\(S=\\{H,T\\}\\) for flipping a coin. \\(S=\\{1,2,3,4,5,6\\}\\) for rolling a 6-sided die. \\(S=\\{(1,1),(1,2),\\ldots,(6,5),(6,6)\\}\\) for rolling two 6-sided dice. \\(S=\\{(H,1),(H,2),\\ldots,(H,6),(T,1),(T,2),\\ldots,(T,6)\\}\\) for rolling a 6-sided die and flipping a coin. To list out a sample space in R, you can do the following. Here is an example of flipping a coin and rolling a die: coinflip=c(&quot;H&quot;,&quot;T&quot;) dieroll=1:6 S = expand.grid(coin=coinflip, die=dieroll) #&#39;die&#39; is the name of the column listing the outcomes of the die roll, # and &#39;coin&#39; is the name of the column listing the outcomes of the coin flip S ## coin die ## 1 H 1 ## 2 T 1 ## 3 H 2 ## 4 T 2 ## 5 H 3 ## 6 T 3 ## 7 H 4 ## 8 T 4 ## 9 H 5 ## 10 T 5 ## 11 H 6 ## 12 T 6 Example: flipping 3 coins: coinflip=c(&quot;H&quot;,&quot;T&quot;) S = expand.grid(coin1=coinflip, coin2=coinflip, coin3=coinflip) S ## coin1 coin2 coin3 ## 1 H H H ## 2 T H H ## 3 H T H ## 4 T T H ## 5 H H T ## 6 T H T ## 7 H T T ## 8 T T T Example: list of all 2-letter syllables with a consonant followed by a vowel. Note that many of these may not actually be valid syllables in the English language. # &#39;letters&#39; is a built-in list of all lower-case letters # &#39;LETTERS&#39; is a built-in list of all upper-case letters vowels=c(&quot;a&quot;,&quot;e&quot;,&quot;i&quot;,&quot;o&quot;,&quot;u&quot;) consonants=setdiff(letters,vowels) S = expand.grid(letter1_c=consonants, letter2_v=vowels) S ## letter1_c letter2_v ## 1 b a ## 2 c a ## 3 d a ## 4 f a ## 5 g a ## 6 h a ## 7 j a ## 8 k a ## 9 l a ## 10 m a ## 11 n a ## 12 p a ## 13 q a ## 14 r a ## 15 s a ## 16 t a ## 17 v a ## 18 w a ## 19 x a ## 20 y a ## 21 z a ## 22 b e ## 23 c e ## 24 d e ## 25 f e ## 26 g e ## 27 h e ## 28 j e ## 29 k e ## 30 l e ## 31 m e ## 32 n e ## 33 p e ## 34 q e ## 35 r e ## 36 s e ## 37 t e ## 38 v e ## 39 w e ## 40 x e ## 41 y e ## 42 z e ## 43 b i ## 44 c i ## 45 d i ## 46 f i ## 47 g i ## 48 h i ## 49 j i ## 50 k i ## 51 l i ## 52 m i ## 53 n i ## 54 p i ## 55 q i ## 56 r i ## 57 s i ## 58 t i ## 59 v i ## 60 w i ## 61 x i ## 62 y i ## 63 z i ## 64 b o ## 65 c o ## 66 d o ## 67 f o ## 68 g o ## 69 h o ## 70 j o ## 71 k o ## 72 l o ## 73 m o ## 74 n o ## 75 p o ## 76 q o ## 77 r o ## 78 s o ## 79 t o ## 80 v o ## 81 w o ## 82 x o ## 83 y o ## 84 z o ## 85 b u ## 86 c u ## 87 d u ## 88 f u ## 89 g u ## 90 h u ## 91 j u ## 92 k u ## 93 l u ## 94 m u ## 95 n u ## 96 p u ## 97 q u ## 98 r u ## 99 s u ## 100 t u ## 101 v u ## 102 w u ## 103 x u ## 104 y u ## 105 z u 2.3.2 Events An event is a set of outcomes. Events are usually denoted by capital letters, e.g. \\(A\\), \\(B\\), \\(C\\), etc. They can be described in words or with the outcomes listed as a set in {} curly brackets. Examples: Flipping a coin. Event \\(A=\\) {heads}. Event \\(B=\\{T\\}\\). Flipping two coins. Event \\(A=\\) no heads, Event \\(B=\\) {1 head and 1 tail}. Event \\(C=\\) {TT,TH} = {first coin is tails}. Rolling a 6-sided die. Event \\(A=\\) {even}. Event \\(B=\\){greater than 4}={5,6}. 2.4 Set Operations Here is a review of the primary operations we will use with sets. Union: all outcomes that are in the events, \\(A\\cup B\\) Intersection: all outcomes common to the events being intersected, \\(A\\cap B\\) Complement: all outcomes not in the given event, \\(A^c\\) Set Difference: all outcomes in one event that are not in the other, \\(A-B\\), note that \\(A-B=A\\cap B^c\\) Example: Let \\(A=\\{1,2,3,4\\}\\), and \\(B=\\{2,4,6\\}\\) be events from sample space \\(S=\\{1,2,3,4,5,6\\}\\). \\(A\\cup B=\\{1,2,3,4,6\\}\\), R command: union(A,B) \\(A\\cap B=\\{2,4\\}\\), R command: intersect(A,B) \\(A^c=\\{5,6\\}\\) \\(A-B=\\{1,3\\}\\) Note that \\(A^c=S-A\\). R command: setdiff(A,B) Two sets are called disjoint if they have empty intersection \\(A\\cap B=\\emptyset\\). When sets represent events for a random experiment and are disjoint, we call them mutually exclusive. In order to do a set complement in R, we will need to specify the sample space and to a set difference: \\(A^c=S-A=\\)setdiff(S,A). Example with rolling a 6-sided die: S=1:6 # sample space is {1,2,3,4,5,6} A=c(2,4,6) B=c(1,2,3,4) union(A,B) ## [1] 2 4 6 1 3 intersect(A,B) ## [1] 2 4 setdiff(A,B) # A-B ## [1] 6 setdiff(S,A) # A complement ## [1] 1 3 5 We can take unions and intersections of more than two sets also: \\[\\bigcup_{k=1}^n A_k=A_1\\cup A_2\\cup\\cdots \\cup A_n\\] \\[\\bigcap_{k=1}^n A_k=A_1\\cap A_2\\cap\\cdots \\cap A_n\\] If we have a countably infinite number of events, \\(A_1,A_2,\\ldots\\) where there is no end to the list of events, so-to-speak, we can still take their union and intersection: \\[\\bigcup_{k=1}^\\infty A_k=A_1\\cup A_2\\cup\\cdots\\] \\[\\bigcap_{k=1}^\\infty A_k=A_1\\cap A_2\\cap\\cdots \\] It may be difficult to imagine what an infinite list of events looks like or how it could be used. Usually infinite unions and intersections are only encountered in more advanced probability and statistics courses, but at this level, you just need to be exposed to the idea. The most important thing is to understand the union and intersection notation for a finite list of events. Associative Laws: \\((A\\cup B)\\cup C = A\\cup (B\\cup C)\\) \\((A\\cap B)\\cap C = A\\cap (B\\cap C)\\) Distributive Laws: \\((A\\cup B)\\cap C = (A\\cap C)\\cup (B\\cap C)\\) \\((A\\cap B)\\cup C = (A\\cup C)\\cap (B\\cup C)\\) DeMorgans Laws: \\((A\\cup B)^c=A^c \\cap B^c\\) \\((A\\cap B)^c=A^c \\cup B^c\\) Inclusion/exclusion rules (for size of sets): \\(|A\\cup B| =|A|+|B|-|A\\cap B|\\) \\(\\begin{aligned} |A\\cup B\\cup C| =&amp;|A|+|B|+|C|\\\\ &amp;-|A\\cap B|-|A\\cap C|-|B\\cap C|\\\\ &amp;+|A\\cap B\\cap C| \\end{aligned}\\) Note that \\(|A|\\) denotes the size of set \\(A\\), which is the number of outcomes in the set. In R, we can accomplish this for a simple list/vector with length(A), e.g. A=c(2,4,6); length(A). If \\(A\\) is not merely a set, list, or vector but has a table-like structure, then instead of length(A), we might want to use nrow(A) to get the number of rows or ncol(A) to get the number of columns. 2.5 Venn Diagrams Graphing the Venn diagrams in R will require package call VennDiagram. To install that package, issue the command: install.packages(\"VennDiagram\") After the package is installed, well have to load it into R using require(VennDiagram) or library(VennDiagram). You will likely see some output on the screen in red colored text showing you what functions or packages have been loaded and possibly giving some warning messages. You can mostly ignore this text unless it explicitly gives an error saying the package is not found or was not loaded. Usually, it will be clear if that is the case. # note that before issuing this command, # you need to have the &quot;VennDiagram&quot;&quot; package installed&quot; # you caninstall it with `install.packages(&quot;VennDiagram&quot;)` library(VennDiagram) Now that the Venn diagram package and its associated commands are loaded, we can draw a Venn diagram: grid.newpage() draw.pairwise.venn(area1 = 25, area2 = 30, cross.area = 12, category = c(&quot;A&quot;,&quot;B&quot;), fill=c(rgb(1,0,0),rgb(0,1,0)), alpha=c(0.25,0.25)) Youll notice that R prints to the screen some information about the polygons that have been drawn. You dont need to pay attention to this information about the polygons. From the figure above, we can see that \\(|A|\\)=25, \\(|B|\\)=30, and \\(|A\\cap B|\\)=12. Venn diagrams can also be drawn with three sets: grid.newpage() draw.triple.venn(area1 = 22, area2 = 20, area3 = 13, n12 = 11, n23 = 4, n13 = 5, n123 = 1, category = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), fill = c(&quot;skyblue&quot;, &quot;pink1&quot;, &quot;mediumorchid&quot;)) 2.6 Counting, permutations, and combinations In order to calculate probabilities, we will often need to be able to count the number of outcomes in the sample space and in particular events. 2.6.1 Multiplication rule If we perform an experiment where the outcome has two different parts (e.g. a coin flip and a die roll) then the total number of possibilities is the product of the number of possibilities for each part: \\(n=n_1\\cdot n_2\\). If there are many parts, e.g. roll several dice with different numbers of sides, then we multiply the number of possibilities for each: \\[n=n_1\\cdot n_2\\cdot \\cdots n_k.\\] Example: If we roll a 6-sided die, flip a coin, roll a 20-sided die, and roll two 4-sided dice, then the total number of outcomes is \\(n=6\\cdot 2 \\cdot 20 \\cdot4\\cdot4\\). This rule can also work, if instead of having multiple distinct parts (like several physical objects), the outcome of an experiment could have multiple observable characteristics that need to be determined. Assume we have a box that contains tokens, and each token is painted a specific color (green, blue, or red), and labeled with a number (1,2,3,4,or 5) and a letter (A or B). Assume that there are 10 tokens of each color so that every combination has exactly one token. Lets list out the sample space using R. token_colors=c(&quot;green&quot;,&quot;blue&quot;,&quot;red&quot;) token_numbers=1:5 token_letters=c(&quot;A&quot;,&quot;B&quot;) S = expand.grid(color=token_colors, number=token_numbers, letter=token_letters) nrow(S) ## [1] 30 Now we can see that the size of the sample space is \\[ \\begin{aligned} |S|&amp;=\\{\\text{# of colors}\\}\\cdot\\{\\text{# of numbers}\\}\\cdot\\{\\text{# of letters}\\}\\\\ &amp;=3\\cdot5\\cdot2=30. \\end{aligned} \\] It is important here that we have the token attributes are evenly distributed so-to-speak, i.e. if each color had a different list of labels such as 3 numerical labels for green tokens and 5 numerical labels for red tokens, then this technique would not work exactly as stated. Each specific attribute must have the same number of repetitions of the other attributes. 2.6.2 \\(n^k\\) Assume we have \\(n\\) distinct objects to choose from. These \\(n\\) objects could be the numbers 1 to 10, 5 types of drinks, or 20 students in a class. We will select \\(k\\) of these objects in a variety of different ways. If we select the \\(k\\) objects one at a time, replacing each as we go, then the total number of possibilities is \\(n^k\\). Here we are allowing for the same object selected multiple times (repetition), and we are concerned with the order of the selection. This is an application of the multiplication rule. Now let us assume we have \\(n\\) objects to arrange in order. Well make \\(k\\) spaces which we will fill with the objects. \\[ \\underset{\\text{obj. $1$}}{\\underbrace{\\qquad\\quad}} \\ \\underset{\\text{obj. 2}}{\\underbrace{\\qquad\\quad}} \\ \\underset{\\text{obj. 3}}{\\underbrace{\\qquad\\quad}} \\ \\cdots \\ \\underset{\\text{obj. $k-1$}}{\\underbrace{\\qquad\\quad}} \\ \\underset{\\text{obj. $k$}}{\\underbrace{\\qquad\\quad}} \\] For each space, we have all \\(n\\) objects to choose from since we will draw an object, record which we have drawn, then replace that object and draw for the next space. \\[ \\overset{n}{\\underset{\\text{obj. $1$}}{\\underbrace{\\qquad\\quad}}} \\ \\overset{n}{\\underset{\\text{obj. 2}}{\\underbrace{\\qquad\\quad}}} \\ \\overset{n}{\\underset{\\text{obj. 3}}{\\underbrace{\\qquad\\quad}}} \\ \\cdots \\ \\overset{n}{\\underset{\\text{obj. $k-1$}}{\\underbrace{\\qquad\\quad}}} \\ \\overset{n}{\\underset{\\text{obj. $k$}}{\\underbrace{\\qquad\\quad}}} \\] 2.6.3 Factorials Now let us assume we have \\(n\\) objects to arrange in order. The difference is that now, each time we select an object to fill a space, we will not replace it, thus will reduce our total number of options for the next space. This will result in an ordered arrangement of \\(n\\) objects. This is often called a permutation of \\(n\\) distinguishable objects. They are called distinguishable since swapping any two results in a distinct ordering or permutation. Again well make \\(n\\) spaces which we will fill with the objects. \\[ \\underset{\\text{obj. $1$}}{\\underbrace{\\qquad\\quad}} \\ \\underset{\\text{obj. 2}}{\\underbrace{\\qquad\\quad}} \\ \\underset{\\text{obj. 3}}{\\underbrace{\\qquad\\quad}} \\ \\cdots \\ \\underset{\\text{obj. $n-1$}}{\\underbrace{\\qquad\\quad}} \\ \\underset{\\text{obj. $n$}}{\\underbrace{\\qquad\\quad}} \\] We will count how many choices we have to fill each space. The first space has \\(n\\) total choices. After putting an object in the first space, we only have \\(n-1\\) objects left to select from, since we are drawing without replacement. Our available choices decrease by one each time we select an object to put in a space. The result is as follows. \\[ \\overset{n}{\\underset{\\text{obj. $1$}}{\\underbrace{\\qquad\\quad}}} \\ \\overset{n-1}{\\underset{\\text{obj. 2}}{\\underbrace{\\qquad\\quad}}} \\ \\overset{n-2}{\\underset{\\text{obj. 3}}{\\underbrace{\\qquad\\quad}}} \\ \\cdots \\ \\overset{2}{\\underset{\\text{obj. $n-1$}}{\\underbrace{\\qquad\\quad}}} \\ \\overset{1}{\\underset{\\text{obj. $n$}}{\\underbrace{\\qquad\\quad}}} \\] Then we use the multiplication rule. So the total number of ways to arrange them is the product \\(n(n-1)(n-2)\\cdots3\\cdot2\\cdot1\\). This is denoted by a factorial: \\[n!=n\\cdot(n-1)\\cdot(n-2)\\cdots3\\cdot2\\cdot1\\] Note that by definition, \\(0!=1\\). A factorial in R can be evaluated as factorial(n). factorial(0) ## [1] 1 factorial(1) ## [1] 1 factorial(2) ## [1] 2 factorial(c(0,1,2,3,4)) ## [1] 1 1 2 6 24 factorial(10) ## [1] 3628800 2.6.4 Permutation Now we are still starting with \\(n\\) total objects to choose from. If we select \\(k\\) objects one at a time, but do not replace them as we go and we are concerned with the order we draw them in, then a permutation results, and there are \\(\\frac{n!}{(n-k)!}\\) possibilities. This is denoted differently in different texts as \\(_nP_k\\), \\(P_{n,k}\\), or \\(P_{k,n}\\) for \\(k\\le n\\). I recommend always writing it out as factorials. Here it is important that \\(0\\le k\\le n\\). The best way to evaluate a permutation in R is with factorials # permute 3 out of 10 factorial(10)/factorial(10-3) ## [1] 720 A permutation can be illustrated just as above, by listing out \\(k\\) spaces and writing in the number of possibilities for each space, but the last space will not have only 1 possibility. \\[ \\overset{n}{\\underset{\\text{obj. $1$}}{\\underbrace{\\qquad\\quad}}} \\ \\overset{n-1}{\\underset{\\text{obj. 2}}{\\underbrace{\\qquad\\quad}}} \\ \\overset{n-2}{\\underset{\\text{obj. 3}}{\\underbrace{\\qquad\\quad}}} \\ \\cdots \\ \\overset{n-(k-2)}{\\underset{\\text{obj. $k-1$}}{\\underbrace{\\qquad\\quad}}} \\ \\overset{n-(k-1)}{\\underset{\\text{obj. $k$}}{\\underbrace{\\qquad\\quad}}} \\] The result is identical to multiplying from \\(n\\) all the way down to 1 but then dividing by \\((n-k)!\\) as is done in the factorial formula for a permutation. 2.6.5 Combination If we select \\(k\\) objects out of \\(n\\) choices (again with \\(0\\le k\\le n\\)) either one at a time without replacement and do not care about the order they are drawn in, or if we just drew all \\(k\\) at once, then a combination results. The total number of possibilities is \\({n\\choose k} = \\frac{n!}{k!(n-k)!}\\) for \\(k\\le n\\). Again, this is variously denoted as \\(_nC_k\\), \\(C_{n,k}\\), or \\(C_{k,n}\\). I recommend the parentheses notation or just writing it out as factorials. The best way to evaluate a combination in R is with factorials # choose 3 out of 10 factorial(10)/factorial(10-3)/factorial(3) ## [1] 120 Alternatively, we can use the choose(n,k) R command: # choose 3 out of 10 choose(10,3) ## [1] 120 2.6.6 Summary of counting Examples: Suppose we have a combination lock with a turn dial that has numbers 1 to 36 on it. A combination consists of three numbers in sequence.Assume that we can have all three numbers the same as you still have to make a complete 360 degree rotation between numbers. Then the total number of allowable combinations is \\(36^3\\). Suppose 150 people are running a race where first, second, and third place will be awarded. How many possible outcomes are there? Here is a situation where order clearly matters, and beyond third place, we will not concern ourselves with the order of the contestants. This is a permutation, permute 3 out of 100, so there are \\(\\frac{150!}{(150-3)!}=150\\cdot 149\\cdot 148\\) possible outcomes. Suppose we have a class of 35 students, and 5 of them will be selected to form a group for a project. Here we are not concerned with the order they are selected in, we just want to know which students will be chosen for the group. We are creating a subset of the total class of students. This is a combination so there are \\({35\\choose5}\\) total possibilities. Consider the last example above. If we were instead going to assign the 5 students distinct roles in the group, e.g. group leader, note taker, researcher, materials gatherer, and data analyzer, then we would be forming different groups even if we had the same 5 students but assigned the roles differently among them. So this becomes a permutation, and there would be \\(\\frac{35!}{(35-5)!}=\\frac{35!}{30!}=35\\cdot34\\cdot33\\cdot32\\cdot31\\) total possible groups (different assignment of group roles results in a different group here). Summary table for the counting methods discussed above: without replacement with replacement order matters permutation: \\(\\frac{n!}{(n-k)!}\\) \\(n^k\\) order doesnt matter combination: \\({n\\choose k}=\\frac{n!}{k!(n-k)!}\\) (special case) 2.6.7 Special case: with replacement, order doesnt matter: the multiset Choosing with replacement, but order does not matter is the most complicated case. Consider throwing \\(k\\) balls into \\(n\\) boxes. If we have the boxes labeled and want to know how many balls are in each box, e.g. 5 in the first box, none in the second box, 2 is the third box, etc., then the number of outcomes is given by \\({n-1+k\\choose k}\\). In this case \\(k\\) and \\(n\\) can be any numbers and \\(k\\le n\\) is not required. Here is an example of tossing 10 balls into 7 boxes. We will pretend the boxes are all adjacent so that among the 7 boxes, there are actually only 8 dividers or walls since adjacent boxes share a wall. We need to arrange the inner 6 dividers between the boxes and the 10 balls. We only need the inner 6 dividers between the boxes because the locations of the outermost left and right walls do not need to be determined. \\[ \\underset{\\text{box 1}}{\\underbrace{\\qquad}}| \\underset{\\text{box 2}}{\\underbrace{\\qquad}}| \\underset{\\text{box 3}}{\\underbrace{\\qquad}}| \\underset{\\text{box 4}}{\\underbrace{\\qquad}}| \\underset{\\text{box 5}}{\\underbrace{\\qquad}}| \\underset{\\text{box 6}}{\\underbrace{\\qquad}}| \\underset{\\text{box 7}}{\\underbrace{\\qquad}} \\] Now we toss the balls in either all at once or one-by-one, it doesnt matter because we dont care for the order of the balls. Assume they land like this: \\[ \\underset{\\text{box 1}}{\\underbrace{\\ \\bullet\\bullet \\ }}| \\underset{\\text{box 2}}{\\underbrace{\\bullet\\bullet\\bullet}}| \\underset{\\text{box 3}}{\\underbrace{\\qquad}}| \\underset{\\text{box 4}}{\\underbrace{\\ \\ \\bullet \\ \\ }}| \\underset{\\text{box 5}}{\\underbrace{\\bullet\\bullet\\bullet\\bullet}}| \\underset{\\text{box 6}}{\\underbrace{\\qquad}}| \\underset{\\text{box 7}}{\\underbrace{\\qquad}} \\] Now we will drop the box labels, and this particular outcome will look as follows. \\[\\cdot\\cdot|\\cdot\\cdot\\cdot||\\cdot|\\cdot\\cdot\\cdot\\cdot||\\] In order to determine all possible outcomes here, we just need to figure out how many ways we can shuffle around the balls and inner dividers. If there are \\(n\\) boxes, then there are \\(n-1\\) inner dividers. So we have \\(n-1\\) dividers and \\(k\\) balls. This is a total of \\(n-1+k\\) symbols to rearrange. We dont care what order they are arranged in though, since the balls are all identical as are the dividers. So we will use a combination. There are a total of \\(n-1+k\\) slots to fill with a symbol, and we will just choose the \\(k\\) slots to put the balls. Then the divider symbols automatically go in the remaining \\(n-1\\) spaces. Here is is illustrated for a simple case, 4 balls and 3 boxes. Here are the boxes with inner dividers: \\[ \\underset{\\text{box 1}}{\\underbrace{\\qquad}}| \\underset{\\text{box 2}}{\\underbrace{\\qquad}}| \\underset{\\text{box 3}}{\\underbrace{\\qquad}} \\] Here are the spaces that we will fill with balls and inner dividers: \\[ \\underset{\\text{space 1}}{\\underbrace{\\qquad}} \\ \\ \\underset{\\text{space 2}}{\\underbrace{\\qquad}} \\ \\ \\underset{\\text{space 3}}{\\underbrace{\\qquad}} \\ \\ \\underset{\\text{space 4}}{\\underbrace{\\qquad}} \\ \\ \\underset{\\text{space 5}}{\\underbrace{\\qquad}} \\ \\ \\underset{\\text{space 6}}{\\underbrace{\\qquad}} \\] Here is a possible outcome. Lets just drop the 4 balls in some spaces: \\[ \\underset{\\text{space 1}}{\\underbrace{ \\ \\ \\bullet \\ \\ }} \\ \\ \\underset{\\text{space 2}}{\\underbrace{\\qquad}} \\ \\ \\underset{\\text{space 3}}{\\underbrace{ \\ \\ \\bullet \\ \\ }} \\ \\ \\underset{\\text{space 4}}{\\underbrace{ \\ \\ \\bullet \\ \\ }} \\ \\ \\underset{\\text{space 5}}{\\underbrace{ \\ \\ \\bullet \\ \\ }} \\ \\ \\underset{\\text{space 6}}{\\underbrace{\\qquad}} \\] Now the remaining 2 spaces will be filled with the 2 inner dividers: \\[ \\underbrace{ \\ \\ \\bullet \\ \\ }_\\text{space 1} \\ \\ \\underbrace{\\ \\ \\ | \\ \\ \\ }_\\text{space 2} \\ \\ \\underbrace{ \\ \\ \\bullet \\ \\ }_\\text{space 3} \\ \\ \\underbrace{ \\ \\ \\bullet \\ \\ }_\\text{space 4} \\ \\ \\underbrace{ \\ \\ \\bullet \\ \\ }_\\text{space 5} \\ \\ \\underbrace{ \\ \\ \\ | \\ \\ \\ }_\\text{space 6} \\] \\[ \\underbrace{ \\qquad\\qquad\\qquad \\ }_\\text{box 1} \\ \\underbrace{ \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad \\ \\ }_\\text{box 2} \\ \\underbrace{ \\qquad\\quad \\ }_\\text{box 3} \\] So that box one has 1 ball, box two has 3 balls, and box three has 0 balls. 2.7 Probability 2.7.1 Equally likely outcomes Many sample spaces contain a finite number of outcomes that are equally likely. This means that the probability of an event is defined as the number of outcomes in that event divided by the total number of outcomes in the sample space. \\[P(A)=\\frac{|A|}{|S|}\\] Examples: Flipping 3 fair coins: \\(|S|=2^3=8\\). Let event \\(A=\\){at least 2 heads}. So we can list all outcomes in \\(A=\\{HHH,HHT,HTH,THH\\}\\) thus \\(|A|=4\\) so that \\(P(A)=\\frac48=50\\%\\). Rolling a fair 6-sided die: \\(|S|=6\\). Let \\(A=\\){odd}\\(=\\{1,3,5\\}\\) and \\(B=\\){greater than 5}\\(=\\{6\\}\\). Then \\(|A|=3\\) and \\(|B|=1\\) so that \\(P(A)=3/6\\) and \\(P(B)=1/6\\). 2.7.2 General probability theory Axioms of probability: \\(0\\leq P(A)\\) for any event \\(A\\). \\(P(S)=1\\) for sample space \\(S\\). If \\(A_1,A_2,\\ldots\\) are mutually exclusive events (all intersections are empty), then \\(P(A_1\\cup A_2\\cup \\cdots)=P(A_i)+P(A_2)+\\cdots\\). Note that the last axiom can involve a finite number of events or even an infinite number of events. Some consequences of the above axioms: \\(P(\\emptyset)=0\\) \\(0\\leq P(A)\\le 1\\) for any event \\(A\\). If \\(A\\subset B\\), then \\(P(A)\\le P(B)\\). \\(P(A^c)=1-P(A)\\). \\(P(A-B)=P(A \\cap B^c)=P(A)-P(A\\cap B)\\) Inclusion/exclusion rules: \\(P(A\\cup B) =P(A)+P(B)-P(A\\cap B)\\) \\(\\begin{aligned}P(A\\cup B\\cup C) =&amp;P(A)+P(B)+P(C)\\\\ &amp;-P(A\\cap B)-P(A\\cap C)-P(B\\cap C)\\\\ &amp;+P(A\\cap B\\cap C)\\end{aligned}\\) Example: Assume that \\(P(A)=0.6\\), \\(P(B)=0.5\\) and \\(P(A\\cap B)=0.2\\), then we can calculate: \\(P(A\\cup B)=0.6+0.5-0.2=0.9\\) \\(P(A^c)=1-0.6=0.4\\). \\(\\begin{aligned}P(A-B)&amp;=P(A \\cap B^c)\\\\ &amp;=P(A)-P(A\\cap B)=0.6-0.2=0.4. \\end{aligned}\\) \\(\\begin{aligned}P(A^c \\cap B^c)&amp;=P((A\\cup B)^c) \\\\ &amp;=1-P(A\\cup B)=1-0.9=0.1.\\end{aligned}\\) Lets draw a Venn diagram with the probabilities: grid.newpage() draw.pairwise.venn(area1 = 0.6, area2 = 0.5, cross.area = 0.2, category = c(&quot;A&quot;,&quot;B&quot;), fill=c(rgb(1,0,0),rgb(0,1,0)), alpha=c(0.25,0.25)) 2.7.3 Independence Events \\(A\\) and \\(B\\) are called independent if \\[P(A\\cap B)=P(A)\\cdot P(B).\\] This is not to be confused with being disjoint (mutually exclusive), which means that \\(P(A\\cap B)=0\\). Independence is a completely new unrelated concept. One way to understand independence is that the probability of either events occurring does not depend on whether or not the other event has occurred. Example: Rolling a fair 6-sided die: Let \\(A=\\){even}\\(=\\{2,4,6\\}\\) and \\(B=\\){less than five}\\(=\\{1,2,3,4\\}\\). Then we have that \\(A\\cap B=\\{2,4\\}\\), \\(P(A)=\\frac12\\), \\(P(B)=\\frac23\\), and \\(P(A\\cap B)=\\frac13=\\frac12\\cdot\\frac23\\) thus \\(A\\) and \\(B\\) are independent. Here is a way to understand this. If we just roll the die, the probability of it being even (event \\(A\\)) is \\(\\frac12\\). This is if the entire sample space is the set of possibilities, but if instead \\(B\\) was our entire set of possible outcomes, then the probability of $A, being even, is still \\(\\frac12\\). We can rearrange the formula like this: \\[P(A)=\\frac{P(A\\cap B)}{P(B)}.\\] So for equally likely events, \\(A\\) and \\(B\\) are independent if \\[P(A)=\\frac{|A|}{|S|}=\\frac{|A\\cap B|}{|B|}.\\] 2.7.4 Conditional probability When conducting a random experiment, sometimes we may have some information about the outcome but not know it completely. Imagine rolling a die behind a barrier so you cannot see the result, but another person can. Under normal circumstances, \\(P(2)=\\frac16\\). If the person who can see the result tells you it is an even number (assume they are truthful), then 2 is one out of only three possibilities now. So, in this sense \\(P(\\{2 \\text{ given that we know it is even}\\})=\\frac13\\). This is a conditional probability. We are conditioning our calculation of the probability on some given information. The conditional probability of \\(A\\) given \\(B\\) is given by: \\[P(A\\mid B)=\\frac{P(A\\cap B)}{P(B)}.\\] Event \\(B\\) is the condition, and we can think of it as meaning that we know that event \\(B\\) has occurred, but we dont yet have enough information to determine whether or not \\(A\\) has occurred. \\(P(A\\mid B)\\) is the probability of \\(A\\) occurring given that we know \\(B\\) has occurred. An intuitive way to understand conditional probability is the following. Think of events \\(A\\) and \\(B\\) indicating areas for a dart to land. Also assume that they are drawn proportional to their respective probabilities. You can envision a Venn diagram where the area of each region divided by the area of the entire diagram is equal to the probability of that region (as discussed in the beginning of this chapter). The probability of the dart landing in \\(A\\) is exactly the area of \\(A\\) relative to the entire region of possible landing zones. Conditioning on event \\(B\\) means that we know that the dart lands in region \\(B\\). So the probability that it also lands in region \\(A\\) is going to be the area of the intersection of \\(A\\) and \\(B\\) divided by the area of all possible landing zones, which is now just \\(B\\). So \\(P(A\\cap B)/P(B)\\). Example: Consider our opening example in this section with events \\(A=\\{2\\}\\) and \\(B=\\){even}. \\[ P(A\\mid B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{\\frac16}{\\frac36}=\\frac13 \\] 2.7.4.1 Conditional probability and independence Note that if events are independent, then conditioning on one or the other does not change probabilities of either occurring: \\[A,B \\text{ are independent if and only if } \\\\ P(A\\mid B)=P(A) \\text{ and } P(B\\mid A)=P(B) \\] Conditional probability can be used to rewrite the probability of an intersection in two ways: \\[P(A\\cap B)=P(A\\mid B)\\cdot P(B)=P(B\\mid A)\\cdot P(A).\\] This is a very useful thing to understand. It will be used when we discuss Bayes theorem. 2.7.4.2 Set operations and probability axioms with conditional probability Conditional probability behaves just like regular probability. All the set operations, rules, and axioms of probability apply to conditional probability as well. \\(P(\\emptyset\\mid B)=0\\) \\(0\\leq P(A\\mid B)\\le 1\\) for any event \\(A\\). If \\(A\\subset B\\), then \\(P(A\\mid C)\\le P(B\\mid C)\\). \\(P(A^c\\mid B)=1-P(A\\mid B)\\). \\(P(A-B\\mid C)=P(A \\cap B^c\\mid C)=P(A\\mid C)-P(A\\cap B\\mid C)\\) Inclusion/exclusion rules: \\(P(A\\cup B\\mid C) =P(A\\mid C)+P(B\\mid C)-P(A\\cap B\\mid C)\\) Example: Rolling a fair 6-sided die: Let \\(A=\\){even}\\(=\\{2,4,6\\}\\) and \\(B=\\){one}\\(=\\{1\\}\\). Then we have that \\(A\\cap B=\\emptyset\\), \\(P(A)=\\frac12\\), \\(P(B)=\\frac16\\), and \\(P(A\\cap B)=0\\). If we know that \\(B\\) has occurred, what is the probability that \\(A\\) has also occurred? Zero! They are mutually exclusive! Example: Rolling a fair 6-sided die: Let \\(A=\\){odd}\\(=\\{1,3,5\\}\\) and \\(B=\\){less than four}\\(=\\{1,2,3\\}\\). Then we have that \\(A\\cap B=\\{1,3\\}\\), \\(P(A)=\\frac12\\), \\(P(B)=\\frac12\\), and \\(P(A\\cap B)=\\frac13\\). If we know that \\(B\\) has occurred, what is the probability that \\(A\\) has also occurred? \\[P(A\\mid B)=\\frac23=\\frac{\\text{# of outcomes in $A$ and $B$}}{\\text{# of outcomes in $B$}}.\\] In both of the examples above, \\(A\\) and \\(B\\) are not independent. Here is an example with independent events: Example: Rolling a fair 6-sided die: Let \\(A=\\){even}\\(=\\{2,4,6\\}\\) and \\(B=\\){less than five}\\(=\\{1,2,3,4\\}\\). Then we have that \\(A\\cap B=\\{2,4\\}\\), \\(P(A)=\\frac12\\), \\(P(B)=\\frac23\\), and \\(P(A\\cap B)=\\frac13=\\frac12\\cdot\\frac23\\) thus \\(A\\) and \\(B\\) are independent. Lets calculate the conditional probability: \\(P(A\\mid B)=\\frac{|A\\cap B|}{|B|}=\\frac{2}{4}=\\frac12=P(A)\\). 2.7.5 Partitions Now we will divide up the sample space into a number of disjoint sets (mutually exclusive events). The collection of sets that the sample space is broken up into is called a partition. It is important that this partition of sets collectively covers the entire sample space (that is the nature of a partition). The simplest partition is \\(S=A\\cup A^c\\). A partition of a sample space is a collection of events \\(A_1,A_2,A_3,\\ldots,A_n\\) such that \\(A_i\\cap A_j=\\emptyset\\) for any \\(i,j\\) and \\(S=\\bigcup_{i=1}^n A_i=A_1 \\cup A_2 \\cup A_3 \\cup \\cdots \\cup A_n\\). Example: rolling a 6-sided die. Let \\(A=\\{2,4,6\\}, B=\\{1,3\\}, C=\\{5\\}\\). Then \\(A,B,C\\) form a partition of the sample space \\(S=\\{1,2,3,4,5,6\\}\\). Example: If our random experiment is producing an item on some industrial production line, then we might be interested in whether the object passes a quality control check or not. This forms a partition of all possible outcomes: \\(Q=\\){passes quality check}, and \\(Q^c=\\){fails quality control check}. If we have a partition \\(A_1,A_2,A_3,\\ldots,A_n\\) and any event \\(B\\), then we can break \\(B\\) up into pieces over the partition: \\[B=\\bigcup_{i=1}^n (B\\cap A_i).\\] Notice that the \\(B\\cap A_i\\) are all disjoint. This is true because \\(B\\cap A_i \\subset A_i\\) and the \\(A_i\\) are all disjoint. We can thus calculate the probability of \\(B\\) as \\[P(B)=P\\left(\\bigcup_{i=1}^n (B\\cap A_i)\\right)=\\sum_{i=1}^n P(B\\cap A_i).\\] Example: Rolling a 6-sided die. Let \\(A_1=\\{1,2,3\\}, A_2=\\{4,5\\}, A_3=\\{6\\}\\). Then \\(A_1,A_2,A_3\\) form a partition of the sample space \\(S=\\{1,2,3,4,5,6\\}\\). Consider event \\(B=\\{\\text{odd}\\}=\\{1,3,5\\}\\). We then have that \\(B\\cap A_1=\\{1,3\\}\\), \\(B\\cap A_2=\\{5\\}\\), \\(B\\cap A_3=\\{\\}=\\emptyset\\). Thus \\(P(B)=\\frac36=P(B\\cap A_1)+P(B\\cap A_2)+P(B\\cap A_3)=\\frac26+\\frac16+0\\). 2.7.6 Bayes Theorem Bayes theorem uses the ideas of partitioning the sample space and conditional probability. Suppose we have an industrial production line machine. A certain number of items produced will be defective and the rest will be non-defective. Additionally sometimes a quality control inspection will not catch that a particular item is defective, or a non-defective item will be incorrectly identified as defective. Assume that: Out of every 1,000 items produced, 20 will be defective. Out of every 20 defective items, quality control will correctly identify 19 of them. Out of every 980 non-defective items, quality control will incorrectly determine 50 of them to be defective. Here is what we want to determine: If an item fails the quality control inspection (i.e. it was determined by quality control to be defective), what is the probability that it is actually defective? Recall that quality control misclassifies items sometimes. We create events: \\(D=\\) {item is defective}, thus \\(D^c=\\) {item is non-defective}. \\(F=\\) quality control control determines item to be defective (fail quality control inspection). The question is to calculate the conditional probability \\(P(D\\mid F)\\). We know the reverse conditional probability \\(P(F\\mid D)=\\frac{19}{20}\\) (probability of failing the quality control inspection given that the item is defective). We also know that \\(P(D)=\\frac{20}{1000}\\) (probability of defective), and \\(P(F\\mid D^c)=\\frac{50}{980}\\) (probability a non-defective item fails quality control inspection  a quality control error). First we use the conditional probability formula: \\[P(D\\mid F)=\\frac{P(D\\cap F)}{P(F)}.\\] Then we write the intersection in the numerator as the reverse conditional probability: \\[P(D\\mid F)=\\frac{P(F\\mid D) P(D)}{P(F)}.\\] Notice that this is useful since we know the value for \\(P(F\\mid D)\\) but do not know \\(P(D\\mid F)\\). Then we write the denominator \\(P(F)\\) in terms of the partition \\(D,D^c\\). \\[P(D\\mid F)=\\frac{P(F\\mid D) P(D)}{P(F\\cap D)+P(F\\cap D^c)}.\\] Then we rewrite the intersections in the denominator as conditional probabilities with the partition \\(D,D^c\\). \\[P(D\\mid F)=\\frac{P(F\\mid D) P(D)}{P(F\\mid D)P(D)+P(F\\mid D^c)P(D^c)}.\\] Now we know the values for all of the probabilities in the formula on the right! \\[ \\begin{aligned} P(D\\mid F)&amp;=\\frac{P(F\\mid D) P(D)}{P(F\\mid D)P(D)+P(F\\mid D^c)P(D^c)}\\\\ &amp;=\\frac{\\frac{19}{20} \\cdot \\frac{20}{1000}}{\\frac{19}{20} \\cdot\\frac{20}{1000}+\\frac{50}{980} \\cdot \\left(1-\\frac{20}{1000}\\right)}. \\end{aligned} \\] Lets take a second look at this problem. Here are the initial numbers given again: Out of every 1,000 items produced, 20 will be defective. Out of every 20 defective items, quality control will correctly identify 19 of them. Out of every 980 non-defective items, quality control will incorrectly determine 50 of them to be defective. We can actually determine \\(P(D\\mid F)\\) directly without much effort. Out of the 1,000 items, there will be a total of \\(19+50=69\\) that will fail the quality control check, and 19 of these are actually defective. Thus the probability of an item being defective given that it failed the quality control check is \\(\\frac{19}{69}\\approx 27.5\\%\\). Notice that in this situation, a majority of items that fail quality control check are actually non-defective! Interesting! The general form of Bayes theorem is given for event \\(B\\) and partition \\(A_1,A_2,\\ldots,A_n\\). \\[ \\begin{aligned} P(A_k\\mid B)&amp;=\\frac{P(B\\cap A_k)}{P(B)}\\\\ &amp;=\\frac{P(B\\cap A_k)}{\\sum_{i=1}^n P(B\\cap A_i)}\\\\ &amp;=\\frac{P(B\\mid A_k)P(A_k)}{\\sum_{i=1}^n P(B\\mid A_i)P(A_i)}. \\end{aligned} \\] A very common form of Bayes theorem is when the partition is \\(A,A^c\\). \\[ P(A\\mid B)=\\frac{P(B\\mid A)P(A)}{P(B\\mid A)P(A)+P(B\\mid A^c)P(A^c)}. \\] 2.7.6.1 Alternative Bayes example If instead of being given the information in terms of whole number ratios as above, sometimes Bayes theorem problems are given with fraction, decimal, or percentage probabilities. Here is a similar problem with information given as percentages: The probability of an item being defective is \\(3\\%\\) If an item is defective, it will fail a quality control check \\(96\\%\\) of the time. If an item is non-defective, it will fail a quality control check \\(2\\%\\) of the time. So now we have \\(P(D)=0.03\\), \\(P(D^c)=0.97\\), \\(P(F\\mid D)=0.96\\), and \\(P(F\\mid D^c)=0.02\\). (Note that these are different from the previous example.) We can calculate similarly: \\[ \\begin{aligned} P(D\\mid F)&amp;=\\frac{P(F\\mid D) P(D)}{P(F\\mid D)P(D)+P(F\\mid D^c)P(D^c)}\\\\ &amp;=\\frac{(0.96) \\cdot (0.03)}{(0.96) \\cdot (0.03)+(0.02) \\cdot (0.97)} \\\\ &amp;\\approx 0.5975. \\end{aligned} \\] 2.7.6.2 A trick for solving Bayes problems It is common for students to find it more difficult to work out Bayes theorem problems when given probabilities instead of whole numbers. Here is a trick: turn all the probabilities into ratios of whole numbers! First take the probability with the most significant figures (which is just decimal places for probabilities written as a decimal). Double this result and raise ten to that as a power. This will be our total sample size. All probabilities in our example have two decimal places, thus multiplying by \\(10^{2+2}=\\) 10,000 will give us whole numbers. Out of 10,000 items, \\(10000\\cdot 0.03=300\\) will be defective Out of 300 defective items, \\(300\\cdot 0.96=288\\) will fail the quality control check (correctly be identified as defective). Out of \\(10000-300=9,\\!700\\) non-defective items, \\(9700\\cdot 0.02=194\\) will fail a quality control check (and be erroneously identified as defective). Thus, now we can see that there will be a total of \\(288+194=482\\) items that fail a quality control check, and \\(288\\) of them will actually be defective. Thus \\(P(D\\mid F)=\\frac{288}{482}\\approx 0.5975\\). \\[ \\diamond \\S \\diamond \\] "],["random-variables-and-distributions.html", "Chapter 3 Random variables and distributions 3.1 Expectation and variance 3.2 Joint distributions 3.3 Independence of random variables 3.4 Discrete: Bernoulli, binomial, geometric, Poisson 3.5 Continuous: Uniform, exponential, normal Summary", " Chapter 3 Random variables and distributions Definition. A random variable \\(X\\) is a variable where you must perform a random experiment to determine its value. The sample space \\(S\\) is the set of numerical values that the random variable can take. An event is a subset of the sample space. A random variable can be either discrete (if it takes values in a discrete set such as \\(\\mathbb N\\)) or continuous (if it can take on any value in some interval). Examples. Let \\(X\\) be the number of dots on the upper face of a dice roll. Let \\(X\\) be the mass of a randomly selected person form a specific population. Let \\(X\\) be the number of cars that pass by a given intersection during a particular day. Let \\(X\\) be the number of radioactive decays in one hour of some particular material. Definition. A probability function allows us to calculate the probabilities of observing specific numerical values or ranges of values for random variable \\(X\\). A discrete random variable has a probability mass function (pmf) \\(f_X(x)=P(X=x)\\), and a continuous random variable has a probability density function (pdf) \\(f_X(x)\\) which we must integrate to get probabilities \\(P(a&lt;X&lt;b)=\\int_a^b f_X(x)dx\\). The cumulative distribution function (cdf) gives cumulative probabilities \\(F_X(x)=P(X\\leq x)\\). 3.1 Expectation and variance The expected value of a random variable is also called its mean or average value, and denoted \\(E(X)\\). Expected value. Discrete:   \\(E(X)=\\sum_x x f_X(x)\\)   \\(E[g(X)]=\\sum_x g(x) f_X(x)\\) Continuous:   \\(E(X)=\\int_{-\\infty}^\\infty x f_X(x)~dx\\)   \\(E[g(X)]=\\int_{-\\infty}^\\infty g(x) f_X(x)~dx\\) Variance. \\(\\mathsf{Var}(X)=E[(X-E(X))^2]=E(X^2)-E(X)^2\\) Let \\(X\\) have pmf given below. \\[ f_X(x)=\\begin{cases} 0.5 &amp;\\text{ if } x=0\\\\ 0.3 &amp;\\text{ if } x=1\\\\ 0.2 &amp;\\text{ if } x=2\\\\ \\end{cases} \\] Then \\(E(X)=0.5\\cdot 0+0.3\\cdot 1+0.2\\cdot 2=0.7\\). If we repeatedly and independently performed the random experiment that led us to observe random variable \\(X\\), then we would observe a sequence of \\(0\\)s, \\(1\\)s, and \\(2\\)s, and the average observed \\(X\\)-value should be close to \\(0.7\\). The law of large numbers says that if we observe more and more \\(X\\)-values, the average will actually get closer and closer to \\(0.7.\\) The variance is \\(Var(X)=E(X^2)-E(X)^2\\) so we need to calculate the expected value of the function \\(X^2\\) which is \\(E(X^2)=0.5\\cdot0^2+0.3\\cdot1^2+0.2\\cdot2^2=1.1\\). Thus \\(Var(X)=1.1-0.7^2=1.1-0.49=0.61.\\) 3.2 Joint distributions Sometimes a random experiment yields more than one measurement. As an example, we might drill a water well and record several things such as the depth, width, flow rate, and density of the material drilled through. There could be dependencies such as a deeper well having a higher flow rate. Or maybe flow rate is completely independent of depth of the well. We can think of flow rate as one random variable, \\(X\\), and depth as another random variable, \\(Y\\). We could ask for the probabilities for \\(X\\) or \\(Y\\) alone, e.g. \\(P(X=1.3)\\) or \\(P(Y&gt;1000)\\) (these are called marginal probabilities), or we could ask for their joint probabilities, e.g. \\(P(X=1.3 \\text{ and } Y&gt;1000)\\). Normally we can list joint probabilities for two random variables, \\(X\\) and \\(Y\\), in a tabular format. We can list the \\(X\\)-values along the first row \\((x_1,x_2,...)\\), the \\(Y\\)-values along the first column \\((y_1,y_2,...)\\), and in the box for the \\(i^{th}\\) column and \\(j^{th}\\) row, we write the joint probability \\(P(X=x_i,Y=y_j)\\). See the example below. X 10 20 Y 5 0.5 0.2 500 0.1 0.2 Here we have \\(P(X=10,Y=500)=0.1\\) for example. If we wish to calculate a marginal probability, then we must sum over the appropriate row or column, e.g. \\(P(X=10)=P(X=10,Y=5)+P(X=10,Y=500)=0.5+0.1=0.6.\\) We can calculate the marginal pmf for \\(X\\) by summing each column and the marginal pmf for \\(Y\\) by summing each row. In general the marginal probabilities are given by: \\[f_X(x)=P(X=x)=\\sum_y P(X=x,Y=y)=\\sum_y f_{X,Y}(x,y)\\] \\[f_Y(y)=P(Y=y)=\\sum_x P(X=x,Y=y)=\\sum_x f_{X,Y}(x,y).\\] The above is using an example of partitioning the entire (joint) sample space. If we are interested in the even \\(\\{X=x\\}\\) but we only know joint probabilities \\(\\{X=x,Y=y_j\\}\\) for \\(j=1,2,...,m\\) (assume there are \\(m\\) total \\(y\\)-values. Then we have a partition \\(\\{Y=y_1\\},\\ldots,\\{Y=y_m\\}\\), and we can intersect \\(\\{X=x\\}\\) with each of these to create a list of pairwise mutually exclusive events: \\(\\{X=x\\}\\cap\\{Y=y_1\\},\\ldots,\\{X=x\\}\\cap\\{Y=y_m\\}\\). Now we calculate \\[ \\begin{aligned} P(X=x)&amp;=P(\\{X=x\\}\\cap\\{Y=y_1\\})+\\cdots P(\\{X=x\\}\\cap\\{Y=y_m\\})\\\\ &amp;=P(X=x,Y=y_1)+\\cdots P(X=x,Y=y_m)\\\\ &amp;=\\sum_{j=1}^m P(X=x,Y=y_j). \\end{aligned} \\] 3.3 Independence of random variables When working with introductory probability the idea of independent events was covered. Events \\(A\\) and \\(B\\) are independent if and only if \\(P(A\\cap B)=P(A)P(B)\\). Independence for random variables works similarly. Definition. Random variables \\(X\\) and \\(Y\\) are independent if and only if every \\(X\\)-event is independent of every \\(Y\\)-event. That is for every subset of possible \\(X\\)-values \\(A\\) and every subset of possible \\(Y\\)-values \\(B\\) we have \\[P(\\{X\\in A\\}\\cap \\{Y\\in B\\})=P(X\\in A)P(Y\\in B).\\] For discrete random variables \\(X\\) and \\(Y\\), they are independent if and only if \\(P(X=x,Y=y)=P(X=x)P(Y=y)\\) for all \\(x\\) and \\(y\\). That is the joint pmf must be exactly the product of the marginal pmfs. For continuous random variables \\(X\\) and \\(Y\\), they are independent if and only if their joint pdf is exactly the product of the marginal pdfs: \\(f_{X,Y}(x,y)=f_X(x)f_Y(y)\\). Note that for independent random variables, independence requires that the allowable values for one variable must not be affected by the other variable, e.g. the set of possible \\(X\\)-values must not depend on \\(Y\\) in any way. Example. Let \\(X\\) be the outcome of a fair 6-sided die and \\(Y\\) be the outcome of a fair coin flip (\\(0=~\\)tails,\\(1=~\\)heads). Then we naturally think of them as independent, and they are in the technical sense of the term. It makes physical sense that the coin flip shouldnt impact the die roll at all unless we are to use some contrived mechanism to force them to interact, e.g. if a machine were to control the force of the coin flip and die roll is some specific way so that the coin tended to be heads when the die tended to be even. Here the marginal pmfs are \\(f_X(x)=\\frac16\\) for \\(x=1,2,3,4,5,6\\) (and zero otherwise), and the pmf for \\(Y\\) is \\(f_Y(y)=\\frac12\\) for \\(y=0,1\\). Their joint pmf is \\(f_{X,Y}(x,y)=\\frac1{12}\\) for \\((x,y)=(1,0),\\ldots,(6,0),(1,1),\\ldots,(6,1)\\). Example. Let \\(X\\) and \\(Y\\) have joint probabilities given by the table below. X 10 20 Y 5 0.5 0.2 500 0.1 0.2 You can actually look at the table and see that \\(X\\) and \\(Y\\) are not independent by seeing the \\(Y\\)-values of 5 and 500 have equal probability mass under the \\(X=20\\) column but unequal probability mass under the \\(X=10\\) column. It isnt always that simple though. Here is a joint pmf for an independent \\(X,Y\\) pair, and it isnt obvious they are independent. X 10 20 30 Y 5 0.36 0.18 0.06 500 0.24 0.12 0.04 3.4 Discrete: Bernoulli, binomial, geometric, Poisson 3.4.1 Bernoulli Situation/explanation: We perform a single random experiment that has only two outcomes: success and failure. Success and failure can mean almost anything. Success can mean a no vote on a particular election item, or it can mean we find a defective item, or it can mean a die roll has a particular value. \\(X=0\\) for failure, \\(X=1\\) for success, \\(p=\\) probability of success. \\[X\\sim \\text{Bernoulli($p$)}\\] \\[X=0,1\\] \\[f_X(x)= \\begin{cases} 1-p &amp;\\text{ if } x=0\\\\ p &amp;\\text{ if } x=1 \\end{cases}\\] \\[E(X)=p\\] \\[\\text{Var}(X)=p(1-p)\\] In R: \\[f(x) = P(X=x) = \\texttt{dbinom($x$,size=$1$,prob=$p$)}.\\] \\[F(x) = P(X\\leq x) =\\texttt{pbinom($x$,size=$1$,prob=$p$)}.\\] 3.4.2 Binomial Situation/explanation: We perform a fixed number of Bernoulli trials. Count the total number of successes. \\(X=\\) number of successes out of \\(n\\) Bernoulli trials with \\(p=\\) probability of success. \\[X\\sim \\text{Bin($n$,$p$)}\\] \\[X=0,1,2,3,\\ldots,n\\] \\[f_X(x)={n\\choose x}p^x(1-p)^{n-x}\\] \\[E(X)=np\\] \\[\\text{Var}(X)=np(1-p)\\] In R: \\[f(x) = P(X=x) = \\texttt{dbinom($x$,size=$n$,prob=$p$)}. \\quad \\text{ (pmf)}\\] \\[F(x) = P(X\\leq x) =\\texttt{pbinom($x$,size=$n$,prob=$p$)}. \\quad \\text{ (cdf)}\\] \\[\\tilde{x}_q=F^{-1}(q) =\\texttt{qbinom($q$,size=$n$,prob=$p$)}. \\quad \\text{ ($q$100$^{th}$ percentile, inverse cdf)}\\] 3.4.3 Geometric Situation/explanation: We perform Bernoulli trials until we get our first success. Count the total number of trials (a bunch of failures, and one success). \\(X=\\) total number of trials required to get a single success with \\(p=\\) probability of success. (the count includes the success also) Note that there is no fixed total number of trials here. Note: there is a single success, the last trial, the first \\(x-1\\) trials are all failures. \\[X\\sim \\text{Geom($p$)}\\] \\[X=0,1,2,3,\\ldots\\] \\[f(x)=p(1-p)^{x-1}\\] \\[F(x)=1-(1-p)^{x}\\] \\[E(X)=\\frac1p\\] \\[\\text{Var}(X)=\\frac{(1-p)}{p^2}\\] (Note that in R, the definition of \\(X\\) varies from this. In R, \\(X\\) is the number of failures. So when calculating in R for the Geometric random variable, we must either subtract 1 from the \\(x\\) values in R for the pmf and cdf, but add 1 to the output from an R quantile.) In R: \\[f(x) = P(X=x) = \\texttt{dgeom($x$-1,prob=$p$)}.\\quad \\text{ (pmf)}\\] \\[F(x) = P(X\\leq x) =\\texttt{pgeom($x$-1,prob=$p$)}.\\quad \\text{ (cdf)}\\] \\[\\tilde{x}_q=F^{-1}(q) =\\texttt{qgeom($q$,prob=$p$)+1}. \\quad \\text{ ($q$100$^{th}$ percentile, inverse cdf)}\\] 3.4.4 Poisson Situation/explanation: (1) Events arrive over time. Count the number of events in a given time interval. (2) Events are distributed over a spatial region randomly. Count the number of events in a given region. \\(X=\\) number of events that occur at rate \\(\\lambda\\). The rate can be thought of as the number of events per unit time or more generally, number of events per unit. Often it is the number of events per unit length or per unit time. \\[X\\sim \\text{Pois($\\lambda$)}\\] \\[X=0,1,2,\\ldots\\] \\[f_X(x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\] \\[E(X)=\\lambda\\] \\[Var(X)=\\lambda\\] In R: \\[f(x) = P(X=x) = \\texttt{dpois($x$,lambda=$\\lambda$)}. \\quad \\text{ (pmf)}\\] \\[F(x) = P(X\\leq x) =\\texttt{ppois($x$,lambda=$\\lambda$)}. \\quad \\text{ (cdf)}\\] \\[\\tilde{x}_q=F^{-1}(q) =\\texttt{qpois($q$,lambda=$\\lambda$)}. \\quad \\text{ ($q$100$^{th}$ percentile, inverse cdf)}\\] 3.5 Continuous: Uniform, exponential, normal 3.5.1 Uniform The random variable is the continuous analog of ``equally likely. \\[X\\sim \\text{Unif($a,b$)}\\] \\[a \\leq X \\leq b\\] \\[f_X(x)=\\frac{1}{b-a} \\quad \\text{ for } x\\in[a,b]\\] \\[ F(x)=\\frac{x-a}{b-a}\\] \\[E(X)=\\frac{a+b}{2}\\] \\[\\text{Var}(X)=\\frac{1}{12}(b-a)^2\\] In R: \\[f(x) = \\texttt{dunif($x$,min=$a$,max=$a$)}. \\quad \\text{ (pdf, not probability mass)}\\] \\[F(x) = P(X\\leq x) =\\texttt{punif($x$,min=$a$,max=$a$)}. \\quad \\text{ (cdf)}\\] 3.5.2 Exponential Suppose events happen at random times (or at random locations along a physical length). The length of time between events can be modeled by an . \\(X=\\) wait time until an event.\\ Rate parameter \\(\\lambda\\) is the number of events per unit time. It can be number of events per unit length as well; it just depends on the context. This is very closely related to the Poisson, as well discuss later. We can refer to \\(X\\) as the inter-arrival time,''wait time, or ``inter-event time. \\[X\\sim \\text{Exp($\\lambda$)}\\] \\[0 \\leq X\\] \\[f_X(x)=\\lambda e^{-\\lambda x} \\quad \\text{ for } x\\geq 0\\] \\[ F(x)=1-e^{-\\lambda x}\\] \\[E(X)=\\frac{1}{\\lambda}\\] \\[\\text{Var}(X)=\\frac{1}{\\lambda^2}\\] In R: \\[f(x) = \\texttt{dexp($x$,rate=$\\lambda$)}. \\quad \\text{ (pdf, not probability mass)}\\] \\[F(x) = P(X\\leq x) =\\texttt{pexp($x$,rate=$\\lambda$)}. \\quad \\text{ (cdf)}\\] \\[\\tilde{x}_q=F^{-1}(q) =\\texttt{qexp($q$,rate=$\\lambda$)}. \\quad \\text{ ($q$100$^{th}$ percentile, inverse cdf)}\\] 3.5.3 Normal The normal distribution is one of the most important. Its probability density function is often called the bell curve or Gaussian. \\[X\\sim \\text{N($\\mu$,$\\sigma^2$)}\\] \\[-\\infty &lt; X &lt; \\infty\\] \\[f_X(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\quad \\text{ for } x\\in\\mathbb R\\] \\[ F(x)=\\text{Unfortunately, can&#39;t be written down in a closed fomula}\\] \\[E(X)=\\mu\\] \\[\\text{Var}(X)=\\sigma^2\\] In R: \\[f(x) = \\texttt{dnorm($x$,mean=$\\mu$,sd=$\\sigma$)}. \\quad \\text{ (pdf, not probability mass)}\\] \\[F(x) = P(X\\leq x) =\\texttt{pnorm($x$,mean=$\\mu$,sd=$\\sigma$)}. \\quad \\text{ (cdf)}\\] \\[\\tilde{x}_q=F^{-1}(q) =\\texttt{qnorm($q$,mean=$\\mu$,sd=$\\sigma$)}. \\quad \\text{ ($q$100$^{th}$ percentile, inverse cdf)}\\] 3.5.3.1 The 68-95-99.7 rule For any normally distributed random variable, we have that there is approximately a 68% chance of being within 1 standard deviation of the mean, a 95% chance of being within 2 standard deviations of the mean, and a 99.7% chance of being within 3 standard deviations of the mean, i.e. that \\[\\begin{aligned} P(\\mu-\\sigma &lt; X &lt; \\mu+\\sigma) &amp;\\approx 68\\%\\\\ P(\\mu-2\\sigma &lt; X &lt; \\mu+2\\sigma) &amp;\\approx 95\\%\\\\ P(\\mu-3\\sigma &lt; X &lt; \\mu+3\\sigma) &amp;\\approx 99.7\\% \\end{aligned}\\] Summary Summary of notation, formulas, and terminology Discrete RVs:   pmf \\(f_X(x)=P(X=x)\\)   cdf \\(F_X(x)=P(X\\leq x)=\\sum_{j\\leq x} f_X(j)\\)   \\(E(X)=\\sum_{x} x P(X=x)=\\sum_{x} x f_X(x)\\) Continuous RVs:   pdf \\(f_X(x)\\), \\(P(a&lt;X&lt;b)=\\int_a^b f_X(x)dx\\)   cdf \\(F_X(x)=P(X\\leq x)=\\int_{-\\infty}^x f_X(t)dt\\), \\(f_X(x)=\\frac{d}{dx}F_X(x)\\)   \\(E(X)=\\int_{-\\infty}^\\infty x f_X(x) dx\\) Variance: \\(\\textrm{Var}(X)=E[(X-E(X))^2]=E(X^2)-E(X)^2\\) Expected value of function of RV: \\(E(h(X))=\\sum_x h(x) f_X(x)\\) (discrete)   \\(E(h(X))=\\int_{-\\infty}^\\infty h(x) f_X(x) dx\\) (continuous) Jointly distributed RVs:   \\(f_{X,Y}(x,y)=P(X=x,Y=y)=P(\\{X=x\\}\\cap\\{Y=y\\})\\) Independence of RVs:   \\(X,Y\\) independent if and only if \\(f_{X,Y}(x,y)=f_X(x) f_Y(y)\\),   i.e. \\(P(X=x,Y=y)=P(X=x)P(Y=y)\\), for all \\(x,y\\) Discrete RVs Bernoulli: models a process with only two outcomes   \\(X\\sim\\mathsf{Bernoulli}(p)\\)   \\(f_X(x)=\\begin{cases}p &amp;\\text{ for } x=1\\\\ 1-p &amp;\\text{ for } x=0\\end{cases}\\)   \\(E(X)=p\\), \\(Var(X)=p(1-p)\\)   R: \\(P(X=x)=~\\)dbinom(x,size=1,prob=p) Binomial: models number of successes in \\(n\\) independent trials with \\(p\\) the probability of success for each trial   \\(X\\sim\\mathsf{Binom}(n,p)\\)   \\(f_X(x)={n\\choose x}p^x (1-p)^{n-x}\\), \\(x=0,1,\\ldots,n\\)   \\(E(X)=np\\), \\(Var(X)=np(1-p)\\)   R: \\(P(X=x)=~\\)dbinom(x,size=n,prob=p) Geometric: models number of trials up to and including first success   \\(X\\sim\\mathsf{Geom}(p)\\)   \\(f_X(x)=p(1-p)^{x-1}\\), \\(x\\in\\{1,2,\\ldots\\}=\\mathbb N\\)   \\(E(X)=\\frac1p\\), \\(Var(X)=\\frac{1-p}{p^2}\\)   R: \\(P(X=x)=~\\)dgeom(x-1,prob=p) (note that R only counts the failures) Poisson: models number of events over a continuous extent   \\(X\\sim\\mathsf{Pois}(\\lambda)\\)   \\(f_X(x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\), \\(x\\in\\{0,1,\\ldots\\}=\\mathbb N_0\\)   \\(E(X)=\\lambda\\), \\(Var(X)=\\lambda\\)   R: \\(P(X=x)=~\\)dpois(x,lambda=) Continuous RVs Uniform: models a continuous quantity that takes any value in an interval with equal likelihood   \\(X\\sim\\mathsf{Unif}(a,b)\\)   \\(f_X(x)=\\frac1{b-a}\\), \\(x\\in(a,b)\\)   \\(E(X)=\\frac12(a+b)\\), \\(Var(X)=\\frac1{12}(b^2-a^2)\\)   R: \\(P(X\\leq x)=~\\)punif(x,min=a,max=b) Normal: models quantity that is symmetrically distributed and random variation from many small additive contributions   \\(X\\sim\\mathsf{N}(\\mu,\\sigma^2)\\)   \\(f_X(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\), \\(x\\in\\mathbb R\\)   \\(E(X)=\\mu\\), \\(Var(X)=\\sigma^2\\)   R: \\(P(X\\leq x)=~\\)pnorm(x,mean=,sd=) Exponential: models wait times between events occurring at random times   \\(X\\sim\\mathsf{Exp}(\\lambda)\\)   \\(f_X(x)=\\lambda e^{-\\lambda x}\\), \\(x&gt;0\\)   \\(E(X)=\\frac1\\lambda\\), \\(Var(X)=\\frac1{\\lambda^2}\\)   R: \\(P(X\\leq x)=~\\)pexp(x,rate=) "],["conditional-expectation.html", "Chapter 4 Conditional expectation 4.1 Conditional distributions 4.2 Conditional expectation 4.3 (Random) conditional expectation Summary", " Chapter 4 Conditional expectation Now we discuss the concept of conditional expectation. Conditioning can be thought of as reducing the number of possibilities for a random experiment. 4.1 Conditional distributions The conditional distribution of \\(X\\) given \\(Y=y\\) is given as \\[f_{X|Y=y}(x|y)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}\\] where \\(f_{X,Y}\\) is the joint distribution of \\(X\\) and \\(Y\\) and \\(f_Y\\) is the marginal distribution of \\(Y\\). If both are discrete random variables, then these are all probability mass functions (and we sum to get probabilities), and if they are both continuous random variables, then all are probability density functions (and we integrate to get probabilities). For the discrete case, which is what we will mostly be working with, we get \\[f_{X|Y=y}(x|y)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}=\\frac{P(X=x,Y=y)}{P(Y=y)}=P(X=x\\mid Y=y).\\] So the conditional pmf is just another way to write conditional probabilities. We can calculate conditional pmfs from the tabular format of the joint distribution simply by dividing a row or column by its total sum. Consider the example below. X 0 1 2 Y 1 0.36 0.18 0.06 2 0.14 0.20 0.06 We can write the conditional pmf for \\(X\\) given \\(Y=1\\) by taking the row \\((0.36,0.18,0.06)\\) and dividing it by its sum \\(0.36+0.18+0.06=0.60=P(Y=1)\\) to get \\((0.36/0.6,0.18/0.6,0.06/0.6)=(0.6,0.3,0.1)\\), and this is exactly the conditional pmf for \\(X\\) given \\(Y=1\\). \\[f_{X|Y=5}=\\begin{cases} 0.6 &amp; \\text{ if } x=0\\\\ 0.3 &amp; \\text{ if } x=1\\\\ 0.1 &amp; \\text{ if } x=2\\\\ \\end{cases}\\] 4.2 Conditional expectation Conditional expectation can be thought of in the following way. Imagine that we perform an experiment repeatedly where each outcome gives us two measurements, i.e. two random variables \\(X\\) and \\(Y\\) which could depend on each other in any arbitrary way. In other words a single outcome of the experiment gives us a pair \\((X,Y)\\). We could simply average the \\(X\\)-values to approximate its mean \\(E(X)\\), or we could average only those \\(X\\) values which came paired with a particular \\(Y\\) value or a predetermined range of \\(Y\\)-values. This would be calculating a conditional expected value for \\(X\\). If we average all \\(X\\)-values that are paired with precisely \\(Y=y\\), then this gives us an estimate of \\(E(X\\mid Y=y)\\) the conditional expectation of \\(X\\) given \\(Y=y\\). We can compute this from our distributions just as we did before for expected value, but we use the conditional pmf: \\[E(X\\mid Y=y)=\\sum_x x f_{X|Y=y}(x|y).\\] It is advised to first think of \\(y\\) as a fixed constant in the above equation. But one can also consider it as a variable \\(y\\) (a standard algebra/calculus variable, not a random variable) which ranges over the set of possible values for random variable \\(Y\\). In other words \\(E(X\\mid Y=y)\\) can be though of as a function of \\(y\\) in the typical algebra/calculus sense, i.e. \\(E(X\\mid Y=y)=h(y)\\) with plugging in a specific \\(y\\)-value will output the fixed numerical conditional expectation of \\(X\\). In the continuous case, we integrate instead of sum: \\[E(X\\mid Y=y)=\\int_{-\\infty}^\\infty x f_{X|Y=y}(x|y) ~dx.\\] In this case, we are working with probability densities though, so the conditional pdf is not the same thing as conditional probability. 4.3 (Random) conditional expectation We can also write \\(E(X\\mid Y)\\) without specifying a particular value for \\(Y\\) and leaving it as a random variable. If we were to decide that we are interested in a particular \\(Y\\)-values, e.g. \\(y\\), then we can plug that in to get \\(E(X\\mid Y=y)\\) which we can calculate as already discussed and can think of as a function \\(E(X\\mid Y=y)=h(y)\\). We can skip the step of plugging in a particular \\(y\\) though and can just write \\(E(X\\mid Y)=h(Y)\\) which is a function of random variable \\(Y\\). Since a function of a random variable is also a random variable, we have that \\(E(X\\mid Y)\\) is a random variable. This means that we can calculate probabilities such as \\(P[E(X\\mid Y)=a]\\) or \\(P\\left[E(X\\mid Y)\\leq b\\right]\\). In fact, these probabilities are really qustions about the random variable \\(Y\\): \\[P[E(X\\mid Y)=a]=P\\left[ Y \\text{ gives a value } y \\text{ that results in} E(X\\mid Y=y)=a\\right]\\] Consider the example below. X 0 1 2 Y 1 0.36 0.18 0.06 2 0.14 0.20 0.06 We can calculate the conditional pmfs \\(f_{X|Y=1}(x|1)\\) and \\(f_{X|Y=2}(x|2)\\), and from these we can calculate \\(E(X\\mid Y=1)\\) and \\(E(X\\mid Y=2)\\). \\[\\begin{aligned} E(X\\mid Y=1)&amp;=0\\cdot f_{X|Y=1}(0|1)+1\\cdot f_{X|Y=1}(1|1)+2\\cdot f_{X|Y=1}(2|1)\\\\ &amp;=0\\cdot \\frac{0.36}{0.6}+1\\cdot \\frac{0.18}{0.6}+2\\cdot \\frac{0.06}{0.6}=0.3+0.2=0.5 \\end{aligned}\\] Similarly, we get \\(E(X\\mid Y=2)=0.2/0.2+2\\cdot 0.06/0.2=1.6\\). Hence we get the function \\[ E(X\\mid Y=y)=h(y)= \\begin{cases} 0.5 &amp;\\text{ if } y=1\\\\ 1.6 &amp;\\text{ if } y=2. \\end{cases} \\] As a random variable, we get \\[ P[E(X\\mid Y)=a]= \\begin{cases} P(Y=1) &amp;\\text{ if } a=0.5\\\\ P(Y=2) &amp;\\text{ if } a=1.6. \\end{cases} \\] So we can write down the pmf for \\(E(X\\mid Y)=h(Y)\\) as \\[ f_{h(Y)}(a)=f_{E(X\\mid Y)}(a)= \\begin{cases} 0.6 &amp;\\text{ if } a=0.5\\\\ 0.4 &amp;\\text{ if } a=1.6. \\end{cases} \\] 4.3.1 Total expectation We can compute the expected value of a random variable using conditioning. \\[ \\begin{aligned} E(X)&amp;=E[E(X\\mid Y)]\\\\ &amp;=\\sum_y E(X\\mid Y=y) P(Y=y)\\\\ \\end{aligned}\\] Example. Consider the example where \\(Y\\sim\\textsf{unif}(1,5)\\) and, conditional on \\(Y=y\\), \\(X\\sim\\textsf{Exp(rate=\\)y\\()}\\). Calculate \\(E(X)\\). Solution: What we have here is called a compound distribution since the distribution of \\(X\\) has a parameter that is random. The conditional pdf of \\(X\\) given \\(Y=y\\) is \\(f_{X|Y=y}(x|y)=y e^{yx}\\) which is the exponential pdf with rate parameter \\(\\lambda\\) replaced by \\(y\\). The expected value is \\(\\frac1\\lambda\\), so in this case it is \\(\\frac1y\\). This is NOT the expected value of \\(X\\) though. This is the conditional expected value of \\(X\\) given that \\(Y=y\\): \\[E(X\\mid Y=y)=\\frac1y.\\] And, in general, if we allow \\(Y\\) to stay random, then we have \\[E(X\\mid Y)=\\frac1Y\\] which is still a random variable. Using the total expectation formula, we get \\[E(X)=E[E(X\\mid Y)]=E\\left[\\frac1Y\\right]=\\int_1^5 \\frac1y f_Y(y)~dy=\\int_1^5 \\frac1y \\frac14~dy=\\frac14\\ln5.\\] Summary Summary of notation, formulas, and terminology \\(E[X\\mid Y]\\) is a random variable whose value is known once we know the value of \\(Y\\). \\(E[X\\mid Y=y]\\) is a real number, it is \\(X\\) averaged according to the restriction that \\(Y=y\\). \\(f_{X|Y=y}(x|y)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}\\) is the conditional probability function for \\(X\\) given \\(Y\\) (conditioned on \\(Y\\)) where \\(f_{X,Y}(x,y)\\) is the joint pf, and \\(f_Y(y)\\) is the marginal pf for \\(Y\\). This works for both pmf and pdf. \\(E[X\\mid Y=y]=\\sum_x x P(X=x\\mid Y=y)=\\sum_x x f_{X|Y=y}(x|y)\\) (discrete case) \\(E[X\\mid Y=y]=\\int_{-\\infty}^\\infty x f_{X|Y=y}(x|y)\\) (continuous case) "],["intro-stochastic-processes.html", "Chapter 5 Intro Stochastic Processes Summary", " Chapter 5 Intro Stochastic Processes Generally, a stochastic process consists of an index set \\(T\\) which can usually be thought of as time, and at each time \\(t\\in T\\) we have a (real-valued) random variable \\(X_t\\). We write this as \\((X_t)_{t\\in T}\\) or \\(\\{X_t\\}_{t\\in T}\\). We can think of \\(X_t\\) as a random function of time. You have seen functions of time like \\(x(t)\\) where you plug in a \\(t\\)-value and it outputs an exact \\(x(t)\\)-value according to some formula, but for a stochastic process \\(X_t\\), even when the \\(t\\)-value is specified, we cannot know the precise value for \\(X_t\\) since it is still random. The index set is usually a subset of the set of natural numbers \\(\\mathbb N=\\{1,2,\\ldots\\}\\) or those including zero \\(\\mathbb N_0=\\{0,1,2,\\ldots\\}\\) or a subset of the real numbers \\(\\mathbb R\\). For example, we can have \\(T=\\{1,2\\}\\), \\(T=\\mathbb N_0\\), \\(T=[0,\\infty)\\subset\\mathbb R\\), or \\(T=[0,1]\\). If the index set is discrete, we call it a discrete-time stochastic process and if the index set is continuous (an interval subset of \\(\\mathbb R\\)), we call it a continuous-time stochastic process. Normally, we use \\(X_n\\) for discrete time and \\(X_t\\) for continuous time. If \\(T=\\{1,2\\}\\), then our stochastic process is \\((X_1,X_2)\\) and is a random point in the plane \\(\\mathbb R^2\\). If \\(T=\\mathbb N_0\\), then our stochastic process is \\((X_0, X_1,X_2,\\ldots)\\) and is a random point in \\(\\mathbb R^\\infty\\) (in other words, a infinite sequence of random numbers). If the index set is a continuous interval such as \\(T=[0,1]\\) or \\(T=[0,\\infty)\\), then we can think of \\(X_t\\) as a random function of \\(t\\). The state space \\(S\\) is the set where each random variable \\(X_t\\) takes its values in. Normally, the state space is a subset of the real numbers. Often, we are counting things and the state space will be \\(\\mathbb N\\) or \\(\\mathbb N_0\\), e.g., counting the number of insurance claims that arrive each day or counting the number of radioactive decays every hour. We call such a stochastic process discrete-space. In other cases, we are measuring something like length or amount of money, and the state space is \\([0,\\infty)\\) or some other real line interval. Such stochastic processes are called continuous-space. There are two intuitive ways to think about a stochastic process. We can think of it as \\(X_t\\) where it is implied that we have several random variables, one variable for each value of \\(t\\). Alternatively, we can think of the entire random sequence or function as a single object and write \\(X=(X_t)_{t\\in T}\\). This \\(X\\) is not a random variable, it is a stochastic process. Each \\(X_t\\) is a real-valued random variable, but \\(X\\) is vector-valued, sequence-valued, or function-valued. In order to know the value of \\(X\\), we have to know the value of each \\(X_t\\) for every possible \\(t\\)-value. We can think of \\(X\\) as a random sequence of numbers, \\(X=(X_0,X_1,X_2,\\ldots)\\) where each \\(X_n\\) is a real-valued random variable. Definition. A stochastic process \\(X\\) with state space \\(S\\) and index set \\(T\\) is a collection of random variables \\(X=(X_t)_{t\\in T}\\). For each \\(t\\in T\\), \\(X_t\\) is a \\(S\\)-valued random variable, that is each \\(X_t\\) takes values in \\(S\\). Definition. For stochastic process \\(X=(X_t)_{t\\in T}\\) with state space \\(S\\) and (time) index set \\(T\\), a sample path is a particular full realization of the stochastic process. That is, we know the precise value of \\(X_t\\) for every \\(t\\). Sample paths are specific determined realizations, and we can say \\(x(t)\\) is a specific sample path, that is, it is just a (fixed) function of \\(t\\). Definition. For stochastic process \\(X=(X_t)_{t\\in T}\\) with state space \\(S\\) and (time) index set \\(T\\), the sample path space is \\(\\Omega=S^T\\), that is, if we know the precise value of \\(X_t\\) for all \\(t\\in T\\), then \\(X\\) is a function from \\(T\\) to \\(S\\). Example. Consider the stochastic process \\(X_n\\) for \\(n\\in\\mathbb N\\) and \\(X_n\\sim\\textsf{Bernoulli}(p)\\) for each \\(n\\). The state space is \\(S=\\{0,1\\}\\) since each \\(X_n\\) is a Bernoulli random variable, and the time index set is \\(\\mathbb N\\). The sample path space is \\(\\Omega=\\{0,1\\}^{\\mathbb N}\\) which can also be written as \\(\\{0,1\\}^\\infty\\) or \\(\\{0,1\\}\\times\\{0,1\\}\\times\\cdots\\). In this case \\(\\Omega\\) is just the set of all infinitely long sequences of 0s and 1s, which we call binary sequences. A particular sample path realization is a particuler fixed sequence of zeros and ones, e.g. \\((0,1,1,0,1,0,0,0,1,0,1,1,0,0,\\ldots)\\). A typical sample path should contain roughly an equal number of 1s and 0s over most of it. For example, the first 1000 states will be fairly close to equal parts 0 and 1 to high probability. We can precisely calculate the probability there are, say, less than 450 or more than 550 ones in this case using the binomial distribution. Let \\(Y\\sim\\textsf{Binom}(n=1000,p)\\) be the number of ones. Then \\(P(Y&lt;450\\text{ or }Y&gt;550)=1-P(450\\leq Y\\leq 550)=1-\\sum_{j=450}^{550}{1000\\choose j}p^j(1-p)^{1000-j}.\\) If we let \\(p=\\frac12\\), then this is \\(1-\\sum_{j=450}^{550}{1000\\choose j}\\frac1{2^{1000}}\\). In \\(\\textsf{R}\\), we can compute this as 1-sum(dbinom(450:550,1000,0.5)). Since the number of trials is large, we can use the normal approximation 1-pnorm(550,500,sqrt(250))+pnorm(450,500,sqrt(250)) to see it is about 0.14% probability. Here are some examples of how a stochastic process might model a physical process. Example. Consider the following examples. A plant is growing in a pot and we want to model its total biomass over time for 1 year. Let \\(X_t\\) be the total biomass at time \\(t\\). We consider \\(X_t\\) for each \\(t\\) to be \\([0,\\infty)\\)-valued since biomass is nonnegative and we wont impose any particular upper limit on biomass. We let \\([0,365]\\) be the (time) index set and will measure time in days. The state space is thus \\([0,\\infty)\\) and the sample path space is \\(\\Omega=[0,\\infty)^{[0,365]}\\). Each physical realization of a plant growing from germination to death will give us a particular sample path realization which will be a function from \\([0,365]\\) to \\([0,\\infty)\\). This is a continuous-time stochastic process. The number of insurance claims per month for a twelve month year. We let \\(X_n\\) be the number of insurance claims in month \\(n\\) with index set \\(\\{1,2,\\ldots,12\\}\\). The state space is \\(\\mathbb N_0\\) as we could have zero claims in a month or potentially any positive number of claims without any specific upper limit. The sample path space is \\(\\Omega=\\mathbb N_0^{12}\\). A particular sample path realization will be a twelve-tuple (duodecuple) of nonnegative integers, e.g. \\(x=(10,4,0,1,0,8,12,25,37,22,13,9)\\in\\Omega\\). Note that it is important that we consider the ordering of the index set, i.e. that \\(X_1=10, X_2=4\\), etc. Try to construct the following example using the technical stochastic process notation. Practice. Write stochastic process notation for the closing price of a stock each day for one week of five trading days. What is the index set? What is the sample path space? Give a possible sample path realization. Show/hide solution. Solution. Let the index set \\(\\{1,2,\\ldots,5\\}\\) represent days one to five. For each day \\(n\\), the random variable \\(X_t\\) is the closing price of the stock on that day. We can write \\(X=(X_n)_{n=1,2,\\ldots,5}\\) or \\(X=(X_1,X_2,X_3,X_4,X_{5})\\). The sample path space is \\([0,\\infty)^{5}\\) since each full realization of the stochastic process is a sequence of five dollar amounts. Each dollar amount should be nonnegative since a stock doesnt ever have a negative price. An example sample path realization might be \\((105.27,103.52,97.21,95.13,96.83)\\) representing a possible realization of the closing prices on the five days. Summary Summary of terminology and notation. \\(\\mathbb N=\\{1,2,\\ldots\\}\\) is the set of natural numbers. \\(\\mathbb N_0=\\{0,1,2,\\ldots\\}\\) is the set of natural numbers including zero. \\(\\mathbb R=(-\\infty,\\infty)\\) is the set of real numbers. \\(t\\in T\\) means \\(t\\) is an element of the set \\(T\\), e.g. \\(3\\in [-1,5]\\) or \\(\\pi\\in\\mathbb R\\). stochastic process \\(X=(X_t)_{t\\in T}\\), for each \\(t\\), \\(X_t\\) is a random variable. state space \\(S\\) is where observations of \\(X_t\\) will take values in, e.g. \\(S=[0,\\infty)\\) or \\(S=\\mathbb N_0\\). index set \\(T\\) gives the times we observe \\(X_t\\) at. discrete-time if \\(T\\) is discrete, and continuous-time if \\(T\\) is a continuous interval. discrete-space if \\(S\\) is discrete, and continuous-space if \\(S\\) is continuous. sample path space \\(\\Omega=S^T=\\) all functions from \\(T\\) to \\(S\\). sample path or path realization \\(x(t)\\in\\Omega\\) with \\(x:T\\to S\\). Next well do some review of probability theory. "],["random-walks.html", "Chapter 6 Random walks 6.1 The simple symmetric random walk (SSRW) 6.2 Distribution of \\(X_n\\) 6.3 Shift invariance &amp; memorylessness 6.4 Reflection principle 6.5 Maximum state reached 6.6 Hitting times 6.7 Return time to state \\(0\\) Summary", " Chapter 6 Random walks The random walk will be one of our first official stochastic process models. It models a particle on a line jumping one unit left or right with equal probability. Variations of this can be used for modeling many physical phenomena, including: a viral particle floating in the air (a 3D random walk), an animal moving in its habitat (2D for most land animals, but 1D can work for a restricted habitat), or a stock price (1D). 6.1 The simple symmetric random walk (SSRW) We let \\(X_n\\) be the state of the process at time step \\(n\\) and fix \\(X_0=0\\) (the particle starts at the origin). The particle moves left or right with equal probability, which is given as \\[P(X_{n+1}=j-1\\mid X_n=j)=\\frac12,\\] \\[P(X_{n+1}=j+1\\mid X_n=j)=\\frac12.\\] More generally, we can let \\(P(X_{n+1}=j+1\\mid X_n=j)=p\\) and \\(P(X_{n+1}=j-1\\mid X_n=j)=1-p\\), and if \\(p\\neq\\frac12\\), then we call it a simple asymmetric random walk (SARW). We call this stochastic process simple because successive steps are independent and identically distributed and symmetric or asymmetric depending on whether the up and down probabilities are equivalent or not. We will refer to the state of the process as the level or location at times, and rather than left or right, we will usually say up and down since we normally graph time horizontally and location on \\(\\mathbb Z\\) vertically. We consider the choice at each time step to go up or down as being independent of every other time step. This model is of a special class of stochastic processes called Markov chains, but well discuss those in more detail later. We can construct this model mathematically in more detail be letting the \\(n^{th}\\) step be random variable \\(Y_n\\) which takes values \\(\\pm1\\) with equal probability and all being independent. We say that the \\(Y_n\\) are i.i.d. (independent and identically distributed) with \\(P(Y_n=1)=P(Y_n=-1)=\\frac12\\) for all \\(n\\). Independence here means that if we want to calculate probabilities for multiple \\(Y_n\\) simultaneously, we can calculate them individually and multiply: \\[P(Y_j=a,Y_k=b)=P(Y_j=a)P(Y_k=b)\\] for any \\(i,j\\in\\mathbb N\\) (with \\(i\\neq j\\)) and any \\(a,b\\in\\{-1,1\\}\\). Then we can write \\[X_n=\\sum_{j=1}^n Y_j.\\] Example. Calculate \\(P(X_3=3)\\). This is only possible if \\(Y_1=Y_2=Y_3=1\\) and so we calculate \\[P(Y_1=1,Y_2=1,Y_3=1)=P(Y_1=1)P(Y_2=1)P(Y_3=1)=\\frac12\\cdot\\frac12\\cdot\\frac12=\\frac18.\\] Example. Calculate \\(P(X_2=0)\\). This is only possible if \\(Y_1=1,Y_2=-1\\) or \\(Y_1=-1,Y_2=1\\) and so we calculate each probability and add the result. \\[P(X_2=0)=P(Y_1=1,Y_2=-1)+P(Y_1=-1,Y_2=1)=\\frac14+\\frac14=\\frac12.\\] The state space of the SSRW is thus the set of integers \\(\\mathbb Z\\) and the time index set is \\(\\mathbb N_0\\). The sample path space will be all infinitely long sequences of integers where consecutive integers only differ by \\(\\pm1\\). We could say that the sample path space is all infinite sequences of integers, but most of those will have probability zero since any sequence which jumps outside of \\(\\pm1\\) would be considered as not possible. So we have our sequence of random variables \\((X_0,X_1,X_2,\\ldots)\\). Of course, \\(X_0\\) is deterministically set to one, but we can still consider it a random variable with full probability mass on one. Now \\(X_1\\) is equally likely to be \\(\\pm1\\). If \\(X_1=1\\), then \\(X_2\\) is equally likely to be \\(0,2\\), and if \\(X_1=-1\\), then \\(X_2\\) is equally likely to be \\(0,-2\\). 6.1.1 Definition of a random walk Let \\(X=(X_n)_{n\\in\\mathbb N_0}\\) be a stochastic process. Then \\(X\\) is a simple random walk if \\(X_0=0\\), and \\(Y_j=X_{j}-X_{j-1}\\) are i.i.d. for \\(j=1,2,\\ldots\\) with probabilities \\(P(Y_j=1)=p\\) and \\(P(Y_j=-1)=1-p\\). What this tells us is that we can construct other random walks from a previous one. Let \\(X_n\\) be the SSRW. Then \\(-X_n\\) is also a SSRW. Lets check that it satisfies the criteria: \\(-X_0=0\\) (since \\(X_0=0\\) because \\(X_n\\) is a SSRW), and \\((-X_{j})-(-X_{j-1})=-Y_j\\) and \\(P(-Y_j=1)=P(Y_j=-1)=\\frac12\\), \\(P(-Y_j=-1)=P(Y_j=1)=\\frac12\\) (where we use \\(Y_j\\) to represent the individual steps for the original SSRW \\(X_n\\)). Hence \\(-X_n\\) is an SSRW also! One important thing to realize here is that when we write \\(X_n\\) and \\(-X_n\\) we do mean precisely that they are literally reflections of one another. In other words, if we know a particular value, say \\(X_5=3\\), then we know that the reflected process is exactly at level \\(-3\\) for time step \\(5\\). This is true because \\(X_5\\) is a random variable, and so anywhere we see \\(X_5\\), it is the same random variable whose value is determined by the same radom experiment (e.g. simulating a random walk path). Example. Let \\(X_n\\) be the SSRW. Let \\(W_n\\) be a stochastic process, and show that the following are also SSRWs. \\(W_n=-X_n\\) \\(W_n=X_{3+n}-X_3\\) \\(W_n=\\sum_{j=1}^n (X_{2j}-X_{2j-1})\\) with \\(W_0=0\\) Show/hide solution. We covered this example in the text above. This is a shifted random walk. The \\(W_n\\) sample path is identical to the \\(X_n\\) sample path but starting from \\(X_3\\) and being shifted back to starting from the origin. In other words, we can imagine a completely simulated sample path for \\(X_n\\), and we just ignore the first three time steps and take the remaining part of the sample path and shift it back to starting at the origin, preserving its exact shape. We calculate \\(W_0=X_{3+0}-X_3=0\\) and \\(W_j-W_{j-1}=(X_{3+j}-X_3)-(X_{3+j-1}-X_3)=X_{3+j}-X_{3+j-1}=Y_{3+j}\\) and know that \\(Y_4,Y_5,\\ldots\\) are i.i.d. with the desired probabilities. The \\(n^{th}\\) step for \\(W_n\\) is the \\(2n^{th}\\) step for \\(X_n\\). In other words, \\(W_n\\) only considers the even steps. If \\(X_n=\\sum_{j=1}^n Y_j\\), then \\(W_n=\\sum_{j=1}^n Y_{2j}\\). We see that \\(W_0=0\\) is given and calculate \\(W_k-W_{k-1}=\\sum_{j=1}^k (X_{2j}-X_{2j-1})-\\sum_{j=1}^{k-1} (X_{2j}-X_{2j-1})=X_{2k}-X_{2k-1}=Y_{2k}\\). We know that \\(Y_2,Y_4,\\ldots\\) are i.i.d. with equal probability on \\(\\pm1\\). 6.2 Distribution of \\(X_n\\) Since \\(X_n\\) is a random sum of a bunch of plus and minus ones, we can relate it to a sum of Bernoulli random variables. If we think of flipping a fair coin \\(n\\) times and let \\(H\\) be the number of heads and \\(T\\) be the number of tails, then we must have \\(H+T=n\\). If the \\(j^{th}\\) coint flip is heads, we set \\(Y_j=1\\) and if it is tails, we set \\(Y_j=-1\\). In this way, we can reason that \\[X_n=(\\# \\text{ heads})-(\\# \\text{ tails})=H-T=H-(n-H)=2H-n.\\] Since we know that \\(H\\sim\\mathsf{binom}(n,\\frac12)\\) (\\(H\\) is governed by the binomial distribution with \\(n\\) trials and \\(p=\\frac12\\) probability of success), we can use this to calculate probabilities for \\(X_n\\): Distribution of \\(X_n\\) for SSRW. \\[P(X_n=j)=P(2H-n=j)=P\\left(H=\\frac{n+j}{2}\\right)={n\\choose \\frac{n+j}{2}}\\frac1{2^n}.\\] More generally, for the asymmetric random walk, with \\(p\\neq\\frac12\\), we have \\[P(X_n=j)={n\\choose \\frac{n+j}{2}}p^{\\frac{n+j}{2}}(1-p)^{\\frac{n-j}{2}}.\\] Now we refresh the normal approximation to the binomial. Normal approximation to binomial. Let \\(X\\sim\\mathsf{binom}(n,p)\\), then for \\(n\\) large (usually \\(n\\geq 30\\) with \\(np\\geq5\\) and \\(n(1-p)\\geq5\\) is acceptable, but it might still be a rough approximation). Then we can say that \\[X\\overset{\\small approx}{\\sim} \\mathsf{N}(\\mu=np,\\sigma^2=np(1-p)).\\] The binomial is a discrete distribution, but the normal is a continuous distribution. Any time we use a continuous distribution to approximate a discrete distribution, it might be useful to use a continuity correction. Continuity correction. If discrete random variable \\(X\\) has values \\(x_1,x_2,\\ldots\\) and we wish to approximate \\(P(X=x_j)\\) by continuous random variable \\(Y\\), then we can integrate the probability density function of \\(Y\\) from halfway to the next \\(x\\)-values on the left and right: \\[P(X=x_j)\\approx P(x_j-(x_j-x_{j-1})/2 &lt; Y \\leq x_j+(x_j-x_{j-1})/2).\\] For the normal approximation to the binomial random variable \\(X\\sim binom(n,p)\\), this translates to using \\(Y\\sim N(np,np(1-p))\\) and \\[P(X=k)\\approx P\\left(k-\\frac12 &lt; Y \\leq k+\\frac12\\right).\\] Normal approximation to distribution of \\(X_n\\) for SSRW. We have that \\[P(X_n=j)=P(2H-n=j)=P\\left(H=\\frac{n+j}{2}\\right)\\] and that \\(H\\) is binomial distributed with parameters \\(n,p\\) and hence is approximately normally distributed with mean \\(\\mu=np\\) and variance \\(\\sigma^2=np(1-p)\\). Now we can then write \\[P(X_n=j)=P(2H-n=j)=P\\left(H=\\frac{n+j}{2}\\right)\\approx P\\left(\\frac{n+j}{2}-\\frac12&lt;Y\\leq \\frac{n+j}{2}+\\frac12\\right)\\] where \\(Y\\sim\\mathsf N(\\mu=np,\\sigma^2=np(1-p))\\). In R we can calculate normal cumulative probabilities using pnorm(), so the above probability for \\(P(X_n=j)\\) is given by the R code (with the continuity correction) pnorm((n+j)/2+1/2,mean=n*p,sd=sqrt(n*p*(1-p)))-pnorm((n+j)/2-1/2,mean=n*p,sd=sqrt(n*p*(1-p))) Example. Lets calculate \\(P(X_7=-3)\\). We have \\(n=7\\), \\(p=0.5\\), and \\(j=-3\\). The exact calculation using the distribution we derived from the bionomial is: \\[P(X_7=-3)={7\\choose\\frac{7-3}{2}}\\frac1{2^7}={7\\choose2}\\frac1{2^7}=\\frac{7\\cdot 3}{2^7}\\approx0.1640625.\\] And using the normal approximation (with continuity correction) we get: \\(P(X_7=-3)\\approx\\)pnorm(2.5,7/2,sqrt(7)/2)-pnorm(1.5,7/2,sqrt(7)/2)\\(\\approx0.1595609\\) which is a reasonable approximation. We can make the R code a bit simpler though. If \\(X\\sim N(\\mu\\sigma^2)\\) then \\(aX+b\\sim N(a\\mu+b,a^2\\sigma^2)\\). This means that \\(aX+b\\) is a normal random variable as well with \\(E(aX+b)=a\\mu+b\\) and \\(Var(aX+b=a^2\\sigma^2)\\). Since we are approximating \\(H\\), the number of up steps, as a normal random variable, then \\(2H-n\\) is also approximately normally distributed. \\[X_n=2H-n\\overset{\\small approx}{\\sim}\\mathsf{N}(\\mu=2np-n,\\sigma^2=4np(1-p)).\\] And for the SSRW this means \\[X_n=2H-n\\overset{\\small approx}{\\sim}\\mathsf{N}(\\mu=0,\\sigma^2=n).\\] Now to apply the continuity correction is a bit trickier though, because \\(X_n\\) only takes on values \\(-n,-n+2,\\ldots,n-2,n\\). Instead of adding and subtracting \\(\\frac12\\), we must add and subtract \\(1\\) and \\(P(X_n=j)\\) is approximated by pnorm(j+1,0,sqrt(n))-pnorm(j-1,0,sqrt(n)). \\[X_n\\overset{approx}\\sim \\mathsf{N}(\\mu=2np-n,\\sigma^2=4np(1-p))\\] Partitioning. Note that when we calculate something like \\(P(X_2=0)\\), we are implicitly using the ideas of joint distributions, partitioning, and conditioning in teh following way \\[\\begin{aligned} P(X_2=0)&amp;=P(X_1=1,X_2=0)+P(X_1=-1,X_2=0)\\\\ &amp;=P(X_1=1)P(X_2=0\\mid X_1=1)+P(X_1=-1)P(X_2=0\\mid X_1=-1)\\\\ &amp;=\\frac12\\cdot\\frac12+\\frac12\\cdot\\frac12. \\end{aligned}\\] Although, one can work out such probabilities by sketching a diagram and counting paths, etc. Here is another example of the use of conditioning: \\[ \\begin{aligned} P(X_1=-1,X_2=0,X_3=1)&amp;=P(X_1=-1)P(X_2=0,X_3=1\\mid X_1=-1)\\\\ &amp;=P(X_1=-1)P(X_2=0\\mid X_1=-1)P(X_3=1\\mid X_1=-1,X_2=0). \\end{aligned}\\] In general, we can calculate probabilities about the future state of the process given information about its current and past states, so a conditional probability like \\(P(X_3=1\\mid X_1=-1,X_2=0)\\) will make sense given that we know the rules for the stochastic process \\(X_n\\). In most cases, we will only need to know the current state (this is memorylessness or the Markov propertymore on that later). The simple random walk satisfies this, and we have \\[P(X_3=1\\mid X_1=-1,X_2=0)=(X_3=1\\mid X_2=0).\\] 6.3 Shift invariance &amp; memorylessness The simple random walk has properties which well call shift invariance and *memorylessness**. Here \\(X_n\\) is a simple random walk which could be symmetric or asymmetric. Shift invariance works by using the fact that successive steps are independent and identically distributed. As an example, if we know that \\(X_5=3\\), then from that point onward, the process behaves exactly like a random walk starting at initial state \\(3\\) and we can re-sync our clock so that time \\(n=5\\) is our new time zero. This gives that, for example, \\(P(X_6=4\\mid X_5=3)=P(X_1=4\\mid X_0=3)\\). Then we can shift the state lattice as well by subtracting 3 form both sides of the conditional bar: \\(P(X_1=4\\mid X_0=3)=P(X_1=4-3\\mid X_0=3-3)=P(X_1=1\\mid X_0=0)=P(X_1=1)\\) where in the last step, we use the fact that starting form initial state zero is the default so we dont need to specify it as a condition. Shift invariance. \\[P(X_{n+j}=k\\mid X_j=\\ell)=P(X_n=k-\\ell)\\] The memoryless tells us that the distribution for future states is fully determined by the most recently known state. If we know the entire history of the process up to time \\(n-1\\), then to calculate probabilities for time step \\(n\\), we can discard the entire history except for the most recent state. Furthermore, even given incomplete information about the past states, we only care about the most recent one for calculating future probabilities. Memorylessness. If we know the state of the process at time steps \\(s_1&lt;s_2&lt;s_3&lt;\\cdots&lt;s_m&lt;n\\), then to calculate probabilities for \\(X_n\\), we only use the most recent state: \\[P(X_{n}=k\\mid X_{s_j}=x_{s_j} \\text{ for } j=1,2,\\ldots,m)=P(X_{n}=k\\mid X_{s_m}=x_{s_m}).\\] In particular, if we know the full history of the process up to time step \\(n-1\\), then to calculate probabilities for time step \\(n\\), we discard the entire history except for the most recent known state: \\[P(X_{n}=k\\mid X_{n-1}=\\ell_{n-1},X_{n-2}=\\ell_{n-2},\\ldots,X_{1}=\\ell_{1})=P(X_{n}=k\\mid X_{n-1}=\\ell_{n-1}).\\] 6.4 Reflection principle Now well look at a property of the sample path space which will help us understand a few more properties for the random walk. Consider some state \\(m\\) which we are interested in the RW hitting at some time step \\(n=0,1,2,\\ldots,k\\). Clearly we must consider \\(m\\) between \\(-k\\) and \\(k\\) since the RW can only go that far in \\(k\\) time steps. We consider a sample path and the first time it hits level \\(m\\). We create a second sample path by tracing the first one up to this point and then reflecting everything after that initial hit of level \\(m\\). In the following plot, we let \\(m=2\\). The red path is identical to the black dash-dot path up until the first hit of level \\(2\\). After that the red path is a reflection of the black dash-dot path. There is a special pattern here, that any path that ends up, in the end, above level \\(m\\) will be reflected to create a path that ends up below level \\(m\\). Note that we are only considering paths that ultimately hit level \\(m\\), otherwise the reflection acros level \\(m\\) doesnt work. We summarize this below. The number of paths that hit level \\(m\\) and end up strictly above level \\(m\\) is the same as the number of paths that hit level \\(m\\) and end up strictly below level \\(m\\). \\[\\#\\{X_j=m \\text{ for some } j\\leq n, X_n&gt;m\\}=\\#\\{X_j=m \\text{ for some } j\\leq n, X_n&lt;m\\}\\] 6.5 Maximum state reached Let \\(M_n=\\max\\{X_0,X_1,\\ldots,X_n\\}\\) be the maximum level hit by the process up to time step \\(n\\). \\[P(M_n=k)=P(X_n=k)+P(X_{n}=k+1)\\] We derive this using the reflection principle and the fact that \\(P(M_n=k)=P(M_n\\geq k)-P(M_n\\geq k+1)\\). \\[P(M_n\\geq k)=P(M_n\\geq k,X_n&gt;k)+P(M_n\\geq k,X_n=k)+P(M_n\\geq k,X_n&lt;k)\\] We can calculate the middle term using know methods: \\[P(M_n\\geq k,X_n=k)=P(X_n=k).\\] Using the reflection principle, we know that the events \\(\\{M_n\\geq k,X_n&gt;k\\}\\) and \\(\\{M_n\\geq k,X_n&gt;k\\}\\) have the same number of paths. For the SSRW, each of these paths is equally likely, so we get that \\(P(M_n\\geq k,X_n&gt;k)=P(M_n\\geq k,X_n&lt;k)\\) and hence only need to calculate \\(P(M_n\\geq k,X_n&gt;k)\\). But \\(P(M_n\\geq k,X_n&gt;k)=P(X_n&gt;k)\\) which we can calculate from known formulas: \\[P(M_n\\geq k,X_n&gt;k)=P(X_n&gt;k)=\\sum_{j=k+1}^n P(X_n=j).\\] Putting all this together we get (for the SSRW with \\(p=0.5\\)): \\[\\begin{aligned} P(M_n\\geq k)&amp;=P(M_n\\geq k,X_n&gt;k)+P(M_n\\geq k,X_n=k)+P(M_n\\geq k,X_n&lt;k)\\\\ &amp;=2P(M_n\\geq k,X_n&gt;k)+P(M_n\\geq k,X_n=k)\\\\ &amp;=2P(X_n&gt;k)+P(X_n=k).\\\\ \\end{aligned}\\] And finally we see that \\[\\begin{aligned} P(M_n= k)&amp;=P(M_n\\geq k-P(M_n\\geq k+1)\\\\ &amp;=(2P(M_n\\geq k,X_n&gt;k)+P(M_n\\geq k,X_n=k)\\\\ &amp;=(2P(X_n&gt;k)+P(X_n=k))-(2P(X_n&gt;k+1)+P(X_n=k+1))\\\\ &amp;=2P(X_n&gt;k+1)+2P(X_n=k+1)+P(X_n=k)-2P(X_n&gt;k+1)-P(X_n=k+1)\\\\ &amp;=P(X_n=k+1)+P(X_n=k).\\\\ \\end{aligned}\\] 6.5.1 Maximum over infinite sample paths for \\(p&lt;1/2\\) If \\(p&lt;\\frac12\\) then the random walk will eventually hit some nonnegative maximum level and then go towards \\(-\\infty\\) without ever exceeding its historical maximum level. Let \\(M_\\infty=\\max\\{X_0,X_1,\\ldots\\}\\) be the maximum state reached over the entire sample path with infinitely-many time steps. Then we have \\[P(M_\\infty=k)=\\frac{1-2p}{1-p}\\left(\\frac{p}{1-p}\\right)^k \\ \\text{ for } \\ k=0,1,\\ldots.\\] Hence \\(M_\\infty\\) is a geometric random variable where we can think of a success as wander off to \\(-\\infty\\) and never exceeding its previous maximum. The probability the walk (starting at \\(X_0=0\\)) never goes above zero is \\(\\frac{1-2p}{1-p}\\). The probability it hits level 1 eventually is \\(\\frac p{1-p}\\). From that point, it is probability \\(\\frac{1-2p}{1-p}\\) that it never exceeds level 1. Working inductively, we can reason that \\(\\left(\\frac p{1-p}\\right)^k\\) is the probability it eventually hits level \\(k\\) and then multiplying by \\(\\frac{1-2p}{1-p}\\) that the walk never again exceeds its current level. 6.6 Hitting times We may be interested in how long it takes the random walk to hit a certain state. We refer to this as a hitting time. For example, if we are modeling a stock price with a random walk, and buy the stock for $250 and we wish to gain a profit of $5, then we might be interested in the how long we expect to wait for the stock to hit $255. 6.6.1 Hitting time for state \\(1\\) We wish to know how long it takes for the SARW to hit level \\(1\\). Let \\(T_1=\\min\\{n&gt;0 : X_n=1\\}\\) which we call the hitting time for state \\(1\\). Theorem (Hitting time for state \\(1\\)). \\[P(T_1=2k+1)=\\frac1{k+1}{2k\\choose k}p^{k+1}(1-p)^k, \\ k=0,1,2,\\ldots.\\] Show/hide proof. Proof. Well first see that the number of paths that end at \\(X_{2k}=0\\) and hit level 1 somewhere before that is the same as the number of paths that start at \\(X_0=0\\) and end at \\(X_{2k}=2\\) See the graph below. Draw any path for yourself that goes form \\(X_0=0\\) to \\(X_{2k}=2\\) for any \\(k\\)-value. Then figure out the first time it hits level 1 and reflect the rest of the path after that around level 1. Then you get a path that goes form \\(X_0=0\\) to \\(X_{2k}=0\\) and it hits level 1 somewhere. This is the number of paths in the event \\(\\{M_{2k}\\geq1,X_{2k}=0\\}\\). An example is shown below Now \\(P(T_1=2k+1)\\) is exactly the probability that the walk hits state \\(0\\) at time \\(2k\\) (and has never hit state \\(1\\) at all) and then goes up to state \\(1\\): \\[P(T_1=2k+1)=P(X_{2k}=0 \\text{ and has never hit state 1 (by time $2k$)}) \\cdot p .\\] The event \\(\\{\\text{has never hit state 1}\\}\\) is exactly the event that the maximum is zero: \\[\\{\\text{has never hit state 1 (by time $2k$)}\\}=\\{M_{2k}=0\\}.\\] So we have \\[P(T_1=2k+1)=P(X_{2k}=0, M_{2k}=0) \\cdot p .\\] Now we know how many paths have \\(X_{2k}=0\\), and we need to subtract form that event, the number of paths that go above \\(0\\) somewhere, i.e. the number of paths in the event \\(\\{X_{2k}=0,M_{2k}\\geq1\\}\\). We have already worked out above (by the reflection principle) that this is the same as in \\(\\{X_{2k}=2\\}\\). So performing the counting of paths we have: \\[\\# \\{X_{2k}=0, M_{2k}\\geq 1 \\}=\\#\\{X_{2k}=2\\}={2k\\choose k-1}.\\] And hence, the number of paths which never hit state \\(1\\) by time \\(2k\\) is \\[ \\begin{aligned} \\# \\{ X_{2k}=0, M_{2k}=0 \\}&amp;=\\# \\{X_{2k}=0\\}-\\# \\{X_{2k}=0, M_{2k}\\geq1 \\}\\\\ &amp;={2k\\choose k}-{2k\\choose k-1}\\\\ &amp;=\\frac{(2k)!}{k!k!}-\\frac{(2k)!}{(k-1)!(k+1)!}\\\\ &amp;=\\frac{(2k)!}{(k-1)!k!} \\left(\\frac1k-\\frac1{k+1}\\right)\\\\ &amp;=\\frac{(2k)!}{(k-1)!k!} \\cdot\\frac1{k(k+1)}\\\\ &amp;=\\frac1{k+1}{2k\\choose k}. \\end{aligned}\\] Now since every path that starts at \\(X_0=0\\) and ends at \\(X_{2k}=0\\) is equally likely with \\(k\\) up steps and \\(k\\) downsteps, they each have probability \\(p^k(1-p)^k\\), and there are \\(\\frac1{k+1}{2k\\choose k}\\) of those paths that never hit state \\(1\\). Now we put this all together to see that \\[\\begin{aligned} P(T_1=2k+1)&amp;=P(X_{2k}=0, M_{2k}=0) \\cdot p \\\\ &amp;=\\# \\{ X_{2k}=0, M_{2k}=0 \\} p^k (1-p)^k \\cdot p\\\\ &amp;=\\frac1{k+1}{2k\\choose k}p^{k+1}(1-p)^k. \\end{aligned}\\] \\(\\square\\) By symmetry, we can understand the distribution for \\(T_{-1}\\), the hitting time for state \\(-1\\) as well. For the SSRW, \\(P(T_{-1}=k)=P(T_1=k)\\) since there is no difference in the probabilities of going up or down. In fact \\(-X_n\\) (the reflected SSRW) has the same distribution as \\(X_n\\), and \\(T_{-1}\\) for \\(X_n\\) is exactly the same thing as \\(T_1\\) for \\(-X_n\\). Since the latter is also a SSRW, then \\(T_1\\) is exactly distributed according to the above given formula. If the walk was asymmetric with \\(p\\neq\\frac12\\), then \\(T_{-1}\\) is distributed according to \\(T_1\\) for the reflected (about 0) random walk, but with probabilities of up and down steps swapped. E.g. \\(P(T_{-1}=k)\\) for the SARW with \\(p\\) probability of up step is identical to \\(P(T_{1}=k)\\) for the SARW with \\(1-p\\) probability of up step. We also have that \\(E(T_1)=\\infty\\) for \\(p\\leq 1/2\\) and that, for \\(p&gt;1/2\\), \\[E(T_1)=\\frac1{2p-1}.\\] 6.6.2 Hitting time for other states If we are interested in how long it takes for the process to hit state \\(2\\), then we wait for it to hit state \\(1\\). From this point, we only need to wait for it to go up another +1 from the current state, which is equivalent for waiting a random time which is distributed exactly the same as the hitting time for state one. Let \\(T_2\\) be the hitting time for state \\(2.\\) What this reasoning shows is that \\(T_2=\\tau_1+\\tau_2\\) where \\(\\tau_1\\) and \\(\\tau_2\\) have the same distribution as \\(T_1\\) and are independent. In general, letting \\(T_k\\) be the hitting time for state \\(k\\), we have \\[T_k=\\sum_{j=1}^k \\tau_j\\] where \\(\\tau_j\\) are i.i.d. and distributed identically to \\(T_1\\). Example. For example \\(T_2=\\tau_1+\\tau_2\\). If we wish to calculate \\(P(T_2=4)\\) then we need to consider all possibilities for \\(\\tau_1,\\tau_2\\) that sum to give us 4 total time steps, and we use independence of the \\(\\tau_j\\). \\[\\begin{aligned} P(T_2=4)&amp;=P(\\tau_1+\\tau_2=4)\\\\ &amp;=P(\\tau_1=1,\\tau_2=3)+P(\\tau_1=3,\\tau_2=1)\\\\ &amp;=P(\\tau_1=1)P(\\tau_2=3)+P(\\tau_1=3)P(\\tau_2=1)\\\\ &amp;=P(T_1=1)P(T_1=3)+P(T_1=3)P(T_2=1)\\\\ &amp;=2P(T_1=1)P(T_1=3)\\\\ &amp;=2 \\frac1{0+1}{2\\cdot 0\\choose 0}p^{0+1}(1-p)^0 \\frac1{1+1}{2\\cdot 1\\choose 1}p^{1+1}(1-p)^1 \\\\ &amp;=2 p^3 (1-p) \\\\ \\end{aligned}\\] The hitting time theorem is given below and allows us to calculate hitting time probabilities for any state for the SARW with any \\(p\\)-value. Hitting time theorem. Let \\(X_n\\) be a simple random walk with \\(p\\) the probability of stepping up. Let \\(T_k\\) be the hitting time for level \\(k\\geq0\\). We have \\[P(T_k=m)=\\frac km P(X_m=k)\\] Here is an example usage of the hitting time theorem. Example. Let \\(X_n\\) be a simple random walk with \\(p\\) the probability of stepping up. Let \\(T_k\\) be the hitting time for level \\(k\\geq0\\). We have \\[P(T_3=5)=\\frac 35 P(X_5=3)=\\frac35 {5\\choose4}p^4(1-p)=3p^4(1-p).\\] The hitting time \\(T_k\\) for state \\(k\\) can be thought of in the following way. We wait for the walk to hit state 1 (a \\(T_1\\) random number of time steps), and then from there, we consider it to be starting a new walk and then wait for it to go up one level from its new starting point (this is another \\(T_1\\) random number of time steps), and then repeat this always waiting for it to go up one level. Each time we wait for it to go up \\(+1\\) level, we wait for a random number of time steps, and each of these waits is equivalent to \\(T_1\\) the time to hit level one. Since non-overlapping time periods are independent, we have a sequence of independent random wait times i.e. a sequence of i.i.d. \\(T_1\\)s. For each \\(j\\in\\mathbb N\\), let \\(T_1^{(j)}\\) be a copy of the wait time to hit state 1 \\(T_1\\). This means that we can plug in whatever we want for \\(j\\) and well always have \\(P(T_1^{(j)}=m)=P(T_1=m)\\). We can think of \\(T_k\\) as the sum of \\(k\\) of these random times: \\[T_k=\\sum_{j=1}^kT_1^{(j)}.\\] Now we can calculate the expected value using linearity of expected value operator: \\[E(T_k)=E\\left(\\sum_{j=1}^kT_1^{(j)}\\right)=\\sum_{j=1}^kE\\left(T_1^{(j)}\\right)=kE(T_1).\\] So now, we can calculate the expected wait time to hit any level using the expected wait time to hit level one! Theorem. \\[E(T_k)=k E(T_1)\\] 6.7 Return time to state \\(0\\) We have that \\(X_0=0\\) initially. We want to know how long it takes for the random walk to return to state zero. Let \\(T_0=\\min\\{n&gt;0 : X_n=0\\}\\) which we call the return time to zero. We can calculate \\(P(T_0=2k)\\) for \\(k=1,2,\\ldots\\) by sketching a graph and counting paths. For example, \\(P(T_0=2)=\\frac12\\) since this means the first two time steps are up then down or vice versa. Similarly, we can calculate \\(P(T_0=4)=\\frac18\\). The general formula is given below. Return time to state \\(0\\). \\[P(T_0=2k)=\\frac1{2k-1}{2k\\choose k}p^{k}(1-p)^k, \\ k=1,2,\\ldots.\\] This is also the return time to the initial state more generally, if we initialized the walk with \\(X_0\\neq0\\). Here is a derivation of the formula using conditioning on the initial state. Conditioning in this way is a very important technique which can often simplify computations. \\[ \\begin{aligned} P(T_0=2k)&amp;=P(T_0=2k\\mid X_1=1)P(X_1=1)+P(T_0=2k\\mid X_1=-1)P(X_1=-1)\\\\ &amp;=P(T_0=2k\\mid X_1=1)p+P(T_0=2k\\mid X_1=-1)(1-p)\\\\ &amp;=P(T_{-1}=2k-1)p+P(T_1=2k-1)(1-p)\\\\ \\end{aligned} \\] This works because conditioning on \\(X_1=1\\), in order to return back to state \\(0\\), we must wait for how long it takes the walk to first go down one level, and (by shift invariance) this probability is identical to starting at zero and waiting to hit state \\(-1\\). Now we already know how to calculate probabilities for \\(T_1\\) and \\(T_{-1}\\) (which is identical to \\(T_1\\) for a reflected walk with \\(p\\) and \\(1-p\\) swapped). \\[P(T_1=2k-1)=\\frac1{k}{2k-2\\choose k-1}p^{k}(1-p)^{k-1}\\] \\[P(T_{-1}=2k-1)=\\frac1{k}{2k-2\\choose k-1}p^{k-1}(1-p)^{k}\\] \\[ \\begin{aligned} P(T_0=2k)&amp;=\\frac2{k}{2k-2\\choose k-1} p^k(1-p)^k\\\\ \\end{aligned} \\] Now we just need to show the coefficient simplifies to \\(\\frac1{2k-1}{2k\\choose k}.\\) \\[\\begin{aligned} \\frac2{k}{2k-2\\choose k-1}&amp;=\\frac2k \\frac{(2k-2)!}{(k-1)!(k-1)!}\\\\ &amp;=\\frac2{\\color{blue}{k}} \\frac{\\color{red}{(2k-2)!}}{\\color{blue}{(k-1)!}\\color{green}{(k-1)!}} \\cdot \\frac{\\color{red}{2k(2k-1)}}{2\\color{green}{k}(2k-1)}\\\\ &amp;=\\frac{\\color{red}{(2k)!}}{\\color{blue}{k!}\\color{green}{k!}} \\cdot \\frac{1}{(2k-1)}\\\\ &amp;={2k \\choose k} \\cdot \\frac{1}{(2k-1)} \\end{aligned}\\] This finishes the calculation. Here is a table for returns to zero with \\(T_0\\) the wait time to return to zero and \\(N\\) the number of times that we return to zero. Note that the actual number of times the walk is in state zero is \\(N+1\\) since it always starts in state zero. \\(r=P(T_0&lt;\\infty)\\) \\(P(T_0=\\infty)\\) \\(E(T_0)\\) \\(P(N&lt;\\infty)\\) \\(P(N=\\infty)\\) \\(E(N)=\\frac{r}{1-r}\\) \\(p&lt;\\frac12\\) \\(2p\\) \\(1-2p\\) \\(\\infty\\) 1 0 \\(\\frac{2p}{1-2p}\\) \\(p=\\frac12\\) 1 0 \\(\\infty\\) 0 1 \\(\\infty\\) \\(p&gt;\\frac12\\) \\(2(1-p)\\) \\(2p-1\\) \\(\\infty\\) 1 0 \\(\\frac{2-2p}{2p-1}\\) Summary Summary of notation, formulas, and terminology For \\(X_n\\) the simple 1D random walk with \\(p\\) probability of an up step (and \\(1-p\\) down step): Distribution of \\(X_n\\): \\(P(X_n=j)={n\\choose (n+j)/2}p^{(n+j)/2}(1-p)^{(n-j)/2}\\)     For SSRW: \\(P(X_n=j)={n\\choose (n+j)/2}\\frac1{2^n}\\) \\(P(X_n=j)\\approx\\)pnorm(j+1,2*n*p-n,sqrt(4*n*p*(1-p)))-pnorm(j-1,2*n*p-n,sqrt(4*n*p*(1-p)))     For SSRW: \\(P(X_n=j)\\approx\\)pnorm(j+1,0,sqrt(n)-pnorm(j-1,0,sqrt(n)) Shift invariance &amp; memorylessness: \\(P(X_{n+m}=k\\mid X_m=j)=P(X_{n}=k-j)\\) \\(P(X_{n}=k\\mid X_{n-1}=j,X_{n-2}=a_{n-2},\\ldots,X_{2}=a_{2},X_{1}=a_{1})=P(X_{n}=k\\mid X_{n-1}=j)\\) Maximum of SSRW: \\(P(M_n=k)=P(X_n=k)+P(X_n=k+1)\\), \\(M_n=\\) max level reached in first \\(n\\) time steps Maximum of SARW w/ \\(p&lt;1/2\\): \\(P(M_\\infty=k)=\\left( \\frac p{1-p}\\right)^k\\left( \\frac {1-2p}{1-p}\\right)\\), \\(M_\\infty=\\) max level reached over the entire walk covering (for infinitely many time steps) Hitting times: \\(P(T_1=2k+1)=\\frac1{k+1}{2k\\choose k}\\frac1{2^{2k+1}}\\), \\(T_1=~\\)first time SSRW (\\(p=\\frac12\\)) to hit state \\(1\\) \\(P(T_1=2k+1)=\\frac1{k+1}{2k\\choose k}p^{k+1}(1-p)^k\\), \\(T_1=~\\)first time SARW (any \\(p\\in[0,1]\\)) to hit state \\(1\\) \\(P(T_k=m)=\\frac mk P(X_m=k)\\), \\(T_k=~\\)first time SARW hits state \\(k\\) Return time to initial state: \\(P(T_0=2k)=\\frac1{2k-1}{2k\\choose k}p^{k}(1-p)^k\\), \\(T_0=~\\)first time SARW returns to state \\(0\\) "],["limit-theorems.html", "Chapter 7 Limit theorems 7.1 Inequalities 7.2 Law of large numbers 7.3 Central limit theorem 7.4 Borel-Cantelli", " Chapter 7 Limit theorems 7.1 Inequalities 7.2 Law of large numbers 7.3 Central limit theorem 7.4 Borel-Cantelli Here we must consider an infinite list of events. As a baseline example consider all possible infinite sequences of coin flips. For each \\(n\\), let \\(A_n\\) be the event that the \\(n^{th}\\) coin flip is heads, \\(A_n=\\{\\text{heads on } n^{th} \\text{ flip}\\}\\). If the coin is a fair coin, we know that a typical sequence of infinitely-many coin flips will have infinitely-many heads (and infinitely-many tails). We state this as: the coin flip is heads infinitely-often. This means that the event \\(A_n\\) occurs for infinitely-many values of \\(n\\). And we write this symbolically as \\(\\mathsf P(A_n \\text{ i.o.})\\) with i.o. an abbreviation for infinitely often. First lets convince ourselves that the probability of having infinitely-many tails consecutively in a row is zero. Let \\(A_n^c=\\{n^{th}\\text{ flip is tails}\\}\\). Then we have that \\[\\{\\text{all tails}\\}=\\{1^{st}\\text{ flip is tails}\\}\\text{ and } \\{2^{nd}\\text{ flip is tails}\\} \\text{ and } \\cdots=\\cap_{n=1}^\\infty A_n^c.\\] Now define event that the first \\(M\\) flips are all tails by \\[D_M=\\{\\text{first }M\\text{ flips are all tails}\\}=\\cap_{n=1}^M A_n^c.\\] Now we have a decreasing sequence of events: \\[D_1\\supset D_2\\supset\\cdots.\\] By the continuity of probability (for increasing or decreasing sequences of events) we have that \\[\\mathsf P(\\lim_{M\\to\\infty} D_M)=\\lim_{M\\to\\infty} \\mathsf P(D_M).\\] Since the coin is fair, we have \\(\\mathsf P(D_M)=\\frac1{2^M}\\). This shows that the limit above is indeed zero. Now we finally are guaranteed that the probability of flipping all tails is zero 9for an infinite sequence of coin flips). Now, we can intuitively understand that \\(\\mathsf P(A_n \\text{ i.o.})=1\\). If we didnt have infinitely-many heads, then the sequence of coin flips ends in an infinite string of tails. The axioms of probability show that an infinite string of consecutive tails has probability zero. Let \\(E_n\\) be the event that flip \\(n\\) onwards is all tails. We have that \\(\\mathsf P(E_n)=0\\) for any \\(n\\) by the above work. The events are related by \\[\\{A_n \\text{ i.o.}\\}=\\cap_{n=1}^\\infty\\{\\text{not all tails from flip }n\\text{ onward}\\}=\\cap_{n=1}^\\infty E_n^c.\\] Now, \\[\\mathsf P(A_n \\text{ i.o.})=\\mathsf P(\\lim_{N\\to\\infty}\\cap_{n=1}^N E_n^c)=\\lim_{N\\to\\infty}\\mathsf P(\\cap_{n=1}^N E_n^c)\\] since we again have a decreasing sequence of events \\[(\\cap_{n=1}^1 E_n^c) \\supset (\\cap_{n=1}^2 E_n^c) \\supset\\cdots.\\] Now we also have that, for any \\(N\\), \\[\\mathsf P(\\cap_{n=1}^N E_n^c)=1-\\mathsf P(\\cup_{n=1}^N E_n)\\geq1-\\sum_{n=1}^N\\mathsf P( E_n)=1\\] since \\(\\mathsf P(\\cup_{n=1}^N E_n)\\leq\\sum_{n=1}^N\\mathsf P( E_n)\\) by Booles inequality and \\(\\mathsf P(E_n)=0\\) always. This finally shows that \\(\\mathsf P(A_n \\text{ i.o.})=1\\). We did this the hard way, but now we give a result that answers this question more easily and more generally. What happens if the probability of heads changes over time? In particular, what is the probability of heads decreases for each subsequent flip? The following two results show that if the probability fo heads decreases fast enough, then we eventually stop getting heads at some point with probability 1, and if the probability of heads decreases slow enough, then we still get infinitely-many heads with probability 1. Borel-Cantelli Lemma. Let \\(\\{A_n\\}_{n\\in\\mathbb N}\\) be a sequence of events. If \\(\\sum_{n=1}^\\infty\\mathsf P(A_n)&lt;\\infty\\), then \\(\\mathsf P(A_n \\text{ i.o.})=0\\). Converse to Borel-Cantelli. Let \\(\\{A_n\\}_{n\\in\\mathbb N}\\) be a sequence of independent events. If \\(\\sum_{n=1}^\\infty\\mathsf P(A_n)=\\infty\\), then \\(\\mathsf P(A_n \\text{ i.o.})=1\\). Example. Consider a coin where the probability of heads decreases at each step. Let \\(\\mathsf P(\\text{heads on }n^{th}\\text{ flip})=\\frac{3^n}{4^n}\\) for each \\(n=1,2,\\ldots\\). Since \\(\\sum_{n=1}^\\infty \\frac{3^n}{4^n}=\\frac34\\frac{1}{1-\\frac34}=3\\) by the geometric series formula, we get that \\(\\mathsf P(\\text{infinitely-many heads})=0\\). If instead, we let \\(P(\\text{heads on }n^{th}\\text{ flip})=\\frac1n\\) (and assume that each coin flip is independent of all others), then we get that \\(\\mathsf P(\\text{infinitely-many heads})=1\\) by the converse to Borel-Cantelli. This is interesting that if the probability of heads decreases (even to zero in the limit), we can still get infinitely-many heads! "],["markov-chains.html", "Chapter 8 Markov Chains 8.1 Graph of a Markov chain 8.2 Classification of states 8.3 Distribution at time \\(n\\) 8.4 Simulating a Markov chain in R 8.5 Return times and hitting probabilities 8.6 Limiting probabilities Summary", " Chapter 8 Markov Chains Markov chains are one of the most important classes of stochastic process. A discrete-time Markov chain (DTMC) consists of specifying a state space and a transition matrix. For us, our state space will generally be \\(S=\\{0,1,\\ldots,k\\}\\) and our time index set will be \\(\\mathbb N_0\\). The transition matrix gives us the transition probabilities between each pair of states and is given as \\[T= \\left(\\begin{matrix} T_{00} &amp; T_{01} &amp; \\cdots &amp; T_{0k}\\\\ T_{10} &amp; T_{11} &amp; \\cdots &amp; T_{1k}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ T_{k0} &amp; T_{k1} &amp; \\cdots &amp; T_{kk}\\\\ \\end{matrix}\\right)\\] where \\[T_{ij}=P(X_n=j\\mid X_{n-1}=i).\\] We assume this stochastic process satisfied the Markov property or memorylessness, which means that the probabilities for the next time step only depends on the current state and no other previous history: \\[P(X_n=j \\mid X_{n-1}=i,X_{n-2}=\\ell_{n-2},\\ldots,X_{1}=\\ell_{1},X_{0}=\\ell_{0})=P(X_n=j \\mid X_{n-1}=i)=T_{ij}.\\] 8.1 Graph of a Markov chain For a Markov chain with state space \\(S\\), we can represent the possible transitions graphically in Euclidean space. For now, lets assume the state space is finite, but even chains with infinite state spaces can have graphs constructed in this way. Plot a point for each state in the plane \\(\\mathbb R^2\\), and label each point by the state it corresponds to. Draw a directed edge (an arrow) from \\(i\\) to \\(j\\) whenever \\(T_{ij}&gt;0\\). We can use install.packages(\"igraph\") if we wish to plot transition graphs for Markov chains. Consider the transition matrix \\[ T=\\begin{pmatrix} 0.5 &amp; 0.3 &amp; 0.2\\\\ 0 &amp; 0.3 &amp; 0.7\\\\ 0.5 &amp; 0.5 &amp; 0 \\end{pmatrix}. \\] Here is some R code for plotting the transition diagram along with the plot. library(igraph) tmvec &lt;- c(0.5,0.3,0.2, 0.0,0.3,0.7, 0.5,0.5,0.0) TM &lt;- matrix(tmvec,byrow=T,nrow=3) mcg &lt;- graph_from_adjacency_matrix(1*(TM&gt;0)) plot.igraph(mcg, vertex.size=50, vertex.color=&quot;grey90&quot;, edge.curved=0.25, edge.color=&quot;black&quot;, edge.label=tmvec[tmvec&gt;0], edge.loop.angle=c(pi,0,0,4*pi/2,0,0,0,0,0), margin=0.5, loop.size=2, layout = matrix(c(0,1,2,1,1,0),byrow=T,nrow=3)) 8.2 Classification of states State \\(j\\) is accessible from state \\(i\\), denoted \\(i\\to j\\) if there is a path from state \\(i\\) to state \\(j\\) in the graph. This means that \\((T^n)_{ij}&gt;0\\) for some \\(n\\geq0\\). Note that \\(i\\to i\\) is always trivially true for any state \\(i\\). States \\(i\\) and \\(j\\) communicate if they are both accessible from each other, i.e. that \\(i\\to j\\) and \\(j\\to i\\). This is denoted \\(i\\leftrightarrow j\\). Again, state \\(i\\) always trivially communicates with itself, \\(i\\leftrightarrow i\\). Communication and accessibility are transitive, that is, \\(i\\to j\\) and \\(j\\to k\\) implies \\(i\\to k\\), and if \\(i\\) communicates with \\(j\\) and \\(j\\) communicates with \\(k\\), then \\(i\\) communicates with \\(k\\). Normally, we break the state space into (communication) classes. A class is a subset of the state space so that all states in the class communicate with each other, but do not communicate with any other states. A Markov chain is called irreducible if the entire state space is a class. It is called reducible if its state space is made of more than one class. As a trivial example, the identity matrix (ones on the diagonal) is a transition matrix with each state being in a class by itself. The chain just starts in some state and stays there forever! A state is called transient if, when starting in that state, there is a positive probability of leaving and never returning. A state is called recurrent if, when starting in that state, with probability one, the chain will always hit that state again at some finite future time. Given that the chain starts in state \\(i\\), let \\(f_i\\) be the probability that it is again in state \\(i\\) at any future time \\(n\\geq1\\). Definition. State \\(i\\) is called recurrent if , when starting in state \\(i\\), the probability that it is visited again in the future is one. That is \\[f_i=\\mathsf P(\\cup_{n=1}^\\infty \\{X_n=i\\}\\mid X_0=i)=1.\\] State \\(i\\) is called transient if , when starting in state \\(i\\), the probability that it is visited again in the future is less than one. That is \\[f_i=\\mathsf P(\\cup_{n=1}^\\infty \\{X_n=i\\}\\mid X_0=i)&lt;1.\\] Example. For the transition matrix given above, we have: \\(1\\leftrightarrow2\\leftrightarrow3\\) and we have a single class \\(\\{0,1,2\\}\\). This is an irreducible Markov chain since its state space is a class. Example. Consider transition matrix with state space \\(S=\\{0,1,2,3,4\\}\\) where positive entries are denoted by \\(\\star\\)s: \\[T=\\begin{pmatrix} \\star&amp;0&amp;\\star&amp;0&amp;0\\\\ 0&amp;0&amp;\\star&amp;0&amp;\\star\\\\ \\star&amp;0&amp;\\star&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;\\star&amp;\\star\\\\ 0&amp;0&amp;0&amp;\\star&amp;0\\\\ \\end{pmatrix}.\\] This has classes \\(\\{0,1,2\\}\\) and \\(\\{3,4\\}\\). Note that the latter is accessible from the former, but not vice versa: \\(\\{0,1,2\\}\\to\\{3,4\\}\\). This is an reducible Markov chain since its state space is not a single class. Eventually, the chain will get absorbed into class \\(\\{3,4\\}\\) and stay there forever. 8.3 Distribution at time \\(n\\) Let \\(\\mathbf p_n\\) be the distribution of the process at time \\(n\\). We call \\(\\mathbf p_0\\) the initial distribution of the process. For a process on state space \\(S=\\{0,1,2,\\ldots,m\\}\\), we have \\[\\mathbf p_n=\\big(P(X_n=0),P(X_n=1),P(X_n=2),\\ldots,P(X_n=m)\\big)\\] Note that this is actually conditional on knowing the initial distribution usually, i.e. when we write \\(\\mathbf p_n\\) and we ask about \\(P(X_n=i)\\), we normally mean \\(P(X_n=i\\mid X_0\\sim \\mathbf p_0)\\). However, we could be conditioning on the state at any earlier time as well. For example, with \\(S=\\{0,1,2,3\\}\\), a point mass on state \\(2\\) initially is \\(\\mathbf p_0=(0,0,1,0)\\) which indicates that \\(X_0=2\\) with certainty. If we wish to select the initial state randomly between states 1 and 2 with a fair coin flip, then \\(\\mathbf p_0=(0,0.5,0.5,0)\\). The future distribution of the process is given by matrix multiplication: \\[\\mathbf p_n = \\mathbf p_{n-1} T.\\] And from this, we can get the distribution at any future time: \\[\\mathbf p_{n} = \\mathbf p_{0} T^n,\\] \\[\\mathbf p_{n+m} = \\mathbf p_{n} T^m.\\] We can think of the transition matrix raised to a power \\(T^n\\) as the \\(n\\)-step transition matrix. Be careful to note that this is matrix exponentiation using standard matrix multiplication. With \\[ T=\\begin{pmatrix} 0.5 &amp; 0.3 &amp; 0.2\\\\ 0 &amp; 0.3 &amp; 0.7\\\\ 0.5 &amp; 0.5 &amp; 0 \\end{pmatrix}, \\] if we start the process with initial distribution \\(\\mathbf p_0=(0.7,0.1,0.2)\\) then the distribution at time one is \\[\\begin{aligned} X_1\\sim \\mathbf p_1 &amp;=\\mathbf p_0 T\\\\ &amp;=(0.7,0.1,0.2)\\begin{pmatrix} 0.5 &amp; 0.3 &amp; 0.2\\\\ 0 &amp; 0.3 &amp; 0.7\\\\ 0.5 &amp; 0.5 &amp; 0 \\end{pmatrix}\\\\[4px] &amp;=(0.45, 0.34, 0.21) \\end{aligned} \\] we can do this in R with the following code (assuming you already have TM enterd) x &lt;- c(0.7,0.1,0.2) x %*% TM ## [,1] [,2] [,3] ## [1,] 0.45 0.34 0.21 Hence \\(\\mathbf p_1=(0.45, 0.34, 0.21)\\) as desired. And the distribution of the process at time \\(n=5\\) is \\[\\begin{aligned} X_5\\sim \\mathbf p_5 &amp;=\\mathbf p_0 T^5\\\\ &amp;=(0.7,0.1,0.2)\\begin{pmatrix} 0.5 &amp; 0.3 &amp; 0.2\\\\ 0 &amp; 0.3 &amp; 0.7\\\\ 0.5 &amp; 0.5 &amp; 0 \\end{pmatrix}^5\\\\[4px] &amp;\\approx(0.31946, 0.364344, 0.316196) \\end{aligned} \\] And in R this is given below. Note that we need to use the expm matrix exponential library. Also note the parentheses around the transition matrix raised to a power (TM %^% 5). library(expm) x &lt;- c(0.7,0.1,0.2) x %*% (TM %^% 5) ## [,1] [,2] [,3] ## [1,] 0.31946 0.364344 0.316196 8.4 Simulating a Markov chain in R Sampling from a discrete distribution can be accomplished as follows. Let \\(\\mathbb p=(p_0,p_1,p_2,\\ldots,p_{m-1},p_m)\\) be a probability mass function on state space \\(S=\\{0,1,2,\\ldots,m\\}\\). If we wish to sample formt he state space according to this distribution we can accomplish this in R using sample(0:m,1,prob=c(p0,p1,\\ldots,pm)). Suppose we have a Markov chain with state space \\(\\{0,1,2,3,4,5\\}\\) and we wish to choose the initial state \\(X_0\\) from initial distribution \\(\\mathbf p_0=(0.2,0,0.1,0.4,0.2,0.1)\\). We can accopmlish this with the following code. sample(0:5,1,prob=c(0.2,0,0.1,0.4,0.2,0.1)) Now, once we know the precise current state of the process, we select the state at the next timestep by randomly sampling from the state space according to the appropriate row of the transition matrix. If \\(X_{n-1}=i\\), we sample \\(X_n\\) using the \\(i^{th}\\) row of \\(T\\). x0 &lt;- sample(0:2,1,prob=c(0.2,0.7,0.1)) x1 &lt;- sample(0:2,1,prob=TM[x0+1,]) Note, that since we index our sample space starting from zero, we must add one to the state to get Rs index number, i.e. state \\(i\\) corresponds to row \\(i+1\\) by Rs indexing scheme. Now we just repeat this procedure for any number of timesteps. Here is a full R code that sets the transition matrix, samples the initial state randomly and simulates the chain for some number of timesteps and plots the resulting sample path. TM &lt;- matrix(c(0.5,0.3,0.2, 0.0,0.3,0.7, 0.5,0.5,0.0),byrow=T,nrow=3) S &lt;- 0:(nrow(TM)-1) nsteps &lt;- 25 initdistr &lt;- c(0.2,0.5,0.3) x &lt;- numeric(length=nsteps+1) x[1] &lt;- sample(S,1,prob=initdistr) for (n in 1:nsteps){ x[n+1] &lt;- sample(S,1,prob=TM[x[n]+1,]) } plot(0:nsteps,x,type=&quot;b&quot;,lwd=2,pch=20,col=rgb(0.7,0.2,0.5),xlab=&quot;time&quot;,ylab=&quot;state&quot;) 8.5 Return times and hitting probabilities Let \\(f_i\\) be the probability that, when starting in state \\(i\\), the chain ever returns to state \\(i\\) at some point in the future. \\[ \\begin{aligned} f_i&amp;=\\mathsf{P}(X_n=i \\text{ for some } n\\in\\mathbb N\\mid X_0=i)\\\\ &amp;=\\mathsf{P}(X_1=i \\text{ or } X_2=i \\text{ or } \\cdots\\mid X_0=i)\\\\ &amp;=\\mathsf{P}\\left(\\cup_{n=1}^\\infty \\{X_n=i\\} \\mid X_0=i\\right) \\end{aligned} \\] Then \\(1-f_i\\) is the probability that the chain immediately leaves state \\(i\\) and never returns. It is possible that \\(f_i=1\\) or \\(f_i=0\\) also. Consider the trivial chain with a single state \\(S=\\{0\\}\\) that just stays there forever which implies \\(f_0=1\\). Consider the chain where state \\(5\\) immediately jumps to state \\(3\\) (with probability 1) but \\(3\\not\\to5\\) (state 5 is not accessible from state 3), then \\(f_5=0\\). Let \\(N_i\\) be the total number of visits to state \\(i\\) (including the initial visit since the chain starts in state \\(i\\)). Then \\[N_i\\sim\\mathsf{Geom}(p=1-f_i)\\] with \\(E(N_i)=\\frac{1}{1-f_i}\\). If \\(f_i=1\\), then \\(P(N_i=\\infty)=1\\) and \\(E(N_i)=\\infty\\). This means that with probability one, state \\(i\\) will be visited infinitely-many times (when the chain starts in state \\(i\\)). In this case, \\(N_i\\) isnt exactly geometrically-distributed, but we can still think of it as a geometric random variable with zero probability of success. Now we can say a bit more about recurrence and transience. Theorem. State \\(i\\) is recurent if and only if \\[\\sum_{n=1}^\\infty (T^n)_{ii}=\\infty.\\] State \\(i\\) is transient if and only if \\[\\sum_{n=1}^\\infty (T^n)_{ii}&lt;\\infty.\\] 8.6 Limiting probabilities Let \\(\\tau_{ij}\\) be the number of steps to first hit state \\(j\\) when starting in state \\(i\\) and \\(\\tau_i\\) the return time to state \\(i\\) (the number of steps to next be in state \\(i\\) when starting in state \\(i\\)): \\[\\tau_{ij}=\\min\\{n\\mid X_n=j\\},\\] \\[\\tau_{i}=\\min\\{n\\mid X_n=i\\},\\] Let \\(m_{ij}=\\mathsf E(\\tau_{ij}\\mid X_0=i)\\) and \\(m_{i}=\\mathsf E(\\tau_{i}\\mid X_0=i)\\). Now we investigate the proportion of time that the chain spends in state \\(i\\). [..to be continued] Summary Summary of notation, formulas, and terminology Transition matrix: \\(T_{ij}=P(X_n=j \\mid X_{n-1}=i)\\) Markov property (memorylessness): \\[P(X_n=j \\mid X_{n-1}=i,X_{n-2}=\\ell_{n-2},\\ldots,X_{1}=\\ell_{1},X_{0}=\\ell_{0})=P(X_n=j \\mid X_{n-1}=i)=T_{ij}\\] Matrix multiplication: \\(\\mathbf p_n =\\mathbf p_{n-1} T\\) \\(N_i\\sim\\mathsf{Geom}(p=1-f_i)\\), \\(E(N_i)=\\frac{1}{1-f_i}\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
