[["random-walks.html", "Chapter 4 Random walks 4.1 The simple symmetric random walk (SSRW)", " Chapter 4 Random walks The random walk will be one of our first official stochastic process models. It models a particle on a line jumping one unit left or right with equal probability. Variations of this can be used for modeling many physical phenomena, including: a viral particle floating in the air (a 3D random walk), an animal moving in its habitat (2D for most land animals, but 1D can work for a restricted habitat), or a stock price (1D). 4.1 The simple symmetric random walk (SSRW) We let \\(X_n\\) be the state of the process at time step \\(n\\) and fix \\(X_0=0\\) (the particle starts at the origin). The particle moves left or right with equal probability, which is given as \\[P(X_{n+1}=j-1\\mid X_n=j)=\\frac12,\\] \\[P(X_{n+1}=j+1\\mid X_n=j)=\\frac12.\\] We consider the choice at each time step to go up or down as being independent of every other time step. This model is of a special class of stochastic processes called Markov chains,b ut weâ€™ll discuss those in more detail later. We can construct this model mathematically in more detail be letting the \\(n^{th}\\) step be random variable \\(Y_n\\) which takes values \\(\\pm1\\) with equal probability and all being independent. We say that the \\(Y_n\\) are i.i.d. (independent and identically distributed) with \\(P(Y_n=1)=P(Y_n=-1)=\\frac12\\) for all \\(n\\). Independence here means that if we want to calculate probabilities for multiple \\(Y_n\\) simultaneously, we can calculate them individually and multiply: \\[P(Y_j=a,Y_k=b)=P(Y_j=a)P(Y_k=b)\\] for any \\(i,j\\in\\mathbb N\\) (with \\(i\\neq j\\)) and any \\(a,b\\in\\{-1,1\\}\\). Then we can write \\[X_n=\\sum_{j=1}^n Y_j.\\] Example. Calculate \\(P(X_3=3)\\). This is only possible if \\(Y_1=Y_2=Y_3=1\\) and so we calculate \\[P(Y_1=1,Y_2=1,Y_3=1)=P(Y_1=1)P(Y_2=1)P(Y_3=1)=\\frac12\\cdot\\frac12\\cdot\\frac12=\\frac18.\\] Example. Calculate \\(P(X_2=0)\\). This is only possible if \\(Y_1=1,Y_2=-1\\) or \\(Y_1=-1,Y_2=1\\) and so we calculate each probability and add the result. \\[P(X_2=0)=P(Y_1=1,Y_2=-1)+P(Y_1=-1,Y_2=1)=\\frac14+\\frac14=\\frac12.\\] The state space of the SSRW is thus the set of integers \\(\\mathbb Z\\) and the time index set is \\(\\mathbb N_0\\). The sample path space will be all infinitely long sequences of integers where consecutive integers only differ by \\(\\pm1\\). We could say that the sample path space is all infinite sequences of integers, but most of those will have probability zero since any sequence which jumps outside of \\(\\pm1\\) would be considered as not possible. So we have our sequence of random variables \\((X_0,X_1,X_2,\\ldots)\\). Of course, \\(X_0\\) is deterministically set to one, but we can still consider it a random variable with full probability mass on one. Now \\(X_1\\) is equally likely to be \\(\\pm1\\). If \\(X_1=1\\), then \\(X_2\\) is equally likely to be \\(0,2\\), and if \\(X_1=-1\\), then \\(X_2\\) is equally likely to be \\(0,-2\\). 4.1.1 Distribution of \\(X_n\\) Since \\(X_n\\) is a random sum of a bunch of plus and minus ones, we can realte it to a sum of Bernoulli random variables. If we think of flipping a fair coint \\(n\\) times and let \\(H\\) be the number of heads and \\(T\\) be the number of tails, then we must have \\(H+T=n\\). If the \\(j^{th}\\) coint flip is heads, we set \\(Y_j=1\\) and if it is tails, we set \\(Y_j=-1\\). In this way, we can reason that \\[X_n=(\\# \\text{ heads})-(\\# \\text{ tails})=H-T=H-(n-H)=2H-n.\\] Since we know that \\(H\\sim\\mathsf{binom}(n,\\frac12)\\) (\\(H\\) is governed by the binomial distribution with \\(n\\) trials and \\(p=\\frac12\\) probability of success), we can use this to calculate probabilities for \\(X_n\\): \\[P(X_n=j)=P(2H-n=j)=P(H=\\frac{n+j}{2})={n\\choose \\frac{n+j}{2}}\\frac1{2^n}.\\] Now we refresh the normal approximation to the binomial. Normal approximation to binomial. Let \\(X\\sim\\mathsf{binom}(n,p)\\), then for \\(n\\) large (usually \\(n\\geq 30\\) with \\(np\\geq5\\) and \\(n(1-p)\\geq5\\) is acceptable, but it might still be a rough approximation). Then we can say that \\[X\\overset{\\small approx}{\\sim} \\mathsf{N}(\\mu=np,\\sigma^2=np(1-p)).\\] The binomial is a discrete distribution, but the normal is a continuous distribution. Any time we use a continuous distribution to approximate a discrete distribution, it might be useful to use a continuity correction. Continuity correction. If discrete random variable \\(X\\) has values \\(x_1,x_2,\\ldots\\) and we wish to approximate \\(P(X=x_j)\\) by continuous random variable \\(Y\\), then we can integrate the probability density function of \\(Y\\) from halfway to the next \\(x\\)-values on the left and right: \\[P(X=x_j)\\approx P(x_j-(x_j-x_{j-1})/2 &lt; Y \\leq x_j+(x_j-x_{j-1})/2).\\] For the normal approximation to the binomial random variable \\(X\\sim binom(n,p)\\), this translates to using \\(Y\\sim N(np,np(1-p))\\) and \\[P(X=k)\\approx P\\left(k-\\frac12 &lt; Y \\leq k+\\frac12\\right).\\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
