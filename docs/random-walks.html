<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Random walks | Math 423 Stochastic Processes Course Notes</title>
  <meta name="description" content="Chapter 6 Random walks | Math 423 Stochastic Processes Course Notes" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Random walks | Math 423 Stochastic Processes Course Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Random walks | Math 423 Stochastic Processes Course Notes" />
  
  
  

<meta name="author" content="Joseph Stover" />


<meta name="date" content="2023-03-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro-stochastic-processes.html"/>
<link rel="next" href="limit-theorems.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script type="text/javascript">
  function unhide(divID) {
    var item = document.getElementById(divID);
    if (item) {
      item.className=(item.className=='hiddendiv')?'unhiddendiv':'hiddendiv';
    }
  }
</script> 


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Stochastic Processes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> Introduction to R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#getting-access-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Getting access to R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#r-basics"><i class="fa fa-check"></i><b>1.2</b> R basics</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#data-structures-vectors-matrices"><i class="fa fa-check"></i><b>1.2.1</b> Data structures, vectors, matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#installing-and-using-packages-in-r"><i class="fa fa-check"></i><b>1.3</b> Installing and using packages in R</a></li>
<li class="chapter" data-level="1.4" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#random-variables"><i class="fa fa-check"></i><b>1.4</b> Random variables</a></li>
<li class="chapter" data-level="1.5" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#basics-of-programming-r-scripts"><i class="fa fa-check"></i><b>1.5</b> Basics of programming, R scripts</a></li>
<li class="chapter" data-level="1.6" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#importing-datasets-into-r"><i class="fa fa-check"></i><b>1.6</b> Importing datasets into R</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html"><i class="fa fa-check"></i><b>2</b> Counting, sets, and probability basics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#ticket-in-a-box-model-of-probability"><i class="fa fa-check"></i><b>2.1</b> Ticket-in-a-box model of probability</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#sampling-from-a-box-of-tickets"><i class="fa fa-check"></i><b>2.1.1</b> Sampling from a box of tickets</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#relative-area-model-of-probability"><i class="fa fa-check"></i><b>2.2</b> Relative area model of probability</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#one-dimension"><i class="fa fa-check"></i><b>2.2.1</b> One dimension</a></li>
<li class="chapter" data-level="2.2.2" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#two-or-more-dimensions"><i class="fa fa-check"></i><b>2.2.2</b> Two or more dimensions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>2.3</b> Sample Spaces and Events</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#sample-space"><i class="fa fa-check"></i><b>2.3.1</b> Sample space</a></li>
<li class="chapter" data-level="2.3.2" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#events"><i class="fa fa-check"></i><b>2.3.2</b> Events</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#set-operations"><i class="fa fa-check"></i><b>2.4</b> Set Operations</a></li>
<li class="chapter" data-level="2.5" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#venn-diagrams"><i class="fa fa-check"></i><b>2.5</b> Venn Diagrams</a></li>
<li class="chapter" data-level="2.6" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#counting-permutations-and-combinations"><i class="fa fa-check"></i><b>2.6</b> Counting, permutations, and combinations</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#multiplication-rule"><i class="fa fa-check"></i><b>2.6.1</b> Multiplication rule</a></li>
<li class="chapter" data-level="2.6.2" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#nk"><i class="fa fa-check"></i><b>2.6.2</b> <span class="math inline">\(n^k\)</span></a></li>
<li class="chapter" data-level="2.6.3" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#factorials"><i class="fa fa-check"></i><b>2.6.3</b> Factorials</a></li>
<li class="chapter" data-level="2.6.4" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#permutation"><i class="fa fa-check"></i><b>2.6.4</b> Permutation</a></li>
<li class="chapter" data-level="2.6.5" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#combination"><i class="fa fa-check"></i><b>2.6.5</b> Combination</a></li>
<li class="chapter" data-level="2.6.6" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#summary-of-counting"><i class="fa fa-check"></i><b>2.6.6</b> Summary of counting</a></li>
<li class="chapter" data-level="2.6.7" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#special-case-with-replacement-order-doesnt-matter-the-multiset"><i class="fa fa-check"></i><b>2.6.7</b> Special case: with replacement, order doesn’t matter: the multiset</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#probability"><i class="fa fa-check"></i><b>2.7</b> Probability</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#equally-likely-outcomes"><i class="fa fa-check"></i><b>2.7.1</b> Equally likely outcomes</a></li>
<li class="chapter" data-level="2.7.2" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#general-probability-theory"><i class="fa fa-check"></i><b>2.7.2</b> General probability theory</a></li>
<li class="chapter" data-level="2.7.3" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#independence"><i class="fa fa-check"></i><b>2.7.3</b> Independence</a></li>
<li class="chapter" data-level="2.7.4" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#conditional-probability"><i class="fa fa-check"></i><b>2.7.4</b> Conditional probability</a></li>
<li class="chapter" data-level="2.7.5" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#partitions"><i class="fa fa-check"></i><b>2.7.5</b> Partitions</a></li>
<li class="chapter" data-level="2.7.6" data-path="counting-sets-and-probability-basics.html"><a href="counting-sets-and-probability-basics.html#bayes-theorem"><i class="fa fa-check"></i><b>2.7.6</b> Baye’s Theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html"><i class="fa fa-check"></i><b>3</b> Random variables and distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#expectation-and-variance"><i class="fa fa-check"></i><b>3.1</b> Expectation and variance</a></li>
<li class="chapter" data-level="3.2" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#joint-distributions"><i class="fa fa-check"></i><b>3.2</b> Joint distributions</a></li>
<li class="chapter" data-level="3.3" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>3.3</b> Independence of random variables</a></li>
<li class="chapter" data-level="3.4" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#discrete-bernoulli-binomial-geometric-poisson"><i class="fa fa-check"></i><b>3.4</b> Discrete: Bernoulli, binomial, geometric, Poisson</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#bernoulli"><i class="fa fa-check"></i><b>3.4.1</b> Bernoulli</a></li>
<li class="chapter" data-level="3.4.2" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#binomial"><i class="fa fa-check"></i><b>3.4.2</b> Binomial</a></li>
<li class="chapter" data-level="3.4.3" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#geometric"><i class="fa fa-check"></i><b>3.4.3</b> Geometric</a></li>
<li class="chapter" data-level="3.4.4" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#poisson"><i class="fa fa-check"></i><b>3.4.4</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#continuous-uniform-exponential-normal"><i class="fa fa-check"></i><b>3.5</b> Continuous: Uniform, exponential, normal</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#uniform"><i class="fa fa-check"></i><b>3.5.1</b> Uniform</a></li>
<li class="chapter" data-level="3.5.2" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#exponential"><i class="fa fa-check"></i><b>3.5.2</b> Exponential</a></li>
<li class="chapter" data-level="3.5.3" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#normal"><i class="fa fa-check"></i><b>3.5.3</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="random-variables-and-distributions.html"><a href="random-variables-and-distributions.html#summary-1"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conditional-expectation.html"><a href="conditional-expectation.html"><i class="fa fa-check"></i><b>4</b> Conditional expectation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#conditional-distributions"><i class="fa fa-check"></i><b>4.1</b> Conditional distributions</a></li>
<li class="chapter" data-level="4.2" data-path="conditional-expectation.html"><a href="conditional-expectation.html#conditional-expectation-1"><i class="fa fa-check"></i><b>4.2</b> Conditional expectation</a></li>
<li class="chapter" data-level="4.3" data-path="conditional-expectation.html"><a href="conditional-expectation.html#random-conditional-expectation"><i class="fa fa-check"></i><b>4.3</b> (Random) conditional expectation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#total-expectation"><i class="fa fa-check"></i><b>4.3.1</b> Total expectation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conditional-expectation.html"><a href="conditional-expectation.html#summary-2"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="intro-stochastic-processes.html"><a href="intro-stochastic-processes.html"><i class="fa fa-check"></i><b>5</b> Intro Stochastic Processes</a>
<ul>
<li class="chapter" data-level="" data-path="intro-stochastic-processes.html"><a href="intro-stochastic-processes.html#summary-3"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="random-walks.html"><a href="random-walks.html"><i class="fa fa-check"></i><b>6</b> Random walks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="random-walks.html"><a href="random-walks.html#the-simple-symmetric-random-walk-ssrw"><i class="fa fa-check"></i><b>6.1</b> The simple symmetric random walk (SSRW)</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="random-walks.html"><a href="random-walks.html#definition-of-a-random-walk"><i class="fa fa-check"></i><b>6.1.1</b> Definition of a random walk</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="random-walks.html"><a href="random-walks.html#distribution-of-x_n"><i class="fa fa-check"></i><b>6.2</b> Distribution of <span class="math inline">\(X_n\)</span></a></li>
<li class="chapter" data-level="6.3" data-path="random-walks.html"><a href="random-walks.html#shift-invariance-memorylessness"><i class="fa fa-check"></i><b>6.3</b> Shift invariance &amp; memorylessness</a></li>
<li class="chapter" data-level="6.4" data-path="random-walks.html"><a href="random-walks.html#reflection-principle"><i class="fa fa-check"></i><b>6.4</b> Reflection principle</a></li>
<li class="chapter" data-level="6.5" data-path="random-walks.html"><a href="random-walks.html#maximum-state-reached"><i class="fa fa-check"></i><b>6.5</b> Maximum state reached</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="random-walks.html"><a href="random-walks.html#maximum-over-infinite-sample-paths-for-p12"><i class="fa fa-check"></i><b>6.5.1</b> Maximum over infinite sample paths for <span class="math inline">\(p&lt;1/2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="random-walks.html"><a href="random-walks.html#hitting-times"><i class="fa fa-check"></i><b>6.6</b> Hitting times</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="random-walks.html"><a href="random-walks.html#hitting-time-for-state-1"><i class="fa fa-check"></i><b>6.6.1</b> Hitting time for state <span class="math inline">\(1\)</span></a></li>
<li class="chapter" data-level="6.6.2" data-path="random-walks.html"><a href="random-walks.html#hitting-time-for-other-states"><i class="fa fa-check"></i><b>6.6.2</b> Hitting time for other states</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="random-walks.html"><a href="random-walks.html#return-time-to-state-0"><i class="fa fa-check"></i><b>6.7</b> Return time to state <span class="math inline">\(0\)</span></a></li>
<li class="chapter" data-level="" data-path="random-walks.html"><a href="random-walks.html#summary-4"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="limit-theorems.html"><a href="limit-theorems.html"><i class="fa fa-check"></i><b>7</b> Limit theorems</a>
<ul>
<li class="chapter" data-level="7.1" data-path="limit-theorems.html"><a href="limit-theorems.html#inequalities"><i class="fa fa-check"></i><b>7.1</b> Inequalities</a></li>
<li class="chapter" data-level="7.2" data-path="limit-theorems.html"><a href="limit-theorems.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2</b> Law of large numbers</a></li>
<li class="chapter" data-level="7.3" data-path="limit-theorems.html"><a href="limit-theorems.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.3</b> Central limit theorem</a></li>
<li class="chapter" data-level="7.4" data-path="limit-theorems.html"><a href="limit-theorems.html#borel-cantelli"><i class="fa fa-check"></i><b>7.4</b> Borel-Cantelli</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="markov-chains.html"><a href="markov-chains.html"><i class="fa fa-check"></i><b>8</b> Markov Chains</a>
<ul>
<li class="chapter" data-level="8.1" data-path="markov-chains.html"><a href="markov-chains.html#graph-of-a-markov-chain"><i class="fa fa-check"></i><b>8.1</b> Graph of a Markov chain</a></li>
<li class="chapter" data-level="8.2" data-path="markov-chains.html"><a href="markov-chains.html#classification-of-states"><i class="fa fa-check"></i><b>8.2</b> Classification of states</a></li>
<li class="chapter" data-level="8.3" data-path="markov-chains.html"><a href="markov-chains.html#distribution-at-time-n"><i class="fa fa-check"></i><b>8.3</b> Distribution at time <span class="math inline">\(n\)</span></a></li>
<li class="chapter" data-level="8.4" data-path="markov-chains.html"><a href="markov-chains.html#simulating-a-markov-chain-in-r"><i class="fa fa-check"></i><b>8.4</b> Simulating a Markov chain in R</a></li>
<li class="chapter" data-level="8.5" data-path="markov-chains.html"><a href="markov-chains.html#return-times-and-hitting-probabilities"><i class="fa fa-check"></i><b>8.5</b> Return times and hitting probabilities</a></li>
<li class="chapter" data-level="8.6" data-path="markov-chains.html"><a href="markov-chains.html#limiting-probabilities"><i class="fa fa-check"></i><b>8.6</b> Limiting probabilities</a></li>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#summary-5"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Math 423 Stochastic Processes Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-walks" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Random walks<a href="random-walks.html#random-walks" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The random walk will be one of our first official stochastic process models. It models a particle on a line jumping one unit left or right with equal probability. Variations of this can be used for modeling many physical phenomena, including:</p>
<ol style="list-style-type: decimal">
<li><p>a viral particle floating in the air (a 3D random walk),</p></li>
<li><p>an animal moving in its habitat (2D for most land animals, but 1D can work for a restricted habitat), or</p></li>
<li><p>a stock price (1D).</p></li>
</ol>
<div id="the-simple-symmetric-random-walk-ssrw" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> The simple symmetric random walk (SSRW)<a href="random-walks.html#the-simple-symmetric-random-walk-ssrw" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We let <span class="math inline">\(X_n\)</span> be the state of the process at time step <span class="math inline">\(n\)</span> and fix <span class="math inline">\(X_0=0\)</span> (the particle starts at the origin). The particle moves left or right with equal probability, which is given as
<span class="math display">\[P(X_{n+1}=j-1\mid X_n=j)=\frac12,\]</span>
<span class="math display">\[P(X_{n+1}=j+1\mid X_n=j)=\frac12.\]</span></p>
<p>More generally, we can let <span class="math inline">\(P(X_{n+1}=j+1\mid X_n=j)=p\)</span> and <span class="math inline">\(P(X_{n+1}=j-1\mid X_n=j)=1-p\)</span>, and if <span class="math inline">\(p\neq\frac12\)</span>, then we call it a <strong>simple asymmetric random walk (SARW)</strong>. We call this stochastic process “simple” because successive steps are independent and identically distributed and “symmetric” or “asymmetric” depending on whether the up and down probabilities are equivalent or not.</p>
<p>We will refer to the state of the process as the “level” or “location” at times, and rather than left or right, we will usually say “up” and “down” since we normally graph time horizontally and location on <span class="math inline">\(\mathbb Z\)</span> vertically.</p>
<p>We consider the choice at each time step to go up or down as being independent of every other time step. This model is of a special class of stochastic processes called Markov chains, but we’ll discuss those in more detail later.</p>
<p>We can construct this model mathematically in more detail be letting the <span class="math inline">\(n^{th}\)</span> step be random variable <span class="math inline">\(Y_n\)</span> which takes values <span class="math inline">\(\pm1\)</span> with equal probability and all being independent. We say that the <span class="math inline">\(Y_n\)</span> are <strong>i.i.d. (independent and identically distributed)</strong> with <span class="math inline">\(P(Y_n=1)=P(Y_n=-1)=\frac12\)</span> for all <span class="math inline">\(n\)</span>. Independence here means that if we want to calculate probabilities for multiple <span class="math inline">\(Y_n\)</span> simultaneously, we can calculate them individually and multiply:
<span class="math display">\[P(Y_j=a,Y_k=b)=P(Y_j=a)P(Y_k=b)\]</span>
for any <span class="math inline">\(i,j\in\mathbb N\)</span> (with <span class="math inline">\(i\neq j\)</span>) and any <span class="math inline">\(a,b\in\{-1,1\}\)</span>. Then we can write
<span class="math display">\[X_n=\sum_{j=1}^n Y_j.\]</span></p>
<div class="exbox">
<p><strong>Example.</strong> Calculate <span class="math inline">\(P(X_3=3)\)</span>. This is only possible if <span class="math inline">\(Y_1=Y_2=Y_3=1\)</span> and so we calculate
<span class="math display">\[P(Y_1=1,Y_2=1,Y_3=1)=P(Y_1=1)P(Y_2=1)P(Y_3=1)=\frac12\cdot\frac12\cdot\frac12=\frac18.\]</span></p>
</div>
<div class="exbox">
<p><strong>Example.</strong> Calculate <span class="math inline">\(P(X_2=0)\)</span>. This is only possible if <span class="math inline">\(Y_1=1,Y_2=-1\)</span> or <span class="math inline">\(Y_1=-1,Y_2=1\)</span> and so we calculate each probability and add the result.
<span class="math display">\[P(X_2=0)=P(Y_1=1,Y_2=-1)+P(Y_1=-1,Y_2=1)=\frac14+\frac14=\frac12.\]</span></p>
</div>
<p>The state space of the SSRW is thus the set of integers <span class="math inline">\(\mathbb Z\)</span> and the time index set is <span class="math inline">\(\mathbb N_0\)</span>. The sample path space will be all infinitely long sequences of integers where consecutive integers only differ by <span class="math inline">\(\pm1\)</span>. We could say that the sample path space is all infinite sequences of integers, but most of those will have probability zero since any sequence which jumps outside of <span class="math inline">\(\pm1\)</span> would be considered as not possible.</p>
<p>So we have our sequence of random variables <span class="math inline">\((X_0,X_1,X_2,\ldots)\)</span>. Of course, <span class="math inline">\(X_0\)</span> is deterministically set to one, but we can still consider it a random variable with full probability mass on one. Now <span class="math inline">\(X_1\)</span> is equally likely to be <span class="math inline">\(\pm1\)</span>. If <span class="math inline">\(X_1=1\)</span>, then <span class="math inline">\(X_2\)</span> is equally likely to be <span class="math inline">\(0,2\)</span>, and if <span class="math inline">\(X_1=-1\)</span>, then <span class="math inline">\(X_2\)</span> is equally likely to be <span class="math inline">\(0,-2\)</span>.</p>
<!-- We illustrate part of the sample path space below. -->
<div id="definition-of-a-random-walk" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Definition of a random walk<a href="random-walks.html#definition-of-a-random-walk" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(X=(X_n)_{n\in\mathbb N_0}\)</span> be a stochastic process. Then <span class="math inline">\(X\)</span> is a simple random walk if</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(X_0=0\)</span>, and</p></li>
<li><p><span class="math inline">\(Y_j=X_{j}-X_{j-1}\)</span> are i.i.d. for <span class="math inline">\(j=1,2,\ldots\)</span> with probabilities <span class="math inline">\(P(Y_j=1)=p\)</span> and <span class="math inline">\(P(Y_j=-1)=1-p\)</span>.</p></li>
</ol>
<p>What this tells us is that we can construct other random walks from a previous one. Let <span class="math inline">\(X_n\)</span> be the SSRW. Then <span class="math inline">\(-X_n\)</span> is also a SSRW. Let’s check that it satisfies the criteria: <span class="math inline">\(-X_0=0\)</span> (since <span class="math inline">\(X_0=0\)</span> because <span class="math inline">\(X_n\)</span> is a SSRW), and <span class="math inline">\((-X_{j})-(-X_{j-1})=-Y_j\)</span> and <span class="math inline">\(P(-Y_j=1)=P(Y_j=-1)=\frac12\)</span>, <span class="math inline">\(P(-Y_j=-1)=P(Y_j=1)=\frac12\)</span> (where we use <span class="math inline">\(Y_j\)</span> to represent the individual steps for the original SSRW <span class="math inline">\(X_n\)</span>). Hence <span class="math inline">\(-X_n\)</span> is an SSRW also!</p>
<p>One important thing to realize here is that when we write <span class="math inline">\(X_n\)</span> and <span class="math inline">\(-X_n\)</span> we do mean precisely that they are literally reflections of one another. In other words, if we know a particular value, say <span class="math inline">\(X_5=3\)</span>, then we know that the reflected process is exactly at level <span class="math inline">\(-3\)</span> for time step <span class="math inline">\(5\)</span>. This is true because <span class="math inline">\(X_5\)</span> is a random variable, and so anywhere we see <span class="math inline">\(X_5\)</span>, it is the same random variable whose value is determined by the same radom experiment (e.g. simulating a random walk path).</p>
<div class="exbox">
<p><strong>Example.</strong></p>
<p>Let <span class="math inline">\(X_n\)</span> be the SSRW. Let <span class="math inline">\(W_n\)</span> be a stochastic process, and show that the following are also SSRW’s.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(W_n=-X_n\)</span></p></li>
<li><p><span class="math inline">\(W_n=X_{3+n}-X_3\)</span></p></li>
<li><p><span class="math inline">\(W_n=\sum_{j=1}^n (X_{2j}-X_{2j-1})\)</span> with <span class="math inline">\(W_0=0\)</span></p></li>
</ol>
<p><a href="javascript:unhide('ex1');">Show/hide solution.</a></p>
<div id="ex1" class="hiddendiv">
<ol style="list-style-type: decimal">
<li><p>We covered this example in the text above.</p></li>
<li><p>This is a shifted random walk. The <span class="math inline">\(W_n\)</span> sample path is identical to the <span class="math inline">\(X_n\)</span> sample path but starting from <span class="math inline">\(X_3\)</span> and being shifted back to starting from the origin. In other words, we can imagine a completely simulated sample path for <span class="math inline">\(X_n\)</span>, and we just ignore the first three time steps and take the remaining part of the sample path and shift it back to starting at the origin, preserving its exact shape.<br><br>
We calculate <span class="math inline">\(W_0=X_{3+0}-X_3=0\)</span> and <span class="math inline">\(W_j-W_{j-1}=(X_{3+j}-X_3)-(X_{3+j-1}-X_3)=X_{3+j}-X_{3+j-1}=Y_{3+j}\)</span> and know that <span class="math inline">\(Y_4,Y_5,\ldots\)</span> are i.i.d. with the desired probabilities.</p></li>
<li><p>The <span class="math inline">\(n^{th}\)</span> step for <span class="math inline">\(W_n\)</span> is the <span class="math inline">\(2n^{th}\)</span> step for <span class="math inline">\(X_n\)</span>. In other words, <span class="math inline">\(W_n\)</span> only considers the even steps. If <span class="math inline">\(X_n=\sum_{j=1}^n Y_j\)</span>, then <span class="math inline">\(W_n=\sum_{j=1}^n Y_{2j}\)</span>.<br><br>
We see that <span class="math inline">\(W_0=0\)</span> is given and calculate
<span class="math inline">\(W_k-W_{k-1}=\sum_{j=1}^k (X_{2j}-X_{2j-1})-\sum_{j=1}^{k-1} (X_{2j}-X_{2j-1})=X_{2k}-X_{2k-1}=Y_{2k}\)</span>. We know that <span class="math inline">\(Y_2,Y_4,\ldots\)</span> are i.i.d. with equal probability on <span class="math inline">\(\pm1\)</span>.</p></li>
</ol>
</div>
</div>
</div>
</div>
<div id="distribution-of-x_n" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Distribution of <span class="math inline">\(X_n\)</span><a href="random-walks.html#distribution-of-x_n" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Since <span class="math inline">\(X_n\)</span> is a random sum of a bunch of plus and minus ones, we can relate it to a sum of Bernoulli random variables. If we think of flipping a fair coin <span class="math inline">\(n\)</span> times and let <span class="math inline">\(H\)</span> be the number of heads and <span class="math inline">\(T\)</span> be the number of tails, then we must have <span class="math inline">\(H+T=n\)</span>. If the <span class="math inline">\(j^{th}\)</span> coint flip is heads, we set <span class="math inline">\(Y_j=1\)</span> and if it is tails, we set <span class="math inline">\(Y_j=-1\)</span>. In this way, we can reason that <span class="math display">\[X_n=(\# \text{ heads})-(\# \text{ tails})=H-T=H-(n-H)=2H-n.\]</span>
Since we know that <span class="math inline">\(H\sim\mathsf{binom}(n,\frac12)\)</span> (<span class="math inline">\(H\)</span> is governed by the binomial distribution with <span class="math inline">\(n\)</span> trials and <span class="math inline">\(p=\frac12\)</span> probability of success), we can use this to calculate probabilities for <span class="math inline">\(X_n\)</span>:</p>
<div class="thmbox">
<p><strong>Distribution of <span class="math inline">\(X_n\)</span> for SSRW.</strong>
<span class="math display">\[P(X_n=j)=P(2H-n=j)=P\left(H=\frac{n+j}{2}\right)={n\choose \frac{n+j}{2}}\frac1{2^n}.\]</span>
More generally, for the asymmetric random walk, with <span class="math inline">\(p\neq\frac12\)</span>, we have
<span class="math display">\[P(X_n=j)={n\choose \frac{n+j}{2}}p^{\frac{n+j}{2}}(1-p)^{\frac{n-j}{2}}.\]</span></p>
</div>
<p>Now we refresh the normal approximation to the binomial.</p>
<div class="defbox">
<p><strong>Normal approximation to binomial.</strong>
Let <span class="math inline">\(X\sim\mathsf{binom}(n,p)\)</span>, then for <span class="math inline">\(n\)</span> large (usually <span class="math inline">\(n\geq 30\)</span> with <span class="math inline">\(np\geq5\)</span> and <span class="math inline">\(n(1-p)\geq5\)</span> is acceptable, but it might still be a rough approximation). Then we can say that
<span class="math display">\[X\overset{\small approx}{\sim} \mathsf{N}(\mu=np,\sigma^2=np(1-p)).\]</span></p>
</div>
<p>The binomial is a discrete distribution, but the normal is a continuous distribution. Any time we use a continuous distribution to approximate a discrete distribution, it might be useful to use a <strong>continuity correction</strong>.</p>
<!-- ::: defbox -->
<p><strong>Continuity correction.</strong>
If discrete random variable <span class="math inline">\(X\)</span> has values <span class="math inline">\(x_1,x_2,\ldots\)</span> and we wish to approximate <span class="math inline">\(P(X=x_j)\)</span> by continuous random variable <span class="math inline">\(Y\)</span>, then we can integrate the probability density function of <span class="math inline">\(Y\)</span> from halfway to the next <span class="math inline">\(x\)</span>-values on the left and right:
<span class="math display">\[P(X=x_j)\approx P(x_j-(x_j-x_{j-1})/2 &lt; Y \leq x_j+(x_j-x_{j-1})/2).\]</span>
For the normal approximation to the binomial random variable <span class="math inline">\(X\sim binom(n,p)\)</span>, this translates to using <span class="math inline">\(Y\sim N(np,np(1-p))\)</span> and
<span class="math display">\[P(X=k)\approx P\left(k-\frac12 &lt; Y \leq k+\frac12\right).\]</span>
<!-- ::: --></p>
<!-- ::: thmbox -->
<p><strong>Normal approximation to distribution of <span class="math inline">\(X_n\)</span> for SSRW.</strong>
We have that
<span class="math display">\[P(X_n=j)=P(2H-n=j)=P\left(H=\frac{n+j}{2}\right)\]</span>
and that <span class="math inline">\(H\)</span> is binomial distributed with parameters <span class="math inline">\(n,p\)</span> and hence is approximately normally distributed with mean <span class="math inline">\(\mu=np\)</span> and variance <span class="math inline">\(\sigma^2=np(1-p)\)</span>. Now we can then write
<span class="math display">\[P(X_n=j)=P(2H-n=j)=P\left(H=\frac{n+j}{2}\right)\approx P\left(\frac{n+j}{2}-\frac12&lt;Y\leq \frac{n+j}{2}+\frac12\right)\]</span>
where <span class="math inline">\(Y\sim\mathsf N(\mu=np,\sigma^2=np(1-p))\)</span>.</p>
<p>In R we can calculate normal cumulative probabilities using <code>pnorm()</code>, so the above probability for <span class="math inline">\(P(X_n=j)\)</span> is given by the R code (with the continuity correction)
<!-- $$P(X_n=j)=\texttt{pnorm((n+j)/2+1/2,n*p,sqrt(n*p*(1-p)))-pnorm((n+j)/2-1/2,n*p,sqrt(n*p*(1-p)))}}$$ -->
<!-- $$P(X_n=j)=$$ --></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="random-walks.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>((n<span class="sc">+</span>j)<span class="sc">/</span><span class="dv">2</span><span class="sc">+</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>n<span class="sc">*</span>p,<span class="at">sd=</span><span class="fu">sqrt</span>(n<span class="sc">*</span>p<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p)))<span class="sc">-</span><span class="fu">pnorm</span>((n<span class="sc">+</span>j)<span class="sc">/</span><span class="dv">2-1</span><span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>n<span class="sc">*</span>p,<span class="at">sd=</span><span class="fu">sqrt</span>(n<span class="sc">*</span>p<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p)))</span></code></pre></div>
<!-- ::: -->
<div class="exbox">
<p><strong>Example.</strong> Let’s calculate <span class="math inline">\(P(X_7=-3)\)</span>. We have <span class="math inline">\(n=7\)</span>, <span class="math inline">\(p=0.5\)</span>, and <span class="math inline">\(j=-3\)</span>. The exact calculation using the distribution we derived from the bionomial is:
<span class="math display">\[P(X_7=-3)={7\choose\frac{7-3}{2}}\frac1{2^7}={7\choose2}\frac1{2^7}=\frac{7\cdot 3}{2^7}\approx0.1640625.\]</span>
And using the normal approximation (with continuity correction) we get:
<span class="math inline">\(P(X_7=-3)\approx\)</span><code>pnorm(2.5,7/2,sqrt(7)/2)-pnorm(1.5,7/2,sqrt(7)/2)</code><span class="math inline">\(\approx0.1595609\)</span> which is a reasonable approximation.</p>
</div>
<p>We can make the R code a bit simpler though.</p>
<!-- ::: thmbox -->
<p>If <span class="math inline">\(X\sim N(\mu\sigma^2)\)</span> then <span class="math inline">\(aX+b\sim N(a\mu+b,a^2\sigma^2)\)</span>. This means that <span class="math inline">\(aX+b\)</span> is a normal random variable as well with <span class="math inline">\(E(aX+b)=a\mu+b\)</span> and <span class="math inline">\(Var(aX+b=a^2\sigma^2)\)</span>. Since we are approximating <span class="math inline">\(H\)</span>, the number of up steps, as a normal random variable, then <span class="math inline">\(2H-n\)</span> is also approximately normally distributed.
<span class="math display">\[X_n=2H-n\overset{\small approx}{\sim}\mathsf{N}(\mu=2np-n,\sigma^2=4np(1-p)).\]</span>
And for the SSRW this means
<span class="math display">\[X_n=2H-n\overset{\small approx}{\sim}\mathsf{N}(\mu=0,\sigma^2=n).\]</span>
Now to apply the continuity correction is a bit trickier though, because <span class="math inline">\(X_n\)</span> only takes on values <span class="math inline">\(-n,-n+2,\ldots,n-2,n\)</span>. Instead of adding and subtracting <span class="math inline">\(\frac12\)</span>, we must add and subtract <span class="math inline">\(1\)</span> and <span class="math inline">\(P(X_n=j)\)</span> is approximated by <code>pnorm(j+1,0,sqrt(n))-pnorm(j-1,0,sqrt(n))</code>.</p>
<!-- ::: -->
<div class="thmbox">
<p><span class="math display">\[X_n\overset{approx}\sim \mathsf{N}(\mu=2np-n,\sigma^2=4np(1-p))\]</span></p>
</div>
<p><strong>Partitioning.</strong> Note that when we calculate something like <span class="math inline">\(P(X_2=0)\)</span>, we are implicitly using the ideas of joint distributions, partitioning, and conditioning in teh following way
<span class="math display">\[\begin{aligned}
P(X_2=0)&amp;=P(X_1=1,X_2=0)+P(X_1=-1,X_2=0)\\
&amp;=P(X_1=1)P(X_2=0\mid X_1=1)+P(X_1=-1)P(X_2=0\mid X_1=-1)\\
&amp;=\frac12\cdot\frac12+\frac12\cdot\frac12.
\end{aligned}\]</span>
Although, one can work out such probabilities by sketching a diagram and counting paths, etc.</p>
<p>Here is another example of the use of conditioning:
<span class="math display">\[
\begin{aligned}
P(X_1=-1,X_2=0,X_3=1)&amp;=P(X_1=-1)P(X_2=0,X_3=1\mid X_1=-1)\\
&amp;=P(X_1=-1)P(X_2=0\mid X_1=-1)P(X_3=1\mid X_1=-1,X_2=0).
\end{aligned}\]</span>
In general, we can calculate probabilities about the future state of the process given information about its current and past states, so a conditional probability like <span class="math inline">\(P(X_3=1\mid X_1=-1,X_2=0)\)</span> will make sense given that we know the rules for the stochastic process <span class="math inline">\(X_n\)</span>. In most cases, we will only need to know the current state (this is <em>memorylessness</em> or the <em>Markov property</em>—more on that later). The simple random walk satisfies this, and we have
<span class="math display">\[P(X_3=1\mid X_1=-1,X_2=0)=(X_3=1\mid X_2=0).\]</span></p>
</div>
<div id="shift-invariance-memorylessness" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Shift invariance &amp; memorylessness<a href="random-walks.html#shift-invariance-memorylessness" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The simple random walk has properties which we’ll call <strong>shift invariance</strong> and *memorylessness**. Here <span class="math inline">\(X_n\)</span> is a simple random walk which could be symmetric or asymmetric.</p>
<p><em>Shift invariance</em> works by using the fact that successive steps are independent and identically distributed. As an example, if we know that <span class="math inline">\(X_5=3\)</span>, then from that point onward, the process behaves exactly like a random walk starting at initial state <span class="math inline">\(3\)</span> and we can re-sync our clock so that time <span class="math inline">\(n=5\)</span> is our new time zero. This gives that, for example, <span class="math inline">\(P(X_6=4\mid X_5=3)=P(X_1=4\mid X_0=3)\)</span>. Then we can shift the state lattice as well by subtracting 3 form both sides of the conditional bar: <span class="math inline">\(P(X_1=4\mid X_0=3)=P(X_1=4-3\mid X_0=3-3)=P(X_1=1\mid X_0=0)=P(X_1=1)\)</span> where in the last step, we use the fact that starting form initial state zero is the default so we don’t need to specify it as a condition.</p>
<div class="thmbox">
<p><strong>Shift invariance.</strong>
<span class="math display">\[P(X_{n+j}=k\mid X_j=\ell)=P(X_n=k-\ell)\]</span></p>
</div>
<p>The <em>memoryless</em> tells us that the distribution for future states is fully determined by the most recently known state. If we know the entire history of the process up to time <span class="math inline">\(n-1\)</span>, then to calculate probabilities for time step <span class="math inline">\(n\)</span>, we can discard the entire history except for the most recent state. Furthermore, even given incomplete information about the past states, we only care about the most recent one for calculating future probabilities.</p>
<div class="thmbox">
<p><strong>Memorylessness.</strong>
If we know the state of the process at time steps <span class="math inline">\(s_1&lt;s_2&lt;s_3&lt;\cdots&lt;s_m&lt;n\)</span>, then to calculate probabilities for <span class="math inline">\(X_n\)</span>, we only use the most recent state:
<span class="math display">\[P(X_{n}=k\mid X_{s_j}=x_{s_j} \text{ for } j=1,2,\ldots,m)=P(X_{n}=k\mid X_{s_m}=x_{s_m}).\]</span>
In particular, if we know the full history of the process up to time step <span class="math inline">\(n-1\)</span>, then to calculate probabilities for time step <span class="math inline">\(n\)</span>, we discard the entire history except for the most recent known state:
<span class="math display">\[P(X_{n}=k\mid X_{n-1}=\ell_{n-1},X_{n-2}=\ell_{n-2},\ldots,X_{1}=\ell_{1})=P(X_{n}=k\mid X_{n-1}=\ell_{n-1}).\]</span></p>
</div>
</div>
<div id="reflection-principle" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Reflection principle<a href="random-walks.html#reflection-principle" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now we’ll look at a property of the sample path space which will help us understand a few more properties for the random walk. Consider some state <span class="math inline">\(m\)</span> which we are interested in the RW hitting at some time step <span class="math inline">\(n=0,1,2,\ldots,k\)</span>. Clearly we must consider <span class="math inline">\(m\)</span> between <span class="math inline">\(-k\)</span> and <span class="math inline">\(k\)</span> since the RW can only go that far in <span class="math inline">\(k\)</span> time steps. We consider a sample path and the first time it hits level <span class="math inline">\(m\)</span>. We create a second sample path by tracing the first one up to this point and then reflecting everything after that initial hit of level <span class="math inline">\(m\)</span>.</p>
<p>In the following plot, we let <span class="math inline">\(m=2\)</span>. The red path is identical to the black dash-dot path up until the first hit of level <span class="math inline">\(2\)</span>. After that the red path is a reflection of the black dash-dot path.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>There is a special pattern here, that any path that ends up, in the end, above level <span class="math inline">\(m\)</span> will be reflected to create a path that ends up below level <span class="math inline">\(m\)</span>. Note that we are only considering paths that ultimately hit level <span class="math inline">\(m\)</span>, otherwise the reflection acros level <span class="math inline">\(m\)</span> doesn’t work. We summarize this below.</p>
<div class="thmbox">
<p>The number of paths that hit level <span class="math inline">\(m\)</span> and end up strictly above level <span class="math inline">\(m\)</span> is the same as the number of paths that hit level <span class="math inline">\(m\)</span> and end up strictly below level <span class="math inline">\(m\)</span>.</p>
<p><span class="math display">\[\#\{X_j=m \text{ for some } j\leq n, X_n&gt;m\}=\#\{X_j=m \text{ for some } j\leq n, X_n&lt;m\}\]</span></p>
</div>
</div>
<div id="maximum-state-reached" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Maximum state reached<a href="random-walks.html#maximum-state-reached" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(M_n=\max\{X_0,X_1,\ldots,X_n\}\)</span> be the maximum level hit by the process up to time step <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[P(M_n=k)=P(X_n=k)+P(X_{n}=k+1)\]</span></p>
<p>We derive this using the reflection principle and the fact that <span class="math inline">\(P(M_n=k)=P(M_n\geq k)-P(M_n\geq k+1)\)</span>.</p>
<p><span class="math display">\[P(M_n\geq k)=P(M_n\geq k,X_n&gt;k)+P(M_n\geq k,X_n=k)+P(M_n\geq k,X_n&lt;k)\]</span>
We can calculate the middle term using know methods:
<span class="math display">\[P(M_n\geq k,X_n=k)=P(X_n=k).\]</span></p>
<p>Using the reflection principle, we know that the events <span class="math inline">\(\{M_n\geq k,X_n&gt;k\}\)</span> and <span class="math inline">\(\{M_n\geq k,X_n&gt;k\}\)</span> have the same number of paths. For the SSRW, each of these paths is equally likely, so we get that <span class="math inline">\(P(M_n\geq k,X_n&gt;k)=P(M_n\geq k,X_n&lt;k)\)</span> and hence only need to calculate <span class="math inline">\(P(M_n\geq k,X_n&gt;k)\)</span>. But <span class="math inline">\(P(M_n\geq k,X_n&gt;k)=P(X_n&gt;k)\)</span> which we can calculate from known formulas:
<span class="math display">\[P(M_n\geq k,X_n&gt;k)=P(X_n&gt;k)=\sum_{j=k+1}^n P(X_n=j).\]</span></p>
<p>Putting all this together we get (for the SSRW with <span class="math inline">\(p=0.5\)</span>):
<span class="math display">\[\begin{aligned}
P(M_n\geq k)&amp;=P(M_n\geq k,X_n&gt;k)+P(M_n\geq k,X_n=k)+P(M_n\geq k,X_n&lt;k)\\
&amp;=2P(M_n\geq k,X_n&gt;k)+P(M_n\geq k,X_n=k)\\
&amp;=2P(X_n&gt;k)+P(X_n=k).\\
\end{aligned}\]</span>
And finally we see that
<span class="math display">\[\begin{aligned}
P(M_n= k)&amp;=P(M_n\geq k-P(M_n\geq k+1)\\
&amp;=(2P(M_n\geq k,X_n&gt;k)+P(M_n\geq k,X_n=k)\\
&amp;=(2P(X_n&gt;k)+P(X_n=k))-(2P(X_n&gt;k+1)+P(X_n=k+1))\\
&amp;=2P(X_n&gt;k+1)+2P(X_n=k+1)+P(X_n=k)-2P(X_n&gt;k+1)-P(X_n=k+1)\\
&amp;=P(X_n=k+1)+P(X_n=k).\\
\end{aligned}\]</span></p>
<div id="maximum-over-infinite-sample-paths-for-p12" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Maximum over infinite sample paths for <span class="math inline">\(p&lt;1/2\)</span><a href="random-walks.html#maximum-over-infinite-sample-paths-for-p12" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(p&lt;\frac12\)</span> then the random walk will eventually hit some nonnegative maximum level and then go towards <span class="math inline">\(-\infty\)</span> without ever exceeding its historical maximum level. Let <span class="math inline">\(M_\infty=\max\{X_0,X_1,\ldots\}\)</span> be the maximum state reached over the entire sample path with infinitely-many time steps. Then we have
<span class="math display">\[P(M_\infty=k)=\frac{1-2p}{1-p}\left(\frac{p}{1-p}\right)^k \ \text{ for } \ k=0,1,\ldots.\]</span>
Hence <span class="math inline">\(M_\infty\)</span> is a geometric random variable where we can think of a success as wander off to <span class="math inline">\(-\infty\)</span> and never exceeding its previous maximum. The probability the walk (starting at <span class="math inline">\(X_0=0\)</span>) never goes above zero is <span class="math inline">\(\frac{1-2p}{1-p}\)</span>. The probability it hits level 1 eventually is <span class="math inline">\(\frac p{1-p}\)</span>. From that point, it is probability <span class="math inline">\(\frac{1-2p}{1-p}\)</span> that it never exceeds level 1. Working inductively, we can reason that <span class="math inline">\(\left(\frac p{1-p}\right)^k\)</span> is the probability it eventually hits level <span class="math inline">\(k\)</span> and then multiplying by <span class="math inline">\(\frac{1-2p}{1-p}\)</span> that the walk never again exceeds its current level.</p>
</div>
</div>
<div id="hitting-times" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Hitting times<a href="random-walks.html#hitting-times" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We may be interested in how long it takes the random walk to hit a certain state. We refer to this as a <strong>hitting time</strong>. For example, if we are modeling a stock price with a random walk, and buy the stock for $250 and we wish to gain a profit of $5, then we might be interested in the how long we expect to wait for the stock to hit $255.</p>
<div id="hitting-time-for-state-1" class="section level3 hasAnchor" number="6.6.1">
<h3><span class="header-section-number">6.6.1</span> Hitting time for state <span class="math inline">\(1\)</span><a href="random-walks.html#hitting-time-for-state-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We wish to know how long it takes for the SARW to hit level <span class="math inline">\(1\)</span>. Let <span class="math inline">\(T_1=\min\{n&gt;0 : X_n=1\}\)</span> which we call the <em>hitting time</em> for state <span class="math inline">\(1\)</span>.</p>
<div class="thmbox">
<p><strong>Theorem (Hitting time for state <span class="math inline">\(1\)</span>).</strong></p>
<p><span class="math display">\[P(T_1=2k+1)=\frac1{k+1}{2k\choose k}p^{k+1}(1-p)^k, \ k=0,1,2,\ldots.\]</span>
<a href="javascript:unhide('t1pf');">Show/hide proof.</a></p>
<div id="t1pf" class="hiddendiv">
<p><em>Proof.</em>
We’ll first see that the number of paths that end at <span class="math inline">\(X_{2k}=0\)</span> and hit level 1 somewhere before that is the same as the number of paths that start at <span class="math inline">\(X_0=0\)</span> and end at <span class="math inline">\(X_{2k}=2\)</span>… See the graph below.</p>
<p>Draw any path for yourself that goes form <span class="math inline">\(X_0=0\)</span> to <span class="math inline">\(X_{2k}=2\)</span> for any <span class="math inline">\(k\)</span>-value. Then figure out the first time it hits level 1 and reflect the rest of the path after that around level 1. Then you get a path that goes form <span class="math inline">\(X_0=0\)</span> to <span class="math inline">\(X_{2k}=0\)</span> and it hits level 1 somewhere. This is the number of paths in the event <span class="math inline">\(\{M_{2k}\geq1,X_{2k}=0\}\)</span>. An example is shown below</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Now <span class="math inline">\(P(T_1=2k+1)\)</span> is exactly the probability that the walk hits state <span class="math inline">\(0\)</span> at time <span class="math inline">\(2k\)</span> (and has never hit state <span class="math inline">\(1\)</span> at all) and then goes up to state <span class="math inline">\(1\)</span>:
<span class="math display">\[P(T_1=2k+1)=P(X_{2k}=0 \text{ and has never hit state 1 (by time $2k$)}) \cdot p .\]</span>
The event <span class="math inline">\(\{\text{has never hit state 1}\}\)</span> is exactly the event that the maximum is zero:
<span class="math display">\[\{\text{has never hit state 1 (by time $2k$)}\}=\{M_{2k}=0\}.\]</span>
So we have
<span class="math display">\[P(T_1=2k+1)=P(X_{2k}=0, M_{2k}=0) \cdot p .\]</span></p>
<p>Now we know how many paths have <span class="math inline">\(X_{2k}=0\)</span>, and we need to subtract form that event, the number of paths that go above <span class="math inline">\(0\)</span> somewhere, i.e. the number of paths in the event <span class="math inline">\(\{X_{2k}=0,M_{2k}\geq1\}\)</span>. We have already worked out above (by the reflection principle) that this is the same as in <span class="math inline">\(\{X_{2k}=2\}\)</span>.</p>
<p>So performing the counting of paths we have:
<span class="math display">\[\# \{X_{2k}=0, M_{2k}\geq 1 \}=\#\{X_{2k}=2\}={2k\choose k-1}.\]</span>
And hence, the number of paths which never hit state <span class="math inline">\(1\)</span> by time <span class="math inline">\(2k\)</span> is
<span class="math display">\[
\begin{aligned}
\# \{ X_{2k}=0, M_{2k}=0 \}&amp;=\# \{X_{2k}=0\}-\# \{X_{2k}=0, M_{2k}\geq1 \}\\
&amp;={2k\choose k}-{2k\choose k-1}\\
&amp;=\frac{(2k)!}{k!k!}-\frac{(2k)!}{(k-1)!(k+1)!}\\
&amp;=\frac{(2k)!}{(k-1)!k!} \left(\frac1k-\frac1{k+1}\right)\\
&amp;=\frac{(2k)!}{(k-1)!k!} \cdot\frac1{k(k+1)}\\
&amp;=\frac1{k+1}{2k\choose k}.
\end{aligned}\]</span></p>
<p>Now since every path that starts at <span class="math inline">\(X_0=0\)</span> and ends at <span class="math inline">\(X_{2k}=0\)</span> is equally likely with <span class="math inline">\(k\)</span> up steps and <span class="math inline">\(k\)</span> downsteps, they each have probability <span class="math inline">\(p^k(1-p)^k\)</span>, and there are <span class="math inline">\(\frac1{k+1}{2k\choose k}\)</span> of those paths that never hit state <span class="math inline">\(1\)</span>. Now we put this all together to see that
<span class="math display">\[\begin{aligned}
P(T_1=2k+1)&amp;=P(X_{2k}=0, M_{2k}=0) \cdot p \\
&amp;=\# \{ X_{2k}=0, M_{2k}=0 \} p^k (1-p)^k \cdot p\\
&amp;=\frac1{k+1}{2k\choose k}p^{k+1}(1-p)^k.
\end{aligned}\]</span>
<span class="math inline">\(\square\)</span></p>
</div>
</div>
<p>By symmetry, we can understand the distribution for <span class="math inline">\(T_{-1}\)</span>, the hitting time for state <span class="math inline">\(-1\)</span> as well. For the SSRW, <span class="math inline">\(P(T_{-1}=k)=P(T_1=k)\)</span> since there is no difference in the probabilities of going up or down. In fact <span class="math inline">\(-X_n\)</span> (the reflected SSRW) has the same distribution as <span class="math inline">\(X_n\)</span>, and <span class="math inline">\(T_{-1}\)</span> for <span class="math inline">\(X_n\)</span> is exactly the same thing as <span class="math inline">\(T_1\)</span> for <span class="math inline">\(-X_n\)</span>. Since the latter is also a SSRW, then <span class="math inline">\(T_1\)</span> is exactly distributed according to the above given formula. If the walk was asymmetric with <span class="math inline">\(p\neq\frac12\)</span>, then <span class="math inline">\(T_{-1}\)</span> is distributed according to <span class="math inline">\(T_1\)</span> for the reflected (about 0) random walk, but with probabilities of up and down steps swapped. E.g. <span class="math inline">\(P(T_{-1}=k)\)</span> for the SARW with <span class="math inline">\(p\)</span> probability of up step is identical to <span class="math inline">\(P(T_{1}=k)\)</span> for the SARW with <span class="math inline">\(1-p\)</span> probability of up step.</p>
<p>We also have that <span class="math inline">\(E(T_1)=\infty\)</span> for <span class="math inline">\(p\leq 1/2\)</span> and that, for <span class="math inline">\(p&gt;1/2\)</span>,
<span class="math display">\[E(T_1)=\frac1{2p-1}.\]</span></p>
</div>
<div id="hitting-time-for-other-states" class="section level3 hasAnchor" number="6.6.2">
<h3><span class="header-section-number">6.6.2</span> Hitting time for other states<a href="random-walks.html#hitting-time-for-other-states" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we are interested in how long it takes for the process to hit state <span class="math inline">\(2\)</span>, then we wait for it to hit state <span class="math inline">\(1\)</span>. From this point, we only need to wait for it to go up another +1 from the current state, which is equivalent for waiting a random time which is distributed exactly the same as the hitting time for state one. Let <span class="math inline">\(T_2\)</span> be the hitting time for state <span class="math inline">\(2.\)</span> What this reasoning shows is that <span class="math inline">\(T_2=\tau_1+\tau_2\)</span> where <span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_2\)</span> have the same distribution as <span class="math inline">\(T_1\)</span> and are independent.</p>
<p>In general, letting <span class="math inline">\(T_k\)</span> be the hitting time for state <span class="math inline">\(k\)</span>, we have
<span class="math display">\[T_k=\sum_{j=1}^k \tau_j\]</span>
where <span class="math inline">\(\tau_j\)</span> are i.i.d. and distributed identically to <span class="math inline">\(T_1\)</span>.</p>
<div class="exbox">
<p><strong>Example.</strong>
For example <span class="math inline">\(T_2=\tau_1+\tau_2\)</span>. If we wish to calculate <span class="math inline">\(P(T_2=4)\)</span> then we need to consider all possibilities for <span class="math inline">\(\tau_1,\tau_2\)</span> that sum to give us 4 total time steps, and we use independence of the <span class="math inline">\(\tau_j\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
P(T_2=4)&amp;=P(\tau_1+\tau_2=4)\\
&amp;=P(\tau_1=1,\tau_2=3)+P(\tau_1=3,\tau_2=1)\\
&amp;=P(\tau_1=1)P(\tau_2=3)+P(\tau_1=3)P(\tau_2=1)\\
&amp;=P(T_1=1)P(T_1=3)+P(T_1=3)P(T_2=1)\\
&amp;=2P(T_1=1)P(T_1=3)\\
&amp;=2 \frac1{0+1}{2\cdot 0\choose 0}p^{0+1}(1-p)^0 \frac1{1+1}{2\cdot 1\choose 1}p^{1+1}(1-p)^1 \\
&amp;=2 p^3 (1-p) \\
\end{aligned}\]</span></p>
</div>
<p>The hitting time theorem is given below and allows us to calculate hitting time probabilities for any state for the SARW with any <span class="math inline">\(p\)</span>-value.</p>
<div class="thmbox">
<p><strong>Hitting time theorem.</strong>
Let <span class="math inline">\(X_n\)</span> be a simple random walk with <span class="math inline">\(p\)</span> the probability of stepping up. Let <span class="math inline">\(T_k\)</span> be the hitting time for level <span class="math inline">\(k\geq0\)</span>. We have
<span class="math display">\[P(T_k=m)=\frac km P(X_m=k)\]</span>
<!-- <a href="javascript:unhide('pfht');">Show/hide solution.</a> --></p>
<!-- ::: {#pfht .hiddendiv} -->
<!-- **Proof.**  -->
<!-- Here is the proof for the SSRW. -->
<!-- $$\begin{aligned} -->
<!-- a_{k,m}=P(T_k=m)&=P(T_k=m\mid X_1=-1)P(X_1=-1)+P(T_k=m\mid X_1=1)P(X_1=1)\\ -->
<!-- &=\frac12[P(T_k=m\mid X_1=-1)+P(T_k=m\mid X_1=1)]\\ -->
<!-- &=\frac12[a_{k+1,m-1}+a_{k-1,m-1}].\\ -->
<!-- \end{aligned}$$ -->
<!-- Now, we have $a_{0,0}=1$ since the process starts in state 0 (hence hits it at time zero with probability one) and $a_{0,m}=a_{k,0}=0$ for all nonzero $m,k$ since it cannot hit zero for the first time at any time step $m>0$, and it cannot hit any state $k>0$ in zero time steps. So we can proceed inductively: -->
<!-- $$a_{1,1}=\frac12[a_{2,0}+a_{0,0}]=\frac12a_{0,0}=\frac12=P(X_1=1).$$ -->
<!-- $$a_{2,1}=\frac12[a_{3,0}+a_{1,0}]=0.$$ -->
<!-- And for any $k\geq2$ we have -->
<!-- $$a_{k,1}=\frac12[a_{k+1,0}+a_{k-1,0}]=0.$$ -->
<!-- $$P(T_1=2)=a_{1,2}=\frac12[a_{2,1}+a_{0,1}]=\frac12(0+0)=0=\frac12P(X_2=1).$$ -->
<!-- Now assume it works for $(k,m)$ and show it works for $(k+1,m)$, $(k,m+1)$, and $(k+1,m+1)$. We assume $P(T_k=m)=\frac km P(X_m=k)$. Now -->
<!-- $$\begin{aligned} -->
<!-- P(T_{k}=m)&=P(T_{k}=m\mid X_1=1)P(X_1=1)+P(T_{k}=m\mid X_1=-1)P(X_1=-1)\\ -->
<!-- &=\frac12[P(T_{k}=m\mid X_1=1)+P(T_{k}=m\mid X_1=-1)]\\ -->
<!-- &=\frac12[P(T_{k-1}=m-1)+P(T_{k+1}=m-1)] -->
<!-- \end{aligned}$$ -->
<!-- Hence -->
<!-- $$\begin{aligned} -->
<!-- P(T_{k+1}=m-1)&=2 P(T_{k}=m)-P(T_{k-1}=m-1)\\ -->
<!-- &=2\frac km P(X_m=k)-\frac {k-1}{m-1} P(X_{m-1}={k-1})\\ -->
<!-- &=2\frac km {m\choose(m+k)/2}\frac1{2^m}-\frac {k-1}{m-1} {m-1\choose(m+k-2)/2}\frac1{2^{m-1}}\\ -->
<!-- &=\frac km \frac{m (m-1)!}{[(m+k)/2]![(m-k)/2]!}\frac1{2^{m-1}} -->
<!-- -\frac {k-1}{m-1} \frac{ (m-1)!}{[(m+k-2)/2]![(m-k+2)/2]!}\frac1{2^{m-1}}\\ -->
<!-- &=\left(k \frac{(m-1)!}{[(m+k)/2]![(m-k)/2]!} -->
<!-- -\frac {k-1}{m-1} \frac{ (m-1)!}{[(m+k)/2-1]![(m-k)/2+1][(m-k)/2]!}\right) \frac1{2^{m-1}}\\ -->
<!-- &=\left(k \frac{1}{[(m+k)/2]}  -->
<!-- -\frac {k-1}{m-1} \frac{1}{[(m-k)/2+1]}\right) \frac{(m-1)!}{[(m+k)/2-1]![(m-k)/2]!}\frac1{2^{m-1}}\\ -->
<!-- &=\left(k \frac{1}{[(m+k)/2]}  -->
<!-- -\frac {k-1}{m-1} \frac{1}{[(m-k)/2+1]}\right) P(X_{m-1}=k)\\ -->
<!-- &= -->
<!-- \end{aligned}$$ -->
<!-- ::: -->
</div>
<p>Here is an example usage of the hitting time theorem.</p>
<div class="exbox">
<p><strong>Example.</strong>
Let <span class="math inline">\(X_n\)</span> be a simple random walk with <span class="math inline">\(p\)</span> the probability of stepping up. Let <span class="math inline">\(T_k\)</span> be the hitting time for level <span class="math inline">\(k\geq0\)</span>. We have
<span class="math display">\[P(T_3=5)=\frac 35 P(X_5=3)=\frac35 {5\choose4}p^4(1-p)=3p^4(1-p).\]</span></p>
</div>
<p>The hitting time <span class="math inline">\(T_k\)</span> for state <span class="math inline">\(k\)</span> can be thought of in the following way. We wait for the walk to hit state 1 (a <span class="math inline">\(T_1\)</span> random number of time steps), and then from there, we consider it to be starting a new walk and then wait for it to go up one level from its new starting point (this is another <span class="math inline">\(T_1\)</span> random number of time steps), and then repeat this always waiting for it to go up one level. Each time we wait for it to go up <span class="math inline">\(+1\)</span> level, we wait for a random number of time steps, and each of these waits is equivalent to <span class="math inline">\(T_1\)</span> the time to hit level one. Since non-overlapping time periods are independent, we have a sequence of independent random wait times… i.e. a sequence of i.i.d. <span class="math inline">\(T_1\)</span>’s.</p>
<p>For each <span class="math inline">\(j\in\mathbb N\)</span>, let <span class="math inline">\(T_1^{(j)}\)</span> be a “copy” of the wait time to hit state 1 <span class="math inline">\(T_1\)</span>. This means that we can plug in whatever we want for <span class="math inline">\(j\)</span> and we’ll always have <span class="math inline">\(P(T_1^{(j)}=m)=P(T_1=m)\)</span>. We can think of <span class="math inline">\(T_k\)</span> as the sum of <span class="math inline">\(k\)</span> of these random times:
<span class="math display">\[T_k=\sum_{j=1}^kT_1^{(j)}.\]</span>
Now we can calculate the expected value using linearity of expected value operator:
<span class="math display">\[E(T_k)=E\left(\sum_{j=1}^kT_1^{(j)}\right)=\sum_{j=1}^kE\left(T_1^{(j)}\right)=kE(T_1).\]</span>
So now, we can calculate the expected wait time to hit any level using the expected wait time to hit level one!</p>
<div class="thmbox">
<p><strong>Theorem.</strong>
<span class="math display">\[E(T_k)=k E(T_1)\]</span></p>
</div>
</div>
</div>
<div id="return-time-to-state-0" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Return time to state <span class="math inline">\(0\)</span><a href="random-walks.html#return-time-to-state-0" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have that <span class="math inline">\(X_0=0\)</span> initially. We want to know how long it takes for the random walk to return to state zero. Let <span class="math inline">\(T_0=\min\{n&gt;0 : X_n=0\}\)</span> which we call the <em>return time to zero</em>. We can calculate <span class="math inline">\(P(T_0=2k)\)</span> for <span class="math inline">\(k=1,2,\ldots\)</span> by sketching a graph and counting paths. For example, <span class="math inline">\(P(T_0=2)=\frac12\)</span> since this means the first two time steps are up then down or vice versa. Similarly, we can calculate <span class="math inline">\(P(T_0=4)=\frac18\)</span>. The general formula is given below.</p>
<div class="thmbox">
<p><strong>Return time to state <span class="math inline">\(0\)</span>.</strong></p>
<p><span class="math display">\[P(T_0=2k)=\frac1{2k-1}{2k\choose k}p^{k}(1-p)^k, \ k=1,2,\ldots.\]</span></p>
</div>
<p>This is also the return time to the initial state more generally, if we initialized the walk with <span class="math inline">\(X_0\neq0\)</span>.</p>
<p>Here is a derivation of the formula using conditioning on the initial state. Conditioning in this way is a very important technique which can often simplify computations.
<span class="math display">\[
\begin{aligned}
P(T_0=2k)&amp;=P(T_0=2k\mid X_1=1)P(X_1=1)+P(T_0=2k\mid X_1=-1)P(X_1=-1)\\
&amp;=P(T_0=2k\mid X_1=1)p+P(T_0=2k\mid X_1=-1)(1-p)\\
&amp;=P(T_{-1}=2k-1)p+P(T_1=2k-1)(1-p)\\
\end{aligned}
\]</span>
This works because conditioning on <span class="math inline">\(X_1=1\)</span>, in order to return back to state <span class="math inline">\(0\)</span>, we must wait for how long it takes the walk to first go down one level, and (by shift invariance) this probability is identical to starting at zero and waiting to hit state <span class="math inline">\(-1\)</span>.</p>
<p>Now we already know how to calculate probabilities for <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_{-1}\)</span> (which is identical to <span class="math inline">\(T_1\)</span> for a reflected walk with <span class="math inline">\(p\)</span> and <span class="math inline">\(1-p\)</span> swapped).
<span class="math display">\[P(T_1=2k-1)=\frac1{k}{2k-2\choose k-1}p^{k}(1-p)^{k-1}\]</span>
<span class="math display">\[P(T_{-1}=2k-1)=\frac1{k}{2k-2\choose k-1}p^{k-1}(1-p)^{k}\]</span>
<span class="math display">\[
\begin{aligned}
P(T_0=2k)&amp;=\frac2{k}{2k-2\choose k-1} p^k(1-p)^k\\
\end{aligned}
\]</span>
Now we just need to show the coefficient simplifies to <span class="math inline">\(\frac1{2k-1}{2k\choose k}.\)</span></p>
<p><span class="math display">\[\begin{aligned}
\frac2{k}{2k-2\choose k-1}&amp;=\frac2k \frac{(2k-2)!}{(k-1)!(k-1)!}\\
&amp;=\frac2{\color{blue}{k}} \frac{\color{red}{(2k-2)!}}{\color{blue}{(k-1)!}\color{green}{(k-1)!}} \cdot \frac{\color{red}{2k(2k-1)}}{2\color{green}{k}(2k-1)}\\
&amp;=\frac{\color{red}{(2k)!}}{\color{blue}{k!}\color{green}{k!}} \cdot \frac{1}{(2k-1)}\\
&amp;={2k \choose k} \cdot \frac{1}{(2k-1)}
\end{aligned}\]</span>
This finishes the calculation.</p>
<!-- ## RW2 -->
<!-- ::: exbox -->
<!-- **Example.** -->
<!-- here is an example problem... -->
<!-- <details> -->
<!-- <summary> -->
<!-- show/hide solution -->
<!-- </summary> -->
<!-- Here is the solution, we use the following theorem: -->
<!-- ::: thmbox -->
<!-- **Theorem.** Here is a hidden theorem... -->
<!-- ::: -->
<!-- and that solves it! -->
<!-- </details> -->
<!-- ::: -->
<p>Here is a table for returns to zero with <span class="math inline">\(T_0\)</span> the wait time to return to zero and <span class="math inline">\(N\)</span> the number of times that we return to zero. Note that the actual number of times the walk is in state zero is <span class="math inline">\(N+1\)</span> since it always starts in state zero.</p>
<table>
<colgroup>
<col width="11%" />
<col width="16%" />
<col width="18%" />
<col width="8%" />
<col width="13%" />
<col width="13%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th align="center"><span class="math inline">\(r=P(T_0&lt;\infty)\)</span></th>
<th align="center"><span class="math inline">\(P(T_0=\infty)\)</span></th>
<th><span class="math inline">\(E(T_0)\)</span></th>
<th><span class="math inline">\(P(N&lt;\infty)\)</span></th>
<th><span class="math inline">\(P(N=\infty)\)</span></th>
<th><span class="math inline">\(E(N)=\frac{r}{1-r}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p&lt;\frac12\)</span></td>
<td align="center"><span class="math inline">\(2p\)</span></td>
<td align="center"><span class="math inline">\(1-2p\)</span></td>
<td><span class="math inline">\(\infty\)</span></td>
<td>1</td>
<td>0</td>
<td><span class="math inline">\(\frac{2p}{1-2p}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(p=\frac12\)</span></td>
<td align="center">1</td>
<td align="center">0</td>
<td><span class="math inline">\(\infty\)</span></td>
<td>0</td>
<td>1</td>
<td><span class="math inline">\(\infty\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(p&gt;\frac12\)</span></td>
<td align="center"><span class="math inline">\(2(1-p)\)</span></td>
<td align="center"><span class="math inline">\(2p-1\)</span></td>
<td><span class="math inline">\(\infty\)</span></td>
<td>1</td>
<td>0</td>
<td><span class="math inline">\(\frac{2-2p}{2p-1}\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="summary-4" class="section level2 unnumbered hasAnchor">
<h2>Summary<a href="random-walks.html#summary-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="defbox">
<p><strong>Summary of notation, formulas, and terminology</strong></p>
<p>For <span class="math inline">\(X_n\)</span> the simple 1D random walk with <span class="math inline">\(p\)</span> probability of an up step (and <span class="math inline">\(1-p\)</span> down step):</p>
<p><strong>Distribution of <span class="math inline">\(X_n\)</span></strong>:<br>
<span class="math inline">\(P(X_n=j)={n\choose (n+j)/2}p^{(n+j)/2}(1-p)^{(n-j)/2}\)</span><br>
    For SSRW: <span class="math inline">\(P(X_n=j)={n\choose (n+j)/2}\frac1{2^n}\)</span></p>
<p><span class="math inline">\(P(X_n=j)\approx\)</span><code>pnorm(j+1,2*n*p-n,sqrt(4*n*p*(1-p)))-pnorm(j-1,2*n*p-n,sqrt(4*n*p*(1-p)))</code><br>
    For SSRW: <span class="math inline">\(P(X_n=j)\approx\)</span><code>pnorm(j+1,0,sqrt(n)-pnorm(j-1,0,sqrt(n))</code><br></p>
<p><strong>Shift invariance &amp; memorylessness</strong>:<br>
<span class="math inline">\(P(X_{n+m}=k\mid X_m=j)=P(X_{n}=k-j)\)</span><br>
<span class="math inline">\(P(X_{n}=k\mid X_{n-1}=j,X_{n-2}=a_{n-2},\ldots,X_{2}=a_{2},X_{1}=a_{1})=P(X_{n}=k\mid X_{n-1}=j)\)</span></p>
<p><strong>Maximum of SSRW</strong>:<br>
<span class="math inline">\(P(M_n=k)=P(X_n=k)+P(X_n=k+1)\)</span>, <span class="math inline">\(M_n=\)</span> max level reached in first <span class="math inline">\(n\)</span> time steps</p>
<p><strong>Maximum of SARW w/ <span class="math inline">\(p&lt;1/2\)</span></strong>:<br>
<span class="math inline">\(P(M_\infty=k)=\left( \frac p{1-p}\right)^k\left( \frac {1-2p}{1-p}\right)\)</span>, <span class="math inline">\(M_\infty=\)</span> max level reached over the entire walk covering (for infinitely many time steps)</p>
<p><strong>Hitting times</strong>:<br>
<span class="math inline">\(P(T_1=2k+1)=\frac1{k+1}{2k\choose k}\frac1{2^{2k+1}}\)</span>, <span class="math inline">\(T_1=~\)</span>first time SSRW (<span class="math inline">\(p=\frac12\)</span>) to hit state <span class="math inline">\(1\)</span></p>
<p><span class="math inline">\(P(T_1=2k+1)=\frac1{k+1}{2k\choose k}p^{k+1}(1-p)^k\)</span>, <span class="math inline">\(T_1=~\)</span>first time SARW (any <span class="math inline">\(p\in[0,1]\)</span>) to hit state <span class="math inline">\(1\)</span></p>
<p><span class="math inline">\(P(T_k=m)=\frac mk P(X_m=k)\)</span>, <span class="math inline">\(T_k=~\)</span>first time SARW hits state <span class="math inline">\(k\)</span></p>
<p><strong>Return time to initial state</strong>:<br>
<span class="math inline">\(P(T_0=2k)=\frac1{2k-1}{2k\choose k}p^{k}(1-p)^k\)</span>, <span class="math inline">\(T_0=~\)</span>first time SARW returns to state <span class="math inline">\(0\)</span></p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro-stochastic-processes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="limit-theorems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jstover79/stochproc423/edit/master/06-rand-walk.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/jstover79/stochproc423/blob/master/06-rand-walk.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
